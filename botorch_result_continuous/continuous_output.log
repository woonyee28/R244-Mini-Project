INFO 12-28 18:13:55 [__init__.py:216] Automatically detected platform cuda.
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset
======================================================================
Starting Bayesian Optimization: 10 random + 30 BO trials.
Skipping trial: 12 GPUs required, only 4 available.
   Initial Trial 1/10 completed. Throughput: 0.00, Energy: (1000000000.0, 0.0)
INFO 12-28 18:14:01 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'tensor_parallel_size': 2, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 5600, 'max_num_seqs': 190, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:14:02 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:14:02 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:14:02 [model.py:1510] Using max model len 32768
INFO 12-28 18:14:05 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=5600.
INFO 12-28 18:14:05 [__init__.py:381] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:05 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:05 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=3994650)[0;0m WARNING 12-28 18:14:05 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_5d520a51'), local_subscribe_addr='ipc:///tmp/c771ec1b-5fed-48a8-8600-e6e06100ec2d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a8396f26'), local_subscribe_addr='ipc:///tmp/60d36440-161f-4800-bfe0-512bf398ca7e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_895e2f6c'), local_subscribe_addr='ipc:///tmp/9834ca81-b453-4608-9529-e1d4d55a7cdd', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3994650)[0;0m WARNING 12-28 18:14:09 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m WARNING 12-28 18:14:09 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_ad2c5939'), local_subscribe_addr='ipc:///tmp/0c7e6bcf-5e5b-49f5-9eb8-71ed705fc618', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:09 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=3994650)[0;0m WARNING 12-28 18:14:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m WARNING 12-28 18:14:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP1 pid=3994662)[0;0m INFO 12-28 18:14:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP0 pid=3994659)[0;0m INFO 12-28 18:14:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP1 pid=3994662)[0;0m INFO 12-28 18:14:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP0 pid=3994659)[0;0m INFO 12-28 18:14:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP1 pid=3994662)[0;0m INFO 12-28 18:14:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP0 pid=3994659)[0;0m INFO 12-28 18:14:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP0 pid=3994659)[0;0m INFO 12-28 18:14:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP1 pid=3994662)[0;0m INFO 12-28 18:14:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP0 pid=3994659)[0;0m INFO 12-28 18:14:18 [default_loader.py:267] Loading weights took 7.62 seconds
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP1 pid=3994662)[0;0m INFO 12-28 18:14:18 [default_loader.py:267] Loading weights took 7.61 seconds
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP0 pid=3994659)[0;0m INFO 12-28 18:14:19 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 8.297232 seconds
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP1 pid=3994662)[0;0m INFO 12-28 18:14:19 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 8.561917 seconds
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP0 pid=3994659)[0;0m INFO 12-28 18:14:23 [gpu_worker.py:298] Available KV cache memory: 12.66 GiB
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP1 pid=3994662)[0;0m INFO 12-28 18:14:23 [gpu_worker.py:298] Available KV cache memory: 12.66 GiB
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:24 [kv_cache_utils.py:1087] GPU KV cache size: 207,440 tokens
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:24 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 21.08x
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:24 [kv_cache_utils.py:1087] GPU KV cache size: 207,440 tokens
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:24 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 21.08x
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP1 pid=3994662)[0;0m WARNING 12-28 18:14:24 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m [1;36m(Worker_TP0 pid=3994659)[0;0m WARNING 12-28 18:14:24 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:24 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.55 seconds
[1;36m(EngineCore_DP0 pid=3994650)[0;0m INFO 12-28 18:14:24 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:14:25 [llm.py:306] Supported_tasks: ['generate']
EmissionsData(timestamp='2025-12-28T18:14:39', project_name='codecarbon', run_id='703ef7b2-64c9-469a-9e04-5650c86cc1c5', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=38.12129015196115, emissions=0.0005154030730521239, emissions_rate=1.3520084734739993e-05, cpu_power=67.19122092, gpu_power=132.46236117398567, ram_power=70.0, cpu_energy=0.000574793457402187, gpu_energy=0.0008839418182642333, ram_energy=0.0007105700080467311, energy_consumed=0.0021693052837131513, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 2/10 completed. Throughput: 446.61, Energy: (1.2505202596264764, 0.0)
INFO 12-28 18:14:44 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 96, 'enable_prefix_caching': True, 'max_num_batched_tokens': 11358, 'max_num_seqs': 82, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:14:45 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:14:45 [model.py:1510] Using max model len 32768
INFO 12-28 18:14:45 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=11358.
INFO 12-28 18:14:45 [__init__.py:381] Cudagraph is disabled under eager mode
WARNING 12-28 18:14:45 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
INFO 12-28 18:14:52 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:14:54 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:14:54 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=3995732)[0;0m WARNING 12-28 18:14:54 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:14:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_80487e52'), local_subscribe_addr='ipc:///tmp/aef0f754-4666-4a23-892c-ab84f987ad3c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:15:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:15:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:15:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:15:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:15:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a24de307'), local_subscribe_addr='ipc:///tmp/ba2170ab-22bc-4231-8850-27f83cc21c1d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:15:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_51d9e317'), local_subscribe_addr='ipc:///tmp/d4caf20c-d9eb-49fa-bc28-99ccbbbb0e77', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:15:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c1b89643'), local_subscribe_addr='ipc:///tmp/ed2ae1c3-b8ea-4e15-9147-b610650e636c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:15:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cf9edcc6'), local_subscribe_addr='ipc:///tmp/969bb258-8d67-460f-bb65-c78d9fac19a5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:15:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:15:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:15:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:15:10 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:15:11 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:15:11 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:15:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_ed514d01'), local_subscribe_addr='ipc:///tmp/61f04002-9dca-4767-a6ee-990056d7a883', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING 12-28 18:15:11 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:15:11 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:15:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_75a63582'), local_subscribe_addr='ipc:///tmp/47d91170-a7f9-4e22-879f-aaceaa353705', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:15:11 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:11 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:11 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:15:11 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:15:11 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:11 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:15:11 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:11 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:15:11 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:11 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:15:11 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:11 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:15:11 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:11 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:15:11 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:15:11 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:15:11 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 18:15:11 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:15:11 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
INFO 12-28 18:15:11 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
WARNING 12-28 18:15:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:15:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:15:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:15:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:11 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:11 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:11 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:11 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:12 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:12 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:12 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:12 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:12 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:12 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:12 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:12 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:12 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:12 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:12 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:12 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:14 [default_loader.py:267] Loading weights took 1.11 seconds
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:14 [default_loader.py:267] Loading weights took 1.05 seconds
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:15 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.232825 seconds
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:15 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:15 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.493202 seconds
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:15 [default_loader.py:267] Loading weights took 1.09 seconds
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:15 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 3.019851 seconds
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:16 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 3.536640 seconds
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:18 [gpu_worker.py:298] Available KV cache memory: 15.59 GiB
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:18 [gpu_worker.py:298] Available KV cache memory: 15.59 GiB
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:19 [gpu_worker.py:298] Available KV cache memory: 15.69 GiB
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:19 [gpu_worker.py:298] Available KV cache memory: 15.69 GiB
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [kv_cache_utils.py:1087] GPU KV cache size: 514,176 tokens
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 33.06x
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [kv_cache_utils.py:1087] GPU KV cache size: 514,176 tokens
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 33.06x
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [kv_cache_utils.py:1087] GPU KV cache size: 510,720 tokens
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 32.84x
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [kv_cache_utils.py:1087] GPU KV cache size: 510,720 tokens
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 32.84x
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m WARNING 12-28 18:15:19 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m WARNING 12-28 18:15:19 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:19 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.51 seconds
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:20 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=3995732)[0;0m INFO 12-28 18:15:20 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:15:20 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m WARNING 12-28 18:15:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m WARNING 12-28 18:15:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:36 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP0 pid=3996269)[0;0m INFO 12-28 18:15:36 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:36 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:36 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP0 pid=3996271)[0;0m INFO 12-28 18:15:36 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=3996270)[0;0m INFO 12-28 18:15:36 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:36 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP1 pid=3996272)[0;0m INFO 12-28 18:15:36 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:15:36', project_name='codecarbon', run_id='2878a6b9-8239-41c5-8a4a-a9d785ea6c31', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=52.239026407012716, emissions=0.0008686446928268485, emissions_rate=1.662827109485025e-05, cpu_power=52.176762896250004, gpu_power=239.55833281908002, ram_power=70.0, cpu_energy=0.0007027550352597005, gpu_energy=0.0019785168605910286, ram_energy=0.0009748093740180291, energy_consumed=0.0036560812698687587, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 3/10 completed. Throughput: 449.14, Energy: (1.9217247147798995, 0.0)
INFO 12-28 18:15:43 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 2, 'block_size': 48, 'enable_prefix_caching': False, 'max_num_batched_tokens': 5222, 'max_num_seqs': 137, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:15:44 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:15:44 [model.py:1510] Using max model len 32768
INFO 12-28 18:15:44 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=5222.
INFO 12-28 18:15:44 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:15:50 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:15:53 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:15:53 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=3999722)[0;0m WARNING 12-28 18:15:53 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:15:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_667ff58d'), local_subscribe_addr='ipc:///tmp/1f4c1c5b-d765-4e78-9ffd-326d033f917b', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:16:00 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:16:00 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:16:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fb1fa3a8'), local_subscribe_addr='ipc:///tmp/a49d3d17-9cf0-4192-9ee2-923c57e064dc', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:16:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f59182a1'), local_subscribe_addr='ipc:///tmp/a7adbff2-e4a4-4a37-8dbc-dccb056e50f1', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:16:06 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:16:06 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:16:06 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:16:06 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:16:06 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:16:06 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
WARNING 12-28 18:16:07 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:16:07 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=3999989)[0;0m INFO 12-28 18:16:07 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:07 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=3999989)[0;0m INFO 12-28 18:16:07 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:07 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:07 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=3999989)[0;0m INFO 12-28 18:16:07 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:08 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=3999989)[0;0m INFO 12-28 18:16:08 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:09 [default_loader.py:267] Loading weights took 1.11 seconds
[1;36m(Worker_PP1 pid=3999989)[0;0m INFO 12-28 18:16:10 [default_loader.py:267] Loading weights took 1.08 seconds
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:10 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 2.213329 seconds
[1;36m(Worker_PP1 pid=3999989)[0;0m INFO 12-28 18:16:10 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 2.646598 seconds
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:12 [gpu_worker.py:298] Available KV cache memory: 12.50 GiB
[1;36m(Worker_PP1 pid=3999989)[0;0m INFO 12-28 18:16:12 [gpu_worker.py:298] Available KV cache memory: 12.46 GiB
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:16:12 [kv_cache_utils.py:1087] GPU KV cache size: 204,768 tokens
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:16:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 21.77x
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:16:12 [kv_cache_utils.py:1087] GPU KV cache size: 204,096 tokens
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:16:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 21.69x
[1;36m(Worker_PP1 pid=3999989)[0;0m WARNING 12-28 18:16:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:16:12 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.11 seconds
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:16:13 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=3999722)[0;0m INFO 12-28 18:16:13 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:16:14 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=3999988)[0;0m WARNING 12-28 18:16:14 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:37 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=3999988)[0;0m INFO 12-28 18:16:37 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=3999989)[0;0m INFO 12-28 18:16:37 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:16:37', project_name='codecarbon', run_id='6c57438b-8ea1-4ec2-a0ef-bcba67ee906f', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=54.22551530099008, emissions=0.0007120547764979949, emissions_rate=1.3131360256248109e-05, cpu_power=58.45268059500001, gpu_power=136.02686634596827, ram_power=70.0, cpu_energy=0.0006859481233056847, gpu_energy=0.0012978007604615982, ram_energy=0.0010132534037880963, energy_consumed=0.0029970022875553787, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 4/10 completed. Throughput: 320.13, Energy: (1.4812202409664148, 0.0)
INFO 12-28 18:16:44 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 6594, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:16:44 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:16:44 [model.py:1510] Using max model len 32768
INFO 12-28 18:16:44 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6594.
INFO 12-28 18:16:44 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:16:51 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:16:54 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:16:54 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4001169)[0;0m WARNING 12-28 18:16:54 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:16:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_88a4a67d'), local_subscribe_addr='ipc:///tmp/856339cb-b39c-426a-bd75-d6d35cc2b1c7', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:17:02 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:17:02 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:17:02 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:17:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7a84596e'), local_subscribe_addr='ipc:///tmp/0271c3ed-eee4-4bdd-8335-449cb6cbb6d2', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:17:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2e72671a'), local_subscribe_addr='ipc:///tmp/56b2e306-2b49-4ed2-911e-09e8e13e37a5', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:17:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_332e42d9'), local_subscribe_addr='ipc:///tmp/4dba4f74-beb4-4053-a525-bf9ee0f65c38', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:17:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:17:08 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:17:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:17:08 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:17:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:17:08 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:17:09 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:17:09 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:17:09 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
WARNING 12-28 18:17:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:17:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:17:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:09 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:09 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:09 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:09 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:09 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:09 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:09 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:11 [default_loader.py:267] Loading weights took 0.63 seconds
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:11 [default_loader.py:267] Loading weights took 0.63 seconds
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:11 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:11 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.246464 seconds
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:11 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.372144 seconds
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:12 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.724856 seconds
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:13 [gpu_worker.py:298] Available KV cache memory: 14.58 GiB
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:14 [gpu_worker.py:298] Available KV cache memory: 14.39 GiB
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:14 [gpu_worker.py:298] Available KV cache memory: 14.74 GiB
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:14 [kv_cache_utils.py:1087] GPU KV cache size: 342,880 tokens
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:14 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.89x
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:14 [kv_cache_utils.py:1087] GPU KV cache size: 347,488 tokens
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:14 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 32.32x
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:14 [kv_cache_utils.py:1087] GPU KV cache size: 386,432 tokens
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:14 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 35.94x
[1;36m(Worker_PP2 pid=4001432)[0;0m WARNING 12-28 18:17:14 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:14 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.07 seconds
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:15 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4001169)[0;0m INFO 12-28 18:17:15 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:17:15 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4001430)[0;0m WARNING 12-28 18:17:15 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4001431)[0;0m WARNING 12-28 18:17:15 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:42 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4001430)[0;0m INFO 12-28 18:17:42 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:42 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4001431)[0;0m INFO 12-28 18:17:42 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:42 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=4001432)[0;0m INFO 12-28 18:17:42 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:17:42', project_name='codecarbon', run_id='93a75a14-ba29-4794-96c0-860b1724aa9c', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=58.747615539003164, emissions=0.0009360496012685615, emissions_rate=1.5933405852823223e-05, cpu_power=53.83789802785715, gpu_power=204.35664208180975, ram_power=70.0, cpu_energy=0.0007579544424435017, gpu_energy=0.002080691386773914, ram_energy=0.001101139267182508, energy_consumed=0.003939785096399924, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 5/10 completed. Throughput: 408.04, Energy: (1.3340130123250307, 0.0)
INFO 12-28 18:17:48 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9169, 'max_num_seqs': 153, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:17:49 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:17:49 [model.py:1510] Using max model len 32768
INFO 12-28 18:17:49 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9169.
INFO 12-28 18:17:49 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:17:56 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:17:59 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:17:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4002733)[0;0m WARNING 12-28 18:17:59 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:17:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d3c37a52'), local_subscribe_addr='ipc:///tmp/1345045d-c056-4435-a173-2dc53b31ddc0', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:18:07 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:18:07 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:18:07 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:18:07 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:18:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bfea5f19'), local_subscribe_addr='ipc:///tmp/eb142503-8f14-429f-a49b-397bc64e7073', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:18:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_110c8cae'), local_subscribe_addr='ipc:///tmp/6cd1d172-036d-4d8e-b584-8384a2029a52', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:18:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6fa9306e'), local_subscribe_addr='ipc:///tmp/3a1f1de1-6b8c-4d33-824e-b78b01d07a9e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:18:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_04f7c1b2'), local_subscribe_addr='ipc:///tmp/fbdbb6b1-dfd5-4281-81ce-a642a98c045e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:18:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:18:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:18:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:18:14 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:18:14 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:18:14 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:18:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_af37073e'), local_subscribe_addr='ipc:///tmp/f840aedd-2124-4264-be0e-ef83a8e50f07', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING 12-28 18:18:14 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:18:14 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:18:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_29f60aea'), local_subscribe_addr='ipc:///tmp/10921e85-5dd6-4029-9a33-4805d647de89', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:18:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:18:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:18:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:18:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:18:15 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:15 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:15 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:18:15 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:18:15 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:15 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:18:15 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:18:15 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:18:15 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
INFO 12-28 18:18:15 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
INFO 12-28 18:18:15 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:18:15 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 18:18:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:18:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:18:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:18:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:16 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:16 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:16 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:16 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:16 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:16 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:16 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:16 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:17 [default_loader.py:267] Loading weights took 1.15 seconds
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:18 [default_loader.py:267] Loading weights took 1.16 seconds
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:18 [default_loader.py:267] Loading weights took 1.13 seconds
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:18 [default_loader.py:267] Loading weights took 1.10 seconds
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:18 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 1.796386 seconds
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:18 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 1.996852 seconds
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:18 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.103581 seconds
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:19 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.334336 seconds
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:21 [gpu_worker.py:298] Available KV cache memory: 15.74 GiB
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:21 [gpu_worker.py:298] Available KV cache memory: 15.74 GiB
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:22 [gpu_worker.py:298] Available KV cache memory: 15.83 GiB
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:22 [gpu_worker.py:298] Available KV cache memory: 15.83 GiB
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [kv_cache_utils.py:1087] GPU KV cache size: 518,656 tokens
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 38.59x
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [kv_cache_utils.py:1087] GPU KV cache size: 518,656 tokens
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 38.59x
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [kv_cache_utils.py:1087] GPU KV cache size: 515,712 tokens
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 38.37x
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [kv_cache_utils.py:1087] GPU KV cache size: 515,712 tokens
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 38.37x
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m WARNING 12-28 18:18:22 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m WARNING 12-28 18:18:22 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:22 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.61 seconds
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:23 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=4002733)[0;0m INFO 12-28 18:18:23 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:18:23 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m WARNING 12-28 18:18:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m WARNING 12-28 18:18:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
WARNING 12-28 18:18:26 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:18:29 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:18:31 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:18:35 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:18:37 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:38 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP0 pid=4002950)[0;0m INFO 12-28 18:18:38 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:38 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP1 pid=4002951)[0;0m INFO 12-28 18:18:38 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:38 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:38 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP1 pid=4002953)[0;0m INFO 12-28 18:18:38 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP0 pid=4002952)[0;0m INFO 12-28 18:18:38 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:18:38', project_name='codecarbon', run_id='7069c89f-5037-4f18-bf6d-43d465283cdb', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=50.21215648506768, emissions=0.0008408300663935086, emissions_rate=1.6745547796648774e-05, cpu_power=60.03486718500001, gpu_power=241.07829048786706, ram_power=70.0, cpu_energy=0.0006755958689789671, gpu_energy=0.001927957097919375, ram_energy=0.0009354579569807458, energy_consumed=0.003539010923879088, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 6/10 completed. Throughput: 466.22, Energy: (1.874696781336774, 0.0)
INFO 12-28 18:18:45 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'tensor_parallel_size': 2, 'block_size': 64, 'enable_prefix_caching': True, 'max_num_batched_tokens': 4673, 'max_num_seqs': 93, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:18:46 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:18:46 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:18:46 [model.py:1510] Using max model len 32768
INFO 12-28 18:18:46 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=4673.
INFO 12-28 18:18:46 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:18:52 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:18:55 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:18:55 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4004275)[0;0m WARNING 12-28 18:18:55 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:18:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_ce0dad86'), local_subscribe_addr='ipc:///tmp/b9a00bfd-e487-42d3-a0f1-5c7eb03e09ec', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:19:02 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:19:02 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:19:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6356d164'), local_subscribe_addr='ipc:///tmp/b43d3266-4bd7-46f6-8401-ef9b65ff4ff7', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:19:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2afe2a82'), local_subscribe_addr='ipc:///tmp/aee69137-a309-465e-939b-0f298d27f2c6', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:19:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:19:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:19:08 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:19:08 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:19:08 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:19:08 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:19:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_d4e01063'), local_subscribe_addr='ipc:///tmp/9f2d330d-7401-4b81-b5ca-68518a9b0f9b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:19:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:19:08 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:19:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:19:08 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:19:08 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:19:08 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 18:19:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:19:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=4004483)[0;0m INFO 12-28 18:19:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=4004483)[0;0m INFO 12-28 18:19:09 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:09 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=4004483)[0;0m INFO 12-28 18:19:09 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:09 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4004483)[0;0m INFO 12-28 18:19:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=4004483)[0;0m INFO 12-28 18:19:17 [default_loader.py:267] Loading weights took 7.53 seconds
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:17 [default_loader.py:267] Loading weights took 7.50 seconds
[1;36m(Worker_TP1 pid=4004483)[0;0m INFO 12-28 18:19:18 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 8.202453 seconds
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:18 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 8.462117 seconds
[1;36m(Worker_TP1 pid=4004483)[0;0m INFO 12-28 18:19:21 [gpu_worker.py:298] Available KV cache memory: 12.72 GiB
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:21 [gpu_worker.py:298] Available KV cache memory: 12.72 GiB
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:19:21 [kv_cache_utils.py:1087] GPU KV cache size: 208,448 tokens
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:19:21 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 23.60x
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:19:21 [kv_cache_utils.py:1087] GPU KV cache size: 208,448 tokens
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:19:21 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 23.60x
[1;36m(Worker_TP0 pid=4004482)[0;0m WARNING 12-28 18:19:21 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=4004483)[0;0m WARNING 12-28 18:19:21 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:19:22 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.60 seconds
[1;36m(EngineCore_DP0 pid=4004275)[0;0m INFO 12-28 18:19:23 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:19:23 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:37 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=4004482)[0;0m INFO 12-28 18:19:37 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=4004483)[0;0m INFO 12-28 18:19:37 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:19:37', project_name='codecarbon', run_id='20e639e7-9771-4156-bb20-367411ab9f94', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=52.429649522993714, emissions=0.0006441601866776171, emissions_rate=1.228618143623318e-05, cpu_power=55.754791860000005, gpu_power=137.66537213406548, ram_power=70.0, cpu_energy=0.0006463684262117421, gpu_energy=0.0010864178135756575, ram_energy=0.0009784511772547074, energy_consumed=0.002711237417042107, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 7/10 completed. Throughput: 474.14, Energy: (1.5048496301806331, 0.0)
Skipping trial: 12 GPUs required, only 4 available.
   Initial Trial 8/10 completed. Throughput: 0.00, Energy: (1000000000.0, 0.0)
INFO 12-28 18:19:44 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'tensor_parallel_size': 3, 'block_size': 64, 'enable_prefix_caching': True, 'max_num_batched_tokens': 4287, 'max_num_seqs': 85, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:19:44 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:19:44 [model.py:1510] Using max model len 32768
INFO 12-28 18:19:44 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=4287.
Error during evaluation: 1 validation error for VllmConfig
  Value error, Total number of attention heads (32) must be divisible by tensor parallel size (3). [type=value_error, input_value=ArgsKwargs((), {'model_co...additional_config': {}}), input_type=ArgsKwargs]
    For further information visit https://errors.pydantic.dev/2.12/v/value_error
Evaluation failed, returning worst case metrics.
   Initial Trial 9/10 completed. Throughput: 0.00, Energy: (1000000000.0, 0.0)
INFO 12-28 18:19:49 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 64, 'enable_prefix_caching': False, 'max_num_batched_tokens': 10696, 'max_num_seqs': 144, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:19:49 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:19:49 [model.py:1510] Using max model len 32768
INFO 12-28 18:19:49 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=10696.
INFO 12-28 18:19:49 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:19:56 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:19:59 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:19:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4005684)[0;0m WARNING 12-28 18:19:59 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:19:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_bd058849'), local_subscribe_addr='ipc:///tmp/589e5f61-8737-4a6a-b7ff-0825009cb582', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:20:07 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:20:07 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:20:07 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:20:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c6b55230'), local_subscribe_addr='ipc:///tmp/f74de880-0c1b-46e7-b936-fcc647da5756', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:20:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9be3ac1e'), local_subscribe_addr='ipc:///tmp/bf6f03a7-78f7-4019-8bf6-160c0d9907c9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:20:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7e2f5998'), local_subscribe_addr='ipc:///tmp/4d08ac10-eac5-443c-b3f8-c666526ea7f5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:20:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:20:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:20:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:20:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:20:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:20:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:20:14 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:20:14 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:20:14 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
WARNING 12-28 18:20:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:20:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:20:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:15 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:15 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:15 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:17 [default_loader.py:267] Loading weights took 0.71 seconds
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:17 [default_loader.py:267] Loading weights took 0.56 seconds
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:18 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.849258 seconds
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:18 [default_loader.py:267] Loading weights took 0.68 seconds
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:18 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 2.196332 seconds
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:18 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 2.730038 seconds
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:20 [gpu_worker.py:298] Available KV cache memory: 14.13 GiB
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:20 [gpu_worker.py:298] Available KV cache memory: 13.97 GiB
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:20 [gpu_worker.py:298] Available KV cache memory: 14.29 GiB
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:21 [kv_cache_utils.py:1087] GPU KV cache size: 332,800 tokens
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:21 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 22.32x
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:21 [kv_cache_utils.py:1087] GPU KV cache size: 336,640 tokens
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:21 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 22.58x
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:21 [kv_cache_utils.py:1087] GPU KV cache size: 374,528 tokens
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:21 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.12x
[1;36m(Worker_PP2 pid=4005918)[0;0m WARNING 12-28 18:20:21 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:21 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.38 seconds
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:22 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4005684)[0;0m INFO 12-28 18:20:22 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:20:22 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4005916)[0;0m WARNING 12-28 18:20:22 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4005917)[0;0m WARNING 12-28 18:20:22 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4005916)[0;0m INFO 12-28 18:20:46 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4005917)[0;0m INFO 12-28 18:20:46 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=4005918)[0;0m INFO 12-28 18:20:46 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:20:46', project_name='codecarbon', run_id='a059efb2-215e-42c6-ba9a-894acacbaf14', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=58.16995139303617, emissions=0.0008955457197792082, emissions_rate=1.5395332097293425e-05, cpu_power=58.46262236142858, gpu_power=178.52160055186755, ram_power=70.0, cpu_energy=0.0007780437959112012, gpu_energy=0.0019012292987605761, ram_energy=0.0010900332275031058, energy_consumed=0.0037693063221748827, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 10/10 completed. Throughput: 528.88, Energy: (1.0743866001448596, 0.0)
Skipping trial: 8 GPUs required, only 4 available.
   BO Iteration 1/30 completed. Throughput: 0.00, Energy: 1000000000.000000. Pareto size: 2.
INFO 12-28 18:20:54 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'tensor_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8715, 'max_num_seqs': 198, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:20:55 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:20:55 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:20:55 [model.py:1510] Using max model len 32768
INFO 12-28 18:20:55 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8715.
INFO 12-28 18:20:55 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:21:02 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:04 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:05 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4008796)[0;0m WARNING 12-28 18:21:05 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_e6b0a5de'), local_subscribe_addr='ipc:///tmp/9f4488b5-dada-4f4e-9149-ad79623fbc2c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:21:11 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:21:11 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:21:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d93f1f85'), local_subscribe_addr='ipc:///tmp/f333f0bf-ab0c-48af-ba04-5c7d420fd0e3', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:21:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_976b62ac'), local_subscribe_addr='ipc:///tmp/4a72d825-c8b1-4677-a7cb-6ecb5b22fa40', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:21:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:21:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:21:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:21:18 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:21:18 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:21:18 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:21:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_cf7dc936'), local_subscribe_addr='ipc:///tmp/64a4dab4-2c0a-4a6c-8093-674b881bf28b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:21:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:21:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:21:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:21:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:21:18 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:21:18 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 18:21:18 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:21:18 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:28 [default_loader.py:267] Loading weights took 7.54 seconds
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:28 [default_loader.py:267] Loading weights took 7.52 seconds
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:28 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 8.714659 seconds
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:29 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 9.170966 seconds
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:33 [gpu_worker.py:298] Available KV cache memory: 12.47 GiB
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:33 [gpu_worker.py:298] Available KV cache memory: 12.47 GiB
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:33 [kv_cache_utils.py:1087] GPU KV cache size: 204,256 tokens
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:33 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 15.88x
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:33 [kv_cache_utils.py:1087] GPU KV cache size: 204,256 tokens
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:33 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 15.88x
[1;36m(Worker_TP0 pid=4009075)[0;0m WARNING 12-28 18:21:33 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=4009076)[0;0m WARNING 12-28 18:21:33 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:33 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.38 seconds
[1;36m(EngineCore_DP0 pid=4008796)[0;0m INFO 12-28 18:21:34 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:21:34 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:49 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=4009075)[0;0m INFO 12-28 18:21:49 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:49 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=4009076)[0;0m INFO 12-28 18:21:49 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:21:49', project_name='codecarbon', run_id='ba3331e1-45bd-4e6d-96d9-f6ac28c55084', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=54.87395233300049, emissions=0.0006779947038531267, emissions_rate=1.2355492451842027e-05, cpu_power=55.86694662000001, gpu_power=139.85800352411454, ram_power=70.0, cpu_energy=0.0006921897818324739, gpu_energy=0.0011356109084887933, ram_energy=0.001025844486236262, energy_consumed=0.002853645176557529, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 2/30 completed. Throughput: 449.75, Energy: 1.669612. Pareto size: 2.
INFO 12-28 18:21:56 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 48, 'enable_prefix_caching': True, 'max_num_batched_tokens': 11951, 'max_num_seqs': 106, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:21:57 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:21:57 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:21:57 [model.py:1510] Using max model len 32768
INFO 12-28 18:21:57 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=11951.
INFO 12-28 18:21:57 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:22:03 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:06 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:06 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4010614)[0;0m WARNING 12-28 18:22:06 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_85cea17f'), local_subscribe_addr='ipc:///tmp/97e93258-dc22-4a13-9d62-da741c123346', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:22:14 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:22:14 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:22:14 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:22:14 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:22:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e3cbd518'), local_subscribe_addr='ipc:///tmp/dc2c9b87-b227-4823-8264-884ba0187b09', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:22:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_17bce5ce'), local_subscribe_addr='ipc:///tmp/61da4389-a088-432e-8c83-0989d88d5c3c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:22:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_75a1f6b8'), local_subscribe_addr='ipc:///tmp/6aa7f33d-acaa-4ae4-ac6e-c25bd76b567a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:22:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_08da5a2b'), local_subscribe_addr='ipc:///tmp/0e4c5019-71a3-4753-aa2e-3605523ae4f8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:22:22 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:22 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:22 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:22 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:22 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:22:22 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:22:22 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:22:22 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:22:23 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:22:23 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:22:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_826f7879'), local_subscribe_addr='ipc:///tmp/a123fba5-4da0-4720-996f-f3a6c1f0c0f5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING 12-28 18:22:23 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:22:23 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:22:23 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_bb327523'), local_subscribe_addr='ipc:///tmp/0d67eb80-3328-4987-aa82-31b09a9695cc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:22:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:23 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:22:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:23 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:22:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:23 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:22:23 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:22:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:23 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:22:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:23 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:22:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:23 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:22:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:22:23 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:22:23 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 18:22:23 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:22:23 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
INFO 12-28 18:22:23 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
WARNING 12-28 18:22:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:22:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:22:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:22:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:24 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m INFO 12-28 18:22:24 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:24 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:24 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:24 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m INFO 12-28 18:22:24 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:24 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:24 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:24 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m INFO 12-28 18:22:24 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:24 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:24 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:25 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:25 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m INFO 12-28 18:22:25 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:25 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:29 [default_loader.py:267] Loading weights took 3.90 seconds
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:30 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 5.044530 seconds
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:30 [default_loader.py:267] Loading weights took 3.92 seconds
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m INFO 12-28 18:22:30 [default_loader.py:267] Loading weights took 3.91 seconds
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:30 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 5.647469 seconds
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:30 [default_loader.py:267] Loading weights took 3.91 seconds
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m INFO 12-28 18:22:31 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 6.142551 seconds
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:31 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 6.557044 seconds
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:33 [gpu_worker.py:298] Available KV cache memory: 15.54 GiB
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m INFO 12-28 18:22:33 [gpu_worker.py:298] Available KV cache memory: 15.54 GiB
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:34 [gpu_worker.py:298] Available KV cache memory: 15.65 GiB
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:35 [gpu_worker.py:298] Available KV cache memory: 15.65 GiB
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [kv_cache_utils.py:1087] GPU KV cache size: 512,928 tokens
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.80x
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [kv_cache_utils.py:1087] GPU KV cache size: 512,928 tokens
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.80x
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [kv_cache_utils.py:1087] GPU KV cache size: 509,376 tokens
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.58x
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [kv_cache_utils.py:1087] GPU KV cache size: 509,376 tokens
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.58x
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m WARNING 12-28 18:22:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m WARNING 12-28 18:22:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:35 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.86 seconds
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:36 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=4010614)[0;0m INFO 12-28 18:22:36 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:22:36 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m WARNING 12-28 18:22:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m WARNING 12-28 18:22:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:51 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP0 pid=4011272)[0;0m INFO 12-28 18:22:51 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:51 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP1 pid=4011273)[0;0m INFO 12-28 18:22:51 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:51 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP0 pid=4011274)[0;0m INFO 12-28 18:22:51 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP1 pid=4011275)[0;0m INFO 12-28 18:22:51 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:22:51', project_name='codecarbon', run_id='06b0bd61-664f-4326-96b0-b1436b91c21f', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=55.84967394103296, emissions=0.0009276837839533686, emissions_rate=1.6610370634085223e-05, cpu_power=55.64995748727274, gpu_power=254.46040097218165, ram_power=70.0, cpu_energy=0.0007647077073111668, gpu_energy=0.002094941120396321, ram_energy=0.0010449249697047182, energy_consumed=0.003904573797412206, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 3/30 completed. Throughput: 479.49, Energy: 1.993542. Pareto size: 3.
Skipping trial: 6 GPUs required, only 4 available.
   BO Iteration 4/30 completed. Throughput: 0.00, Energy: 1000000000.000000. Pareto size: 3.
INFO 12-28 18:23:01 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': True, 'max_num_batched_tokens': 7398, 'max_num_seqs': 73, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:23:01 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:23:01 [model.py:1510] Using max model len 32768
INFO 12-28 18:23:01 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=7398.
INFO 12-28 18:23:01 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:23:08 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:11 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:11 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4013249)[0;0m WARNING 12-28 18:23:11 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_40048189'), local_subscribe_addr='ipc:///tmp/2e6e6f4a-ef97-40bc-b9fa-8e67c3329203', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:23:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:23:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:23:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:23:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_71890b16'), local_subscribe_addr='ipc:///tmp/4caa6484-afab-4a78-936b-455e68a70986', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:23:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b43ad474'), local_subscribe_addr='ipc:///tmp/07dc3dd7-7cd2-4e40-8caf-ad14494bfe4b', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:23:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6ffb79fd'), local_subscribe_addr='ipc:///tmp/ad360da5-2c6f-4adf-bc5d-9413b49ec656', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:23:26 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:23:26 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:23:26 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:23:26 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:23:26 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:23:26 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:23:26 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:23:26 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:23:26 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
WARNING 12-28 18:23:26 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:23:26 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:23:26 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:26 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:26 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:26 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:27 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:27 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:27 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:27 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:27 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:27 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:27 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:27 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:27 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:27 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:27 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:27 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:28 [default_loader.py:267] Loading weights took 0.78 seconds
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:28 [default_loader.py:267] Loading weights took 0.74 seconds
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:28 [default_loader.py:267] Loading weights took 0.68 seconds
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:29 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.426559 seconds
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:29 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.659023 seconds
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:29 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.832574 seconds
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:31 [gpu_worker.py:298] Available KV cache memory: 14.49 GiB
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:31 [gpu_worker.py:298] Available KV cache memory: 14.31 GiB
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:31 [gpu_worker.py:298] Available KV cache memory: 14.65 GiB
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:31 [kv_cache_utils.py:1087] GPU KV cache size: 340,880 tokens
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:31 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.39x
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:31 [kv_cache_utils.py:1087] GPU KV cache size: 345,360 tokens
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:31 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.77x
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:31 [kv_cache_utils.py:1087] GPU KV cache size: 384,080 tokens
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:31 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 33.11x
[1;36m(Worker_PP2 pid=4013480)[0;0m WARNING 12-28 18:23:31 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:31 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.14 seconds
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:32 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4013249)[0;0m INFO 12-28 18:23:32 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:23:32 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4013478)[0;0m WARNING 12-28 18:23:32 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4013479)[0;0m WARNING 12-28 18:23:32 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:57 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4013478)[0;0m INFO 12-28 18:23:57 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:57 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=4013480)[0;0m INFO 12-28 18:23:57 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4013479)[0;0m INFO 12-28 18:23:57 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:23:57', project_name='codecarbon', run_id='f0be34a3-d21f-45b4-aaab-1d2068e336d0', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=57.006608914001845, emissions=0.000896864314901017, emissions_rate=1.5732637530746562e-05, cpu_power=59.436564960000005, gpu_power=199.77713152446486, ram_power=70.0, cpu_energy=0.000736375699475872, gpu_energy=0.001971425188250109, ram_energy=0.0010670553341572589, energy_consumed=0.0037748562218832397, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 5/30 completed. Throughput: 441.36, Energy: 1.270283. Pareto size: 3.
INFO 12-28 18:24:05 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 2, 'block_size': 48, 'enable_prefix_caching': False, 'max_num_batched_tokens': 6104, 'max_num_seqs': 175, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:24:05 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:24:05 [model.py:1510] Using max model len 32768
INFO 12-28 18:24:05 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6104.
INFO 12-28 18:24:05 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:24:12 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:15 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:15 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4014722)[0;0m WARNING 12-28 18:24:15 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_833a08a7'), local_subscribe_addr='ipc:///tmp/148786e2-dff1-4882-96f7-bbc787d2444f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:24:22 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:24:22 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:24:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2cc4ee9f'), local_subscribe_addr='ipc:///tmp/af49f2a5-a83c-410e-9210-19e68935795a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:24:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b5d37446'), local_subscribe_addr='ipc:///tmp/d0ecd57c-a5de-4715-9ab0-bc78fca1df62', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:24:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:24:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:24:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:24:29 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:24:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:24:29 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:24:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_dd0ee48a'), local_subscribe_addr='ipc:///tmp/d9ec2845-ccda-4f2f-8d59-28fe12f309cf', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:24:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:24:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:24:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:24:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:24:29 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:24:29 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 18:24:29 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:24:29 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:30 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:30 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:30 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:30 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:30 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:30 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:33 [default_loader.py:267] Loading weights took 2.17 seconds
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:33 [default_loader.py:267] Loading weights took 2.08 seconds
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:33 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 3.262710 seconds
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:34 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 3.621855 seconds
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:38 [gpu_worker.py:298] Available KV cache memory: 12.63 GiB
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:38 [gpu_worker.py:298] Available KV cache memory: 12.63 GiB
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:38 [kv_cache_utils.py:1087] GPU KV cache size: 206,928 tokens
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:38 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 20.14x
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:38 [kv_cache_utils.py:1087] GPU KV cache size: 206,928 tokens
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:38 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 20.14x
[1;36m(Worker_TP0 pid=4014961)[0;0m WARNING 12-28 18:24:38 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=4014962)[0;0m WARNING 12-28 18:24:38 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:38 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.34 seconds
[1;36m(EngineCore_DP0 pid=4014722)[0;0m INFO 12-28 18:24:39 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:24:39 [llm.py:306] Supported_tasks: ['generate']
WARNING 12-28 18:24:45 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:54 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=4014961)[0;0m INFO 12-28 18:24:54 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:54 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=4014962)[0;0m INFO 12-28 18:24:54 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:24:53', project_name='codecarbon', run_id='adf4b195-3d39-4ec5-9b44-7abab6a61db8', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=49.11375144997146, emissions=0.000606644114624492, emissions_rate=1.2351817906690256e-05, cpu_power=58.91156409000001, gpu_power=128.93415125697933, ram_power=70.0, cpu_energy=0.0005998211291937499, gpu_energy=0.001039506109380639, ram_energy=0.0009140069419831789, energy_consumed=0.002553334180557568, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 6/30 completed. Throughput: 474.09, Energy: 1.421810. Pareto size: 3.
Skipping trial: 6 GPUs required, only 4 available.
   BO Iteration 7/30 completed. Throughput: 0.00, Energy: 1000000000.000000. Pareto size: 2.
INFO 12-28 18:25:07 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 96, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 120, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:25:07 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:25:07 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:25:07 [model.py:1510] Using max model len 32768
INFO 12-28 18:25:07 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 18:25:07 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:25:14 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:17 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:17 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4016452)[0;0m WARNING 12-28 18:25:17 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_24fd26f7'), local_subscribe_addr='ipc:///tmp/3a90e719-a1ed-4d18-9431-3e1fb3d89cc7', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:25:25 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:25:25 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:25:25 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:25:30 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:25:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cc9907a9'), local_subscribe_addr='ipc:///tmp/ee91d8ed-cf1c-4814-beeb-223685f64a07', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:25:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c781dc8e'), local_subscribe_addr='ipc:///tmp/5e25a5a8-365c-4f9c-8b6c-ace0c2a66841', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:25:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_15a15cd3'), local_subscribe_addr='ipc:///tmp/6f5b0a87-aa2d-401d-b8b1-bf520a66df6f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:25:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e840fa2b'), local_subscribe_addr='ipc:///tmp/572a15f3-a7a0-4eb0-bccc-d61ac912a022', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:25:35 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:25:35 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:25:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_cf0505af'), local_subscribe_addr='ipc:///tmp/43816a9c-bf6f-422a-b570-7b0189c8fa4b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING 12-28 18:25:35 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:25:35 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:25:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_59cce06c'), local_subscribe_addr='ipc:///tmp/cfbc7e12-20d1-4b19-97c8-51970c54a102', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:25:35 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:25:35 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
INFO 12-28 18:25:35 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
INFO 12-28 18:25:35 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:25:35 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 18:25:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:25:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:25:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:25:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:36 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m INFO 12-28 18:25:36 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m INFO 12-28 18:25:36 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:36 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:36 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:36 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m INFO 12-28 18:25:36 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m INFO 12-28 18:25:36 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:36 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m INFO 12-28 18:25:36 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m INFO 12-28 18:25:36 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:36 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m INFO 12-28 18:25:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m INFO 12-28 18:25:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:40 [default_loader.py:267] Loading weights took 3.16 seconds
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:40 [default_loader.py:267] Loading weights took 3.17 seconds
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m INFO 12-28 18:25:40 [default_loader.py:267] Loading weights took 3.15 seconds
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:40 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 3.724461 seconds
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m INFO 12-28 18:25:41 [default_loader.py:267] Loading weights took 3.15 seconds
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:41 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 4.016568 seconds
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m INFO 12-28 18:25:41 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 4.223353 seconds
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m INFO 12-28 18:25:41 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 4.522116 seconds
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m INFO 12-28 18:25:43 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m INFO 12-28 18:25:43 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:44 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:44 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [kv_cache_utils.py:1087] GPU KV cache size: 512,256 tokens
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.02x
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [kv_cache_utils.py:1087] GPU KV cache size: 512,256 tokens
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.02x
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [kv_cache_utils.py:1087] GPU KV cache size: 508,608 tokens
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.80x
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [kv_cache_utils.py:1087] GPU KV cache size: 508,608 tokens
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.80x
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m WARNING 12-28 18:25:45 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m WARNING 12-28 18:25:45 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.63 seconds
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=4016452)[0;0m INFO 12-28 18:25:45 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:25:46 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m WARNING 12-28 18:25:46 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m WARNING 12-28 18:25:46 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:59 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP0 pid=4017345)[0;0m INFO 12-28 18:25:59 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:59 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP1 pid=4017346)[0;0m INFO 12-28 18:25:59 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP0 pid=4017347)[0;0m INFO 12-28 18:25:59 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP1 pid=4017348)[0;0m INFO 12-28 18:25:59 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:25:59', project_name='codecarbon', run_id='81406c18-7e5e-42e4-b11b-2041c5986f52', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=52.94472911802586, emissions=0.000829131955813316, emissions_rate=1.5660330492294934e-05, cpu_power=30.475879860000003, gpu_power=271.15331120850476, ram_power=70.0, cpu_energy=0.000511658170478153, gpu_energy=0.001989917425266441, ram_energy=0.0009881985765163943, energy_consumed=0.003489774172260988, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 8/30 completed. Throughput: 539.39, Energy: 1.781760. Pareto size: 1.
Skipping trial: 6 GPUs required, only 4 available.
   BO Iteration 9/30 completed. Throughput: 0.00, Energy: 1000000000.000000. Pareto size: 2.
INFO 12-28 18:26:09 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'tensor_parallel_size': 2, 'block_size': 48, 'enable_prefix_caching': False, 'max_num_batched_tokens': 10461, 'max_num_seqs': 157, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:26:09 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:26:09 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:26:09 [model.py:1510] Using max model len 32768
INFO 12-28 18:26:09 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=10461.
INFO 12-28 18:26:09 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:26:18 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:21 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:21 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4020331)[0;0m WARNING 12-28 18:26:21 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_f0331757'), local_subscribe_addr='ipc:///tmp/2adf9203-6371-45be-a09b-5ff5d53541e0', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:26:27 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:26:27 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:26:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a1eb354d'), local_subscribe_addr='ipc:///tmp/55b555e2-c1f1-4731-a0c8-ee8e2584aa22', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:26:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c20491df'), local_subscribe_addr='ipc:///tmp/06bea8ab-1ae3-4fd8-8f84-4c063027667e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:26:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:26:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:26:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:26:34 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:26:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:26:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:26:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_297ec07d'), local_subscribe_addr='ipc:///tmp/62bd13ca-75f2-42b8-87d8-781aec93d544', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:26:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:26:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:26:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:26:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:26:34 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 18:26:34 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 18:26:35 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:26:35 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=4020595)[0;0m INFO 12-28 18:26:35 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:26:35 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:26:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=4020595)[0;0m INFO 12-28 18:26:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:26:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4020595)[0;0m INFO 12-28 18:26:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:26:35 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=4020595)[0;0m INFO 12-28 18:26:35 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=4020595)[0;0m INFO 12-28 18:26:42 [default_loader.py:267] Loading weights took 6.06 seconds
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:26:42 [default_loader.py:267] Loading weights took 6.63 seconds
[1;36m(Worker_TP1 pid=4020595)[0;0m INFO 12-28 18:26:43 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 7.004124 seconds
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:26:43 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 7.342727 seconds
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:26:47 [gpu_worker.py:298] Available KV cache memory: 12.36 GiB
[1;36m(Worker_TP1 pid=4020595)[0;0m INFO 12-28 18:26:47 [gpu_worker.py:298] Available KV cache memory: 12.36 GiB
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:47 [kv_cache_utils.py:1087] GPU KV cache size: 202,416 tokens
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:47 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 13.83x
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:47 [kv_cache_utils.py:1087] GPU KV cache size: 202,416 tokens
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:47 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 13.83x
[1;36m(Worker_TP0 pid=4020594)[0;0m WARNING 12-28 18:26:47 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=4020595)[0;0m WARNING 12-28 18:26:47 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:47 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.36 seconds
[1;36m(EngineCore_DP0 pid=4020331)[0;0m INFO 12-28 18:26:48 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:26:48 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:27:03 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=4020594)[0;0m INFO 12-28 18:27:03 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=4020595)[0;0m INFO 12-28 18:27:03 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:27:02', project_name='codecarbon', run_id='3a3cb6d2-2436-407e-aa70-437174e1b80d', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=54.1688892490929, emissions=0.0006108717292979101, emissions_rate=1.1277169197412324e-05, cpu_power=30.048123423000003, gpu_power=139.52076290919607, ram_power=70.0, cpu_energy=0.0004344940201361265, gpu_energy=0.001124284510539475, ram_energy=0.0010123494647194297, energy_consumed=0.002571127995395031, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 10/30 completed. Throughput: 502.64, Energy: 1.355802. Pareto size: 2.
INFO 12-28 18:27:10 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 11397, 'max_num_seqs': 117, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:27:11 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:27:11 [model.py:1510] Using max model len 32768
INFO 12-28 18:27:11 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=11397.
INFO 12-28 18:27:11 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:27:19 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:22 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:22 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4021740)[0;0m WARNING 12-28 18:27:22 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_9d38cc3d'), local_subscribe_addr='ipc:///tmp/9d5e9bc9-384e-46aa-abeb-b3378a400b27', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:27:29 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:27:29 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:27:30 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:27:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2a3db685'), local_subscribe_addr='ipc:///tmp/260ea385-52c9-4596-aad7-75a12f2f8b23', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:27:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8024f398'), local_subscribe_addr='ipc:///tmp/51db4340-bdcd-4f48-82d3-bec500b7fb67', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:27:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_543e1f7a'), local_subscribe_addr='ipc:///tmp/adce3f04-950d-4af8-94d5-ad86bf0da274', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:27:36 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:27:36 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:27:36 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:27:36 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:27:36 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:27:36 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:27:36 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:27:36 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:27:36 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
WARNING 12-28 18:27:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:27:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:27:37 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:27:37 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:27:37 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:27:37 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:27:37 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:27:37 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:27:37 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:27:37 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:27:37 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:27:37 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:27:37 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:27:37 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:27:37 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:27:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:27:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:27:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:27:39 [default_loader.py:267] Loading weights took 0.63 seconds
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:27:39 [default_loader.py:267] Loading weights took 0.64 seconds
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:27:39 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.716442 seconds
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:27:40 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 2.137225 seconds
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:27:40 [default_loader.py:267] Loading weights took 0.69 seconds
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:27:41 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 2.994282 seconds
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:27:42 [gpu_worker.py:298] Available KV cache memory: 14.05 GiB
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:27:42 [gpu_worker.py:298] Available KV cache memory: 13.89 GiB
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:27:42 [gpu_worker.py:298] Available KV cache memory: 14.21 GiB
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:43 [kv_cache_utils.py:1087] GPU KV cache size: 330,960 tokens
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 21.22x
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:43 [kv_cache_utils.py:1087] GPU KV cache size: 334,720 tokens
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 21.46x
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:43 [kv_cache_utils.py:1087] GPU KV cache size: 372,400 tokens
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 23.87x
[1;36m(Worker_PP2 pid=4021981)[0;0m WARNING 12-28 18:27:43 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:43 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.28 seconds
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:44 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4021740)[0;0m INFO 12-28 18:27:44 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:27:44 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4021979)[0;0m WARNING 12-28 18:27:44 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4021980)[0;0m WARNING 12-28 18:27:44 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:28:06 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4021979)[0;0m INFO 12-28 18:28:06 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:28:06 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4021980)[0;0m INFO 12-28 18:28:06 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4021981)[0;0m INFO 12-28 18:28:06 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:28:06', project_name='codecarbon', run_id='493a007a-b0e6-4082-b7b7-b1cff4c78496', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=56.027803339995444, emissions=0.0007668653626402366, emissions_rate=1.3687228785084454e-05, cpu_power=30.071903135454548, gpu_power=179.39266377230143, ram_power=70.0, cpu_energy=0.00045041614372275514, gpu_energy=0.0017289749942897004, ram_energy=0.0010483061149758417, energy_consumed=0.0032276972529882973, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 11/30 completed. Throughput: 355.22, Energy: 1.547644. Pareto size: 2.
INFO 12-28 18:28:15 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'tensor_parallel_size': 2, 'block_size': 80, 'enable_prefix_caching': True, 'max_num_batched_tokens': 11655, 'max_num_seqs': 84, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:28:15 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:28:15 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:28:15 [model.py:1510] Using max model len 32768
INFO 12-28 18:28:15 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=11655.
INFO 12-28 18:28:15 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:28:23 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:26 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:26 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4023238)[0;0m WARNING 12-28 18:28:26 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_222fa4f1'), local_subscribe_addr='ipc:///tmp/863ae154-b18c-4708-9824-da1660875f4b', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:28:33 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:28:33 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:28:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_38336dc8'), local_subscribe_addr='ipc:///tmp/e646ec22-8a5f-41ee-9d19-83eb38b91d3a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:28:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6cd523ad'), local_subscribe_addr='ipc:///tmp/f49c0acf-e7d3-43a5-8ff1-3d785e1b849d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:28:39 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:28:39 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:28:39 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:28:39 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:28:39 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:28:39 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:28:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_971cf540'), local_subscribe_addr='ipc:///tmp/36f08bd8-a283-4c32-940c-c727a6c42f6e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:28:39 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:28:39 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:28:39 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:28:39 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:28:39 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:28:39 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 18:28:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:28:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:28:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:28:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:28:40 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:28:40 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:28:40 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:28:40 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:28:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:28:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:28:46 [default_loader.py:267] Loading weights took 5.75 seconds
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:28:47 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 6.851439 seconds
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:28:48 [default_loader.py:267] Loading weights took 6.34 seconds
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:28:48 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 7.850020 seconds
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:28:52 [gpu_worker.py:298] Available KV cache memory: 12.28 GiB
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:28:52 [gpu_worker.py:298] Available KV cache memory: 12.28 GiB
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:52 [kv_cache_utils.py:1087] GPU KV cache size: 201,200 tokens
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:52 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.70x
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:52 [kv_cache_utils.py:1087] GPU KV cache size: 201,200 tokens
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:52 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.70x
[1;36m(Worker_TP0 pid=4023516)[0;0m WARNING 12-28 18:28:52 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=4023517)[0;0m WARNING 12-28 18:28:52 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:53 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.50 seconds
[1;36m(EngineCore_DP0 pid=4023238)[0;0m INFO 12-28 18:28:54 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:28:54 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:29:08 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=4023516)[0;0m INFO 12-28 18:29:08 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:29:08 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=4023517)[0;0m INFO 12-28 18:29:08 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:29:08', project_name='codecarbon', run_id='1cd10b96-abfb-4212-9967-d200a32f2268', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=53.85476749704685, emissions=0.0006087704402739339, emissions_rate=1.1303928483347664e-05, cpu_power=30.045060810000003, gpu_power=138.1716041981345, ram_power=70.0, cpu_energy=0.00043164132876804206, gpu_energy=0.001124357288375677, ram_energy=0.001006285159773284, energy_consumed=0.0025622837769170032, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 12/30 completed. Throughput: 467.01, Energy: 1.436347. Pareto size: 2.
INFO 12-28 18:29:17 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 71, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:29:18 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:29:18 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:29:18 [model.py:1510] Using max model len 32768
INFO 12-28 18:29:18 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 18:29:18 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:29:26 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:29 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:29 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4024727)[0;0m WARNING 12-28 18:29:29 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_a41e8914'), local_subscribe_addr='ipc:///tmp/e55c9d7b-a24b-4211-8044-99516352fac8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:29:35 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:29:35 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:29:35 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:29:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b4deb088'), local_subscribe_addr='ipc:///tmp/d8e94ab2-1347-4931-b68f-fa0d69c16a25', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:29:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_12b13eb0'), local_subscribe_addr='ipc:///tmp/a934aa32-f8bc-45dc-8656-cd2ce07b634f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:29:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a8fcdbae'), local_subscribe_addr='ipc:///tmp/984f6fad-0b49-424f-b7e4-0f518a199f9e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:29:41 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:29:41 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:29:41 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:29:41 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:29:41 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:29:41 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:29:41 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:29:41 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:29:41 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
WARNING 12-28 18:29:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:29:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:29:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:29:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:29:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:29:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:29:42 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:29:42 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:29:42 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:29:42 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:29:42 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:29:42 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:29:42 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:29:42 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:29:42 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:29:42 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:29:42 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:29:42 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:29:46 [default_loader.py:267] Loading weights took 4.35 seconds
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:29:47 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 4.941693 seconds
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:29:47 [default_loader.py:267] Loading weights took 5.01 seconds
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:29:48 [default_loader.py:267] Loading weights took 4.88 seconds
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:29:48 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 5.787740 seconds
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:29:48 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 5.923570 seconds
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:29:50 [gpu_worker.py:298] Available KV cache memory: 13.80 GiB
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:29:50 [gpu_worker.py:298] Available KV cache memory: 13.95 GiB
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:29:50 [gpu_worker.py:298] Available KV cache memory: 14.11 GiB
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:50 [kv_cache_utils.py:1087] GPU KV cache size: 328,928 tokens
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:50 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 20.04x
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:50 [kv_cache_utils.py:1087] GPU KV cache size: 332,512 tokens
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:50 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 20.26x
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:50 [kv_cache_utils.py:1087] GPU KV cache size: 369,952 tokens
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:50 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 22.54x
[1;36m(Worker_PP2 pid=4024992)[0;0m WARNING 12-28 18:29:50 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:50 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.26 seconds
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:51 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4024727)[0;0m INFO 12-28 18:29:51 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:29:52 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4024990)[0;0m WARNING 12-28 18:29:52 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4024991)[0;0m WARNING 12-28 18:29:52 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:30:15 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4024990)[0;0m INFO 12-28 18:30:15 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:30:15 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4024991)[0;0m INFO 12-28 18:30:15 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4024992)[0;0m INFO 12-28 18:30:15 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:30:15', project_name='codecarbon', run_id='ee8adf3d-b86c-4b3b-8839-5b79b74ac08f', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=58.154686533962376, emissions=0.0007961424226644895, emissions_rate=1.3690081919701204e-05, cpu_power=30.087361335, gpu_power=177.5333655673282, ram_power=70.0, cpu_energy=0.0004680584455087561, gpu_energy=0.001793058656669011, ram_energy=0.0010898058052989654, energy_consumed=0.0033509229074767327, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 13/30 completed. Throughput: 304.73, Energy: 1.740739. Pareto size: 2.
INFO 12-28 18:30:24 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 2, 'block_size': 112, 'enable_prefix_caching': False, 'max_num_batched_tokens': 7747, 'max_num_seqs': 162, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:30:24 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:30:24 [model.py:1510] Using max model len 32768
INFO 12-28 18:30:24 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=7747.
INFO 12-28 18:30:24 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:30:32 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:35 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4027734)[0;0m WARNING 12-28 18:30:35 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_37c8706b'), local_subscribe_addr='ipc:///tmp/4477dd25-0af3-44fe-acc1-3c41ef3ba7c9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:30:41 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:30:42 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:30:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4533c129'), local_subscribe_addr='ipc:///tmp/1dd00136-5197-408f-aea1-1fe03f993617', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:30:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_947b446a'), local_subscribe_addr='ipc:///tmp/5c454f3b-e025-466c-94ad-fe575d555ddf', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:30:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:30:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:30:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:30:48 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:30:48 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:30:48 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:30:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_d7ae51d2'), local_subscribe_addr='ipc:///tmp/3e653781-9b7b-4863-87f0-a034e7dd7499', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:30:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:30:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:30:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:30:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:30:48 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:30:48 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 18:30:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:30:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:30:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:30:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:30:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:30:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:30:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:30:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:30:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:30:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:30:52 [default_loader.py:267] Loading weights took 2.20 seconds
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:30:52 [default_loader.py:267] Loading weights took 2.25 seconds
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:30:53 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 3.390476 seconds
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:30:53 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 3.923024 seconds
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:30:57 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:30:57 [gpu_worker.py:298] Available KV cache memory: 12.53 GiB
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:57 [kv_cache_utils.py:1087] GPU KV cache size: 205,184 tokens
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:57 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 17.12x
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:57 [kv_cache_utils.py:1087] GPU KV cache size: 205,184 tokens
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:57 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 17.12x
[1;36m(Worker_TP1 pid=4028124)[0;0m WARNING 12-28 18:30:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=4028123)[0;0m WARNING 12-28 18:30:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:58 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.37 seconds
[1;36m(EngineCore_DP0 pid=4027734)[0;0m INFO 12-28 18:30:58 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:30:58 [llm.py:306] Supported_tasks: ['generate']
WARNING 12-28 18:31:03 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:31:04 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:31:12 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=4028123)[0;0m INFO 12-28 18:31:12 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:31:12 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=4028124)[0;0m INFO 12-28 18:31:12 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:31:12', project_name='codecarbon', run_id='e91aa8e9-372c-495d-9ca2-15fd2194e1c9', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=49.17599216999952, emissions=0.0005580937306500883, emissions_rate=1.1348906367171602e-05, cpu_power=30.034244477999998, gpu_power=129.7184819873306, ram_power=70.0, cpu_energy=0.000392663243012571, gpu_energy=0.001041085832867772, ram_energy=0.0009152390113210359, energy_consumed=0.0023489880872013787, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 14/30 completed. Throughput: 488.80, Energy: 1.272207. Pareto size: 2.
INFO 12-28 18:31:22 [utils.py:233] non-default args: {'seed': 42, 'block_size': 48, 'enable_prefix_caching': True, 'max_num_batched_tokens': 6596, 'max_num_seqs': 91, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:31:22 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:31:22 [model.py:1510] Using max model len 32768
INFO 12-28 18:31:22 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6596.
INFO 12-28 18:31:22 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:31:29 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:32 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:32 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:34 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4029187)[0;0m WARNING 12-28 18:31:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:34 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:34 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:34 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:35 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:37 [default_loader.py:267] Loading weights took 1.69 seconds
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:38 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.831533 seconds
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:40 [gpu_worker.py:298] Available KV cache memory: 5.61 GiB
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:40 [kv_cache_utils.py:1087] GPU KV cache size: 45,936 tokens
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:40 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 4.27x
[1;36m(EngineCore_DP0 pid=4029187)[0;0m WARNING 12-28 18:31:40 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:41 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.89 seconds
[1;36m(EngineCore_DP0 pid=4029187)[0;0m INFO 12-28 18:31:41 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:31:42 [llm.py:306] Supported_tasks: ['generate']
EmissionsData(timestamp='2025-12-28T18:32:06', project_name='codecarbon', run_id='50712967-c300-4888-ba03-d9c53988909e', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=45.221898116054945, emissions=0.00044680628039895775, emissions_rate=9.880307970538945e-06, cpu_power=30.003418183125007, gpu_power=70.36287778055174, ram_power=70.0, cpu_energy=0.0003642245883638225, gpu_energy=0.0006678210898121151, ram_energy=0.0008485391990656758, energy_consumed=0.0018805848772416136, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 15/30 completed. Throughput: 307.22, Energy: 0.912290. Pareto size: 3.
INFO 12-28 18:32:13 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'pipeline_parallel_size': 2, 'block_size': 64, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 85, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:32:13 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:32:13 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:32:13 [model.py:1510] Using max model len 32768
INFO 12-28 18:32:13 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 18:32:13 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:32:22 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:25 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:25 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4030294)[0;0m WARNING 12-28 18:32:25 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_a96942df'), local_subscribe_addr='ipc:///tmp/319265f4-97cd-4026-a441-b53cb10164db', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:32:31 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:32:31 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:32:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3e151ab1'), local_subscribe_addr='ipc:///tmp/4907bb97-7429-4233-8b82-2200a7ea1bf1', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:32:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8394cbee'), local_subscribe_addr='ipc:///tmp/494c6b87-b987-44cb-8cf1-4f8efc3828f3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:32:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:32:37 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:32:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:32:37 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:32:37 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:32:37 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 18:32:37 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:32:37 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4030579)[0;0m INFO 12-28 18:32:37 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:32:37 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4030579)[0;0m INFO 12-28 18:32:38 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4030579)[0;0m INFO 12-28 18:32:38 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:32:38 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:32:38 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4030579)[0;0m INFO 12-28 18:32:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:32:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4030579)[0;0m INFO 12-28 18:32:46 [default_loader.py:267] Loading weights took 6.84 seconds
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:32:46 [default_loader.py:267] Loading weights took 6.68 seconds
[1;36m(Worker_PP1 pid=4030579)[0;0m INFO 12-28 18:32:46 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 7.933317 seconds
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:32:47 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 8.234588 seconds
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:32:49 [gpu_worker.py:298] Available KV cache memory: 11.77 GiB
[1;36m(Worker_PP1 pid=4030579)[0;0m INFO 12-28 18:32:49 [gpu_worker.py:298] Available KV cache memory: 11.68 GiB
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:49 [kv_cache_utils.py:1087] GPU KV cache size: 192,832 tokens
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:49 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 11.72x
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:49 [kv_cache_utils.py:1087] GPU KV cache size: 191,296 tokens
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:49 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 11.63x
[1;36m(Worker_PP1 pid=4030579)[0;0m WARNING 12-28 18:32:49 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:49 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.70 seconds
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:50 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=4030294)[0;0m INFO 12-28 18:32:50 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:32:50 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4030578)[0;0m WARNING 12-28 18:32:50 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
WARNING 12-28 18:32:56 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:32:57 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:01 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:02 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:04 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:05 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:06 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:06 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:09 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:09 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:10 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:11 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 18:33:13 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:33:14 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4030578)[0;0m INFO 12-28 18:33:14 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4030579)[0;0m INFO 12-28 18:33:14 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:33:14', project_name='codecarbon', run_id='20c75bde-0f60-4c7d-aa7f-5c1f0e4ab794', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=62.44281584699638, emissions=0.0007369343299532476, emissions_rate=1.1801747245975545e-05, cpu_power=30.031097520000003, gpu_power=106.3997147398111, ram_power=70.0, cpu_energy=0.0004986573469767262, gpu_energy=0.001440362541177187, ram_energy=0.0011626991727169502, energy_consumed=0.0031017190608708636, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 16/30 completed. Throughput: 338.03, Energy: 1.404905. Pareto size: 3.
INFO 12-28 18:33:23 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'tensor_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': False, 'max_num_batched_tokens': 6523, 'max_num_seqs': 185, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:33:24 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:33:24 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:33:24 [model.py:1510] Using max model len 32768
INFO 12-28 18:33:24 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6523.
INFO 12-28 18:33:24 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:33:32 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:33:35 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:33:35 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4032039)[0;0m WARNING 12-28 18:33:35 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:33:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_aeac5365'), local_subscribe_addr='ipc:///tmp/04d2e108-ecea-4e47-b639-8913717bd324', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:33:42 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:33:42 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:33:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f3e51bb6'), local_subscribe_addr='ipc:///tmp/5122516f-d5a3-4371-9c09-aeea763a219e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:33:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ff0e0455'), local_subscribe_addr='ipc:///tmp/6785fa8c-c2ec-4b8e-a26c-256298286665', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:33:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:33:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:33:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:33:48 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:33:48 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:33:48 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:33:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_2cbd2d96'), local_subscribe_addr='ipc:///tmp/6804c6dc-dd94-4f30-8da1-ec9eac6cde9b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:33:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:33:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:33:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:33:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:33:48 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:33:48 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 18:33:49 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:33:49 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=4032329)[0;0m INFO 12-28 18:33:49 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:33:49 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=4032329)[0;0m INFO 12-28 18:33:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:33:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=4032329)[0;0m INFO 12-28 18:33:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:33:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=4032329)[0;0m INFO 12-28 18:33:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:33:50 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=4032329)[0;0m INFO 12-28 18:33:56 [default_loader.py:267] Loading weights took 6.34 seconds
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:33:57 [default_loader.py:267] Loading weights took 6.67 seconds
[1;36m(Worker_TP1 pid=4032329)[0;0m INFO 12-28 18:33:57 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 7.115842 seconds
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:33:57 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 7.638576 seconds
[1;36m(Worker_TP1 pid=4032329)[0;0m INFO 12-28 18:34:01 [gpu_worker.py:298] Available KV cache memory: 12.61 GiB
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:34:01 [gpu_worker.py:298] Available KV cache memory: 12.61 GiB
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:34:01 [kv_cache_utils.py:1087] GPU KV cache size: 206,528 tokens
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:34:01 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 19.38x
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:34:01 [kv_cache_utils.py:1087] GPU KV cache size: 206,528 tokens
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:34:01 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 19.38x
[1;36m(Worker_TP0 pid=4032328)[0;0m WARNING 12-28 18:34:01 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=4032329)[0;0m WARNING 12-28 18:34:01 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:34:01 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.88 seconds
[1;36m(EngineCore_DP0 pid=4032039)[0;0m INFO 12-28 18:34:02 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:34:02 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:34:16 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=4032328)[0;0m INFO 12-28 18:34:16 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=4032329)[0;0m INFO 12-28 18:34:16 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:34:16', project_name='codecarbon', run_id='60c8423d-cffa-4cc5-9272-28c242738b52', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=53.64444247097708, emissions=0.0006033774991031573, emissions_rate=1.1247716842795019e-05, cpu_power=30.05115606, gpu_power=138.5195728040437, ram_power=70.0, cpu_energy=0.0004301414375435386, gpu_energy=0.0011073058858430684, ram_energy=0.001002137838818534, energy_consumed=0.0025395851622051413, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 17/30 completed. Throughput: 500.29, Energy: 1.335062. Pareto size: 2.
INFO 12-28 18:34:25 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 107, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:34:26 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:34:26 [model.py:1510] Using max model len 32768
INFO 12-28 18:34:26 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 18:34:26 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:34:34 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:37 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:37 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4033502)[0;0m WARNING 12-28 18:34:37 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_1f280667'), local_subscribe_addr='ipc:///tmp/4e712490-caec-45e0-952a-9c2683c30d5e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:34:43 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:34:43 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:34:43 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:34:44 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:34:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_30d28bd5'), local_subscribe_addr='ipc:///tmp/ecc0c07f-94fd-41fe-9488-31ef91e3b855', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:34:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b65a503c'), local_subscribe_addr='ipc:///tmp/d7a3f331-24ce-4002-a56c-db88153f9454', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:34:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6d8f18a5'), local_subscribe_addr='ipc:///tmp/b22c58f4-2a33-4da8-bef6-c6d67279f49a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:34:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_979eaaa6'), local_subscribe_addr='ipc:///tmp/4cd27615-32d1-425e-9a00-73c3e058e6a8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:34:50 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:50 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:50 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:34:50 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:50 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:34:50 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:50 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:34:50 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 18:34:50 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:34:50 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 18:34:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_1965f2b5'), local_subscribe_addr='ipc:///tmp/44b8908f-df35-4c69-9ce6-b7dfede3e7fd', remote_subscribe_addr=None, remote_addr_ipv6=False)
WARNING 12-28 18:34:50 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 18:34:50 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:34:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_6e82732f'), local_subscribe_addr='ipc:///tmp/db9ba966-971a-4ff7-80a9-cf353d486d86', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:34:50 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:50 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:34:50 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:50 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:34:50 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:50 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:34:50 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:50 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:34:51 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:51 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:34:51 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:51 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:34:51 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:51 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:34:51 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:34:51 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:34:51 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 18:34:51 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 18:34:51 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
INFO 12-28 18:34:51 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
WARNING 12-28 18:34:51 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:34:51 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:34:51 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:34:51 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:34:51 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:34:51 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:34:51 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m INFO 12-28 18:34:51 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:34:51 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:34:51 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:34:51 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:34:51 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m INFO 12-28 18:34:51 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:34:51 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:34:51 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m INFO 12-28 18:34:51 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:34:52 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:34:52 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:34:52 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m INFO 12-28 18:34:52 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:34:53 [default_loader.py:267] Loading weights took 0.94 seconds
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:34:54 [default_loader.py:267] Loading weights took 0.94 seconds
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:34:54 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.032848 seconds
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:34:54 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.475693 seconds
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m INFO 12-28 18:34:54 [default_loader.py:267] Loading weights took 1.12 seconds
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:34:55 [default_loader.py:267] Loading weights took 0.96 seconds
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m INFO 12-28 18:34:55 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 3.103003 seconds
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:34:55 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 3.467218 seconds
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m INFO 12-28 18:34:57 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:34:57 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:34:59 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:34:59 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [kv_cache_utils.py:1087] GPU KV cache size: 512,320 tokens
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.09x
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [kv_cache_utils.py:1087] GPU KV cache size: 512,320 tokens
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.09x
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [kv_cache_utils.py:1087] GPU KV cache size: 508,640 tokens
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.86x
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [kv_cache_utils.py:1087] GPU KV cache size: 508,640 tokens
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.86x
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m WARNING 12-28 18:34:59 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m WARNING 12-28 18:34:59 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:34:59 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.97 seconds
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:35:00 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=4033502)[0;0m INFO 12-28 18:35:00 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:35:00 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m WARNING 12-28 18:35:00 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m WARNING 12-28 18:35:00 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:35:14 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP0 pid=4033787)[0;0m INFO 12-28 18:35:14 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:35:14 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP1 pid=4033788)[0;0m INFO 12-28 18:35:14 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:35:14 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP1 pid=4033790)[0;0m INFO 12-28 18:35:14 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP0 pid=4033789)[0;0m INFO 12-28 18:35:14 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:35:14', project_name='codecarbon', run_id='a7d970be-c48b-41f9-b9d1-f41178aae09f', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=49.486132693942636, emissions=0.0007643955049416491, emissions_rate=1.544666077806511e-05, cpu_power=30.166690170000003, gpu_power=261.0324431765876, ram_power=70.0, cpu_energy=0.0003963036900892127, gpu_energy=0.001900080964509776, ram_energy=0.0009209170931319629, energy_consumed=0.003217301747730952, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 18/30 completed. Throughput: 536.25, Energy: 1.652016. Pareto size: 5.
INFO 12-28 18:35:24 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'pipeline_parallel_size': 4, 'block_size': 112, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8655, 'max_num_seqs': 142, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:35:24 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:35:24 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:35:24 [model.py:1510] Using max model len 32768
INFO 12-28 18:35:24 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8655.
INFO 12-28 18:35:24 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:35:31 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:34 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:34 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=4, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4036416)[0;0m WARNING 12-28 18:35:34 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_74050cae'), local_subscribe_addr='ipc:///tmp/7a50a152-0db3-4814-9a9b-c6b786223fdc', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:35:41 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:35:41 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:35:41 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:35:42 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:35:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e35b2ed1'), local_subscribe_addr='ipc:///tmp/36021f29-4c00-4e30-a8ea-847ec6f01233', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:35:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bd9f4b94'), local_subscribe_addr='ipc:///tmp/18b3f5b4-027f-4707-bced-21c683bf4dcd', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:35:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c981fc86'), local_subscribe_addr='ipc:///tmp/d76db8d7-5826-4870-8493-49f28096ecd8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:35:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5f0fb6f4'), local_subscribe_addr='ipc:///tmp/b282e31a-e457-42ab-9682-e3de89cfe174', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 18:35:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:35:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:35:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:35:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:35:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:35:48 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:35:48 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:35:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:35:48 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
INFO 12-28 18:35:48 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:35:48 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 3, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:35:48 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
WARNING 12-28 18:35:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:35:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:35:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:35:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP3 pid=4036857)[0;0m INFO 12-28 18:35:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:35:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:35:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:35:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP3 pid=4036857)[0;0m INFO 12-28 18:35:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:35:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP3 pid=4036857)[0;0m INFO 12-28 18:35:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:35:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:35:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:35:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:35:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:35:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP3 pid=4036857)[0;0m INFO 12-28 18:35:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:35:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:35:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:35:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:35:53 [default_loader.py:267] Loading weights took 3.33 seconds
[1;36m(Worker_PP3 pid=4036857)[0;0m INFO 12-28 18:35:53 [default_loader.py:267] Loading weights took 4.15 seconds
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:35:53 [gpu_model_runner.py:2653] Model loading took 3.5021 GiB and 4.136768 seconds
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:35:54 [default_loader.py:267] Loading weights took 3.87 seconds
[1;36m(Worker_PP3 pid=4036857)[0;0m INFO 12-28 18:35:54 [gpu_model_runner.py:2653] Model loading took 3.5021 GiB and 4.714474 seconds
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:35:54 [default_loader.py:267] Loading weights took 3.86 seconds
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:35:54 [gpu_model_runner.py:2653] Model loading took 3.2580 GiB and 5.025547 seconds
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:35:55 [gpu_model_runner.py:2653] Model loading took 3.2580 GiB and 5.264341 seconds
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:35:56 [gpu_worker.py:298] Available KV cache memory: 15.39 GiB
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:35:56 [gpu_worker.py:298] Available KV cache memory: 15.57 GiB
[1;36m(Worker_PP3 pid=4036857)[0;0m INFO 12-28 18:35:56 [gpu_worker.py:298] Available KV cache memory: 15.33 GiB
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:35:56 [gpu_worker.py:298] Available KV cache memory: 15.57 GiB
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [kv_cache_utils.py:1087] GPU KV cache size: 504,336 tokens
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 39.16x
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [kv_cache_utils.py:1087] GPU KV cache size: 510,272 tokens
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 39.62x
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [kv_cache_utils.py:1087] GPU KV cache size: 510,272 tokens
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 39.62x
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [kv_cache_utils.py:1087] GPU KV cache size: 502,208 tokens
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 38.99x
[1;36m(Worker_PP3 pid=4036857)[0;0m WARNING 12-28 18:35:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [core.py:210] init engine (profile, create kv cache, warmup model) took 1.99 seconds
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [core.py:149] Batch queue is enabled with size 4
[1;36m(EngineCore_DP0 pid=4036416)[0;0m INFO 12-28 18:35:57 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:35:57 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4036854)[0;0m WARNING 12-28 18:35:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4036855)[0;0m WARNING 12-28 18:35:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP2 pid=4036856)[0;0m WARNING 12-28 18:35:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:36:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:36:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4036854)[0;0m INFO 12-28 18:36:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4036855)[0;0m INFO 12-28 18:36:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:36:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=4036856)[0;0m INFO 12-28 18:36:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP3 pid=4036857)[0;0m INFO 12-28 18:36:18 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:36:18', project_name='codecarbon', run_id='3df343da-ca23-4ae2-a763-a3a6a27361a8', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=55.07972746505402, emissions=0.0008953133720778747, emissions_rate=1.625486205693223e-05, cpu_power=30.092170120909092, gpu_power=255.76469716849547, ram_power=70.0, cpu_energy=0.00044274243823359447, gpu_energy=0.0022956682254209326, ram_energy=0.0010299177188794906, energy_consumed=0.0037683283825340175, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 19/30 completed. Throughput: 466.25, Energy: 1.422458. Pareto size: 4.
INFO 12-28 18:36:27 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 4, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 148, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:36:28 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:36:28 [model.py:1510] Using max model len 32768
INFO 12-28 18:36:28 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 18:36:28 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:36:34 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:36:37 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:36:37 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=4, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4038172)[0;0m WARNING 12-28 18:36:37 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:36:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_bbca2c0a'), local_subscribe_addr='ipc:///tmp/4070838c-d70f-40e5-a196-301e2fde0252', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:36:44 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:36:44 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:36:44 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:36:44 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:36:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ce2ee6d0'), local_subscribe_addr='ipc:///tmp/db897f9b-1275-4dea-8504-96683aac3abc', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:36:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2c31afe2'), local_subscribe_addr='ipc:///tmp/8f3d8792-ac50-43b2-b98d-4256bdc6dc64', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:36:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b2ef9528'), local_subscribe_addr='ipc:///tmp/bea6db18-6d88-4513-ba0f-fefa579b135a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:36:54 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fa200384'), local_subscribe_addr='ipc:///tmp/189bf071-3c9c-41c0-bae6-3195afda9ea4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 18:36:54 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:36:54 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:36:54 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:36:54 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:36:54 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:36:54 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:36:54 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:36:54 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:36:55 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:36:55 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:36:55 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:36:55 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 3, TP rank 0, EP rank 0
WARNING 12-28 18:36:55 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:36:55 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:36:55 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:36:55 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:36:55 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:36:55 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:36:55 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:36:55 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:36:55 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:36:55 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:36:55 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:36:55 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:36:55 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:36:55 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:36:55 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:36:56 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:36:56 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:36:56 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:36:56 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:36:56 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:36:57 [default_loader.py:267] Loading weights took 0.47 seconds
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:36:57 [default_loader.py:267] Loading weights took 0.50 seconds
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:36:57 [gpu_model_runner.py:2653] Model loading took 3.5021 GiB and 1.529909 seconds
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:36:58 [default_loader.py:267] Loading weights took 0.44 seconds
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:36:58 [gpu_model_runner.py:2653] Model loading took 3.2580 GiB and 2.006220 seconds
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:36:58 [default_loader.py:267] Loading weights took 0.54 seconds
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:36:58 [gpu_model_runner.py:2653] Model loading took 3.2580 GiB and 2.481026 seconds
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:36:59 [gpu_model_runner.py:2653] Model loading took 3.5021 GiB and 3.038221 seconds
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:37:01 [gpu_worker.py:298] Available KV cache memory: 15.17 GiB
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:37:01 [gpu_worker.py:298] Available KV cache memory: 15.17 GiB
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:37:01 [gpu_worker.py:298] Available KV cache memory: 14.93 GiB
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:37:01 [gpu_worker.py:298] Available KV cache memory: 15.02 GiB
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [kv_cache_utils.py:1087] GPU KV cache size: 492,160 tokens
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.86x
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [kv_cache_utils.py:1087] GPU KV cache size: 497,120 tokens
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.17x
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [kv_cache_utils.py:1087] GPU KV cache size: 497,120 tokens
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.17x
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [kv_cache_utils.py:1087] GPU KV cache size: 489,040 tokens
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.67x
[1;36m(Worker_PP3 pid=4038437)[0;0m WARNING 12-28 18:37:01 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:01 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.22 seconds
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:02 [core.py:149] Batch queue is enabled with size 4
[1;36m(EngineCore_DP0 pid=4038172)[0;0m INFO 12-28 18:37:02 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:37:02 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4038434)[0;0m WARNING 12-28 18:37:02 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4038435)[0;0m WARNING 12-28 18:37:02 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP2 pid=4038436)[0;0m WARNING 12-28 18:37:02 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:37:24 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4038434)[0;0m INFO 12-28 18:37:24 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:37:24 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:37:24 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=4038436)[0;0m INFO 12-28 18:37:24 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4038435)[0;0m INFO 12-28 18:37:24 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:37:24 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP3 pid=4038437)[0;0m INFO 12-28 18:37:24 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:37:24', project_name='codecarbon', run_id='2f4899c9-cfd3-4a1a-b33a-ca6c3030ac4c', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=56.632852440932766, emissions=0.0008678885544048696, emissions_rate=1.5324825026429045e-05, cpu_power=30.113057955000002, gpu_power=214.35382195673924, ram_power=70.0, cpu_energy=0.00045554547358610557, gpu_energy=0.0021371422652691763, ram_energy=0.0010602109829073822, energy_consumed=0.0036528987217626643, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 20/30 completed. Throughput: 344.84, Energy: 1.817111. Pareto size: 4.
INFO 12-28 18:37:32 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9273, 'max_num_seqs': 154, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:37:32 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:37:32 [model.py:1510] Using max model len 32768
INFO 12-28 18:37:32 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9273.
INFO 12-28 18:37:32 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:37:40 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:37:43 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:37:43 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4039842)[0;0m WARNING 12-28 18:37:43 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:37:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_cb8b3ea7'), local_subscribe_addr='ipc:///tmp/482efcce-0b99-4403-a13d-d5ff395e0bca', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:37:49 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:37:49 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:37:49 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:37:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c46e172e'), local_subscribe_addr='ipc:///tmp/37ed3461-8417-4816-a472-b8c7f7d1a9ac', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:37:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_42a9cb56'), local_subscribe_addr='ipc:///tmp/eaf173d5-3fd6-4d60-920c-acc70d54d394', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:37:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ad703d03'), local_subscribe_addr='ipc:///tmp/2afcbcbe-7622-4f82-9ba0-08088b20a8a1', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:37:55 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:37:55 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:37:55 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:37:55 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:37:55 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:37:55 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:37:55 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:37:55 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:37:55 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 18:37:56 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:37:56 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:37:56 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:37:56 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:37:56 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:37:56 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:37:56 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:37:56 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:37:56 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:37:56 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:37:56 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:37:56 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:37:56 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:37:56 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:37:56 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:37:56 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:37:56 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:37:56 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:37:57 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:37:58 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:37:58 [default_loader.py:267] Loading weights took 0.57 seconds
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:37:58 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.313834 seconds
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:37:58 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.541656 seconds
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:37:58 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.637446 seconds
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:38:00 [gpu_worker.py:298] Available KV cache memory: 14.45 GiB
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:38:00 [gpu_worker.py:298] Available KV cache memory: 14.29 GiB
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:38:00 [gpu_worker.py:298] Available KV cache memory: 14.11 GiB
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:00 [kv_cache_utils.py:1087] GPU KV cache size: 336,240 tokens
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:00 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.87x
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:00 [kv_cache_utils.py:1087] GPU KV cache size: 340,400 tokens
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:00 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.18x
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:00 [kv_cache_utils.py:1087] GPU KV cache size: 378,640 tokens
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:00 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.01x
[1;36m(Worker_PP2 pid=4040078)[0;0m WARNING 12-28 18:38:00 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:01 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.22 seconds
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:01 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4039842)[0;0m INFO 12-28 18:38:01 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:38:01 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4040076)[0;0m WARNING 12-28 18:38:01 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4040077)[0;0m WARNING 12-28 18:38:01 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:38:25 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4040076)[0;0m INFO 12-28 18:38:25 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:38:25 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4040077)[0;0m INFO 12-28 18:38:25 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4040078)[0;0m INFO 12-28 18:38:25 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:38:25', project_name='codecarbon', run_id='c59bb35c-17ae-423f-a2d1-9db4705494c3', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=53.98335165390745, emissions=0.0007967675538762248, emissions_rate=1.4759505096763528e-05, cpu_power=30.03710448, gpu_power=200.6102420341247, ram_power=70.0, cpu_energy=0.000432990709937286, gpu_energy=0.0019120015296012483, ram_energy=0.001008561813368902, energy_consumed=0.003353554052907436, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 21/30 completed. Throughput: 544.91, Energy: 0.936311. Pareto size: 2.
INFO 12-28 18:38:35 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': True, 'max_num_batched_tokens': 7810, 'max_num_seqs': 79, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:38:35 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 18:38:35 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 18:38:35 [model.py:1510] Using max model len 32768
INFO 12-28 18:38:35 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=7810.
INFO 12-28 18:38:35 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:38:44 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:38:47 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:38:47 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4041326)[0;0m WARNING 12-28 18:38:47 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:38:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_c2a99955'), local_subscribe_addr='ipc:///tmp/3919f9bf-088f-4598-9b28-1eb0832e4ed6', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:38:54 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:38:54 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:38:55 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:39:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3df08266'), local_subscribe_addr='ipc:///tmp/aa320cf2-a11a-47aa-841e-b4048b8f46d5', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:39:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_87c1faee'), local_subscribe_addr='ipc:///tmp/02ec39fc-ed5d-46ff-bb81-560b2d03c5b6', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:39:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c1adb8f9'), local_subscribe_addr='ipc:///tmp/c2288250-7d50-43d6-b9eb-47df6b36add4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:39:01 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:39:01 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:39:01 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:39:01 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:39:01 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:39:01 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:39:01 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:39:01 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:39:01 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
WARNING 12-28 18:39:01 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:39:01 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:39:01 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:02 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:02 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:02 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:02 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:02 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:02 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:02 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:02 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:02 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:02 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:02 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:02 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:02 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:03 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:07 [default_loader.py:267] Loading weights took 4.13 seconds
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:08 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 5.201395 seconds
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:08 [default_loader.py:267] Loading weights took 4.35 seconds
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:08 [default_loader.py:267] Loading weights took 4.35 seconds
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:08 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 5.816387 seconds
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:09 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 6.343143 seconds
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:10 [gpu_worker.py:298] Available KV cache memory: 14.26 GiB
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:10 [gpu_worker.py:298] Available KV cache memory: 14.44 GiB
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:10 [gpu_worker.py:298] Available KV cache memory: 14.60 GiB
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:11 [kv_cache_utils.py:1087] GPU KV cache size: 339,760 tokens
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:11 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.31x
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:11 [kv_cache_utils.py:1087] GPU KV cache size: 344,160 tokens
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:11 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.68x
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:11 [kv_cache_utils.py:1087] GPU KV cache size: 382,800 tokens
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:11 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.90x
[1;36m(Worker_PP2 pid=4041608)[0;0m WARNING 12-28 18:39:11 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 1.98 seconds
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:12 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4041326)[0;0m INFO 12-28 18:39:12 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:39:12 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4041606)[0;0m WARNING 12-28 18:39:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4041607)[0;0m WARNING 12-28 18:39:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:36 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4041606)[0;0m INFO 12-28 18:39:36 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:36 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4041607)[0;0m INFO 12-28 18:39:36 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4041608)[0;0m INFO 12-28 18:39:36 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:39:36', project_name='codecarbon', run_id='df19c1d7-55fb-449d-83dc-5c0dd7847a18', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=62.07159167598002, emissions=0.0008885881911459564, emissions_rate=1.431553738438796e-05, cpu_power=30.05018679, gpu_power=147.36030936075014, ram_power=70.0, cpu_energy=0.0004957375380573967, gpu_energy=0.0020893394492462747, ram_energy=0.0011549454512181307, energy_consumed=0.003740022438521802, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 22/30 completed. Throughput: 507.36, Energy: 1.109433. Pareto size: 2.
INFO 12-28 18:39:45 [utils.py:233] non-default args: {'seed': 42, 'block_size': 80, 'enable_prefix_caching': True, 'max_num_batched_tokens': 6070, 'max_num_seqs': 77, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:39:46 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:39:46 [model.py:1510] Using max model len 32768
INFO 12-28 18:39:46 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6070.
INFO 12-28 18:39:46 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:39:55 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:39:59 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:39:59 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:02 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4042988)[0;0m WARNING 12-28 18:40:02 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:03 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:03 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:04 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:06 [default_loader.py:267] Loading weights took 2.06 seconds
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:07 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 3.210621 seconds
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:09 [gpu_worker.py:298] Available KV cache memory: 5.67 GiB
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:09 [kv_cache_utils.py:1087] GPU KV cache size: 46,400 tokens
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 4.50x
[1;36m(EngineCore_DP0 pid=4042988)[0;0m WARNING 12-28 18:40:09 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:10 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.82 seconds
[1;36m(EngineCore_DP0 pid=4042988)[0;0m INFO 12-28 18:40:10 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:40:11 [llm.py:306] Supported_tasks: ['generate']
EmissionsData(timestamp='2025-12-28T18:40:35', project_name='codecarbon', run_id='e817bbf9-8e57-4b6f-99f1-e68db6009360', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=49.88748296489939, emissions=0.0004814733981747218, emissions_rate=9.651186421120592e-06, cpu_power=30.016962804000002, gpu_power=68.2201406229892, ram_power=70.0, cpu_energy=0.00039908877097308103, gpu_energy=0.0006984191698462539, ram_energy=0.000928989069457748, energy_consumed=0.002026497010277083, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 23/30 completed. Throughput: 292.81, Energy: 1.053334. Pareto size: 2.
INFO 12-28 18:40:41 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8277, 'max_num_seqs': 152, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:40:42 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:40:42 [model.py:1510] Using max model len 32768
INFO 12-28 18:40:42 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8277.
INFO 12-28 18:40:42 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:40:49 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:40:52 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:40:52 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4045893)[0;0m WARNING 12-28 18:40:52 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:40:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_4e8c90f5'), local_subscribe_addr='ipc:///tmp/260caae9-436c-4836-92ab-a99f2e1d9ab6', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:40:59 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:40:59 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:40:59 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:41:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c0ffc611'), local_subscribe_addr='ipc:///tmp/1caf4b19-faf5-4ac8-87c9-ac4255c70b4d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:41:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8ba503d9'), local_subscribe_addr='ipc:///tmp/790e84a5-5c69-4a88-9b56-501e26a351b9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:41:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_41dec87c'), local_subscribe_addr='ipc:///tmp/aad6c3ac-a82b-4e49-af19-f4bdb7fb12dc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:41:06 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:41:06 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:41:06 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:41:06 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:41:06 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:41:06 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:41:06 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:41:06 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:41:06 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 18:41:06 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:41:06 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:41:06 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:06 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:06 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:06 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:07 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:07 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:07 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:07 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:07 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:07 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:07 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:07 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:07 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:07 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:07 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:07 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:08 [default_loader.py:267] Loading weights took 0.66 seconds
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:08 [default_loader.py:267] Loading weights took 0.65 seconds
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:09 [default_loader.py:267] Loading weights took 0.70 seconds
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:09 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.252929 seconds
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:09 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.413749 seconds
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:09 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.752174 seconds
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:11 [gpu_worker.py:298] Available KV cache memory: 14.22 GiB
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:11 [gpu_worker.py:298] Available KV cache memory: 14.40 GiB
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:11 [gpu_worker.py:298] Available KV cache memory: 14.56 GiB
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:11 [kv_cache_utils.py:1087] GPU KV cache size: 338,688 tokens
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:11 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 27.00x
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:11 [kv_cache_utils.py:1087] GPU KV cache size: 343,040 tokens
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:11 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 27.35x
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:11 [kv_cache_utils.py:1087] GPU KV cache size: 381,568 tokens
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:11 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.42x
[1;36m(Worker_PP2 pid=4046138)[0;0m WARNING 12-28 18:41:11 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.16 seconds
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:12 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4045893)[0;0m INFO 12-28 18:41:12 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:41:12 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4046136)[0;0m WARNING 12-28 18:41:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4046137)[0;0m WARNING 12-28 18:41:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:36 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4046136)[0;0m INFO 12-28 18:41:36 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:36 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4046137)[0;0m INFO 12-28 18:41:36 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4046138)[0;0m INFO 12-28 18:41:36 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:41:35', project_name='codecarbon', run_id='8a67f8e2-bdf0-457a-9e34-16a406c11b92', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=54.54915130091831, emissions=0.0007986546567362034, emissions_rate=1.4641009762561757e-05, cpu_power=30.035635545, gpu_power=203.8586993506952, ram_power=70.0, cpu_energy=0.0004373856313519003, gpu_energy=0.0019045468014144973, ram_energy=0.0010195643399638359, energy_consumed=0.0033614967727302334, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 24/30 completed. Throughput: 516.99, Energy: 1.021904. Pareto size: 2.
INFO 12-28 18:41:45 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 64, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8870, 'max_num_seqs': 151, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:41:45 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:41:45 [model.py:1510] Using max model len 32768
INFO 12-28 18:41:45 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8870.
INFO 12-28 18:41:45 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:41:53 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:41:56 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:41:56 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4047394)[0;0m WARNING 12-28 18:41:56 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:41:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_4a35db0a'), local_subscribe_addr='ipc:///tmp/4157c0e2-226a-45d4-8d6d-b4f2b7c781e9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:42:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:42:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:42:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c3220591'), local_subscribe_addr='ipc:///tmp/b9641ace-953e-42c2-b23c-2e14bc5475fc', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:42:08 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:42:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_764e006d'), local_subscribe_addr='ipc:///tmp/95ba41ab-53b2-4d55-bdc9-8fbd92666676', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:42:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_78d1b443'), local_subscribe_addr='ipc:///tmp/c4b233f3-2fad-4c79-9442-b4ded04e9f47', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:42:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:42:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:42:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:42:14 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:42:14 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:42:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:42:14 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:42:14 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:42:14 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 18:42:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:42:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:42:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:14 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:14 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:14 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:15 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:15 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:15 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:15 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:15 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:15 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:16 [default_loader.py:267] Loading weights took 0.61 seconds
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:16 [default_loader.py:267] Loading weights took 0.64 seconds
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:16 [default_loader.py:267] Loading weights took 0.73 seconds
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:16 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.187809 seconds
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:17 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.373112 seconds
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:17 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.702588 seconds
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:19 [gpu_worker.py:298] Available KV cache memory: 14.33 GiB
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:19 [gpu_worker.py:298] Available KV cache memory: 14.15 GiB
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:19 [gpu_worker.py:298] Available KV cache memory: 14.49 GiB
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:19 [kv_cache_utils.py:1087] GPU KV cache size: 337,216 tokens
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.83x
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:19 [kv_cache_utils.py:1087] GPU KV cache size: 341,440 tokens
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 26.15x
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:19 [kv_cache_utils.py:1087] GPU KV cache size: 379,776 tokens
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 29.09x
[1;36m(Worker_PP2 pid=4047642)[0;0m WARNING 12-28 18:42:19 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:19 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.22 seconds
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:20 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4047394)[0;0m INFO 12-28 18:42:20 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:42:20 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4047640)[0;0m WARNING 12-28 18:42:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4047641)[0;0m WARNING 12-28 18:42:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:44 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4047640)[0;0m INFO 12-28 18:42:44 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:44 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:44 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4047641)[0;0m INFO 12-28 18:42:44 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4047642)[0;0m INFO 12-28 18:42:44 [multiproc_executor.py:599] WorkerProc shutting down.
EmissionsData(timestamp='2025-12-28T18:42:44', project_name='codecarbon', run_id='d3a995cd-84d0-45f0-9ff1-fc45cded6a53', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=59.605047816061415, emissions=0.0008566212388022422, emissions_rate=1.4371622374093853e-05, cpu_power=30.034840008000003, gpu_power=205.46875120073022, ram_power=70.0, cpu_energy=0.00047953150654958646, gpu_energy=0.0020080157730779646, ram_energy=0.0011179278846361236, energy_consumed=0.0036054751642636746, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 25/30 completed. Throughput: 509.63, Energy: 1.092292. Pareto size: 2.
INFO 12-28 18:42:54 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 32, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9351, 'max_num_seqs': 144, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:42:54 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:42:54 [model.py:1510] Using max model len 32768
INFO 12-28 18:42:54 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9351.
INFO 12-28 18:42:54 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:43:03 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:06 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:06 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4049053)[0;0m WARNING 12-28 18:43:06 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_6afa34f2'), local_subscribe_addr='ipc:///tmp/d34b1360-e1bf-41a4-a152-e083ed396922', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:43:12 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:43:13 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:43:13 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:43:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a16d8958'), local_subscribe_addr='ipc:///tmp/319eb5b8-99ea-42aa-a2a3-a2f76b586bac', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:43:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8d1ef7c7'), local_subscribe_addr='ipc:///tmp/ed4e3160-efd5-48c1-9a51-193bedfc24ef', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:43:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e6ab5eab'), local_subscribe_addr='ipc:///tmp/c80fd676-2683-43ab-9837-74ad60a3f650', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:43:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:43:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:43:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:43:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:43:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:43:18 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:43:19 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:43:19 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:43:19 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 18:43:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:43:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:43:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:19 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:19 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:19 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:20 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:21 [default_loader.py:267] Loading weights took 0.59 seconds
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:22 [default_loader.py:267] Loading weights took 0.56 seconds
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:22 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.705598 seconds
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:22 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 2.181071 seconds
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:22 [default_loader.py:267] Loading weights took 0.73 seconds
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:23 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 2.788315 seconds
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:24 [gpu_worker.py:298] Available KV cache memory: 14.27 GiB
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:25 [gpu_worker.py:298] Available KV cache memory: 14.43 GiB
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:25 [gpu_worker.py:298] Available KV cache memory: 14.10 GiB
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:25 [kv_cache_utils.py:1087] GPU KV cache size: 336,064 tokens
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:25 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.89x
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:25 [kv_cache_utils.py:1087] GPU KV cache size: 340,160 tokens
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:25 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.19x
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:25 [kv_cache_utils.py:1087] GPU KV cache size: 378,368 tokens
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:25 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.02x
[1;36m(Worker_PP2 pid=4049350)[0;0m WARNING 12-28 18:43:25 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:25 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.28 seconds
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:26 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4049053)[0;0m INFO 12-28 18:43:26 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:43:26 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4049348)[0;0m WARNING 12-28 18:43:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4049349)[0;0m WARNING 12-28 18:43:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:50 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4049348)[0;0m INFO 12-28 18:43:50 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4049349)[0;0m INFO 12-28 18:43:50 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=4049350)[0;0m INFO 12-28 18:43:50 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:43:50', project_name='codecarbon', run_id='0f481acc-b924-471f-a69c-833e3c0a4175', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=57.217855714960024, emissions=0.0008350763653019423, emissions_rate=1.4594681238353458e-05, cpu_power=30.03473375538462, gpu_power=206.20264110785254, ram_power=70.0, cpu_energy=0.0004596402283219103, gpu_energy=0.001983706309185429, ram_energy=0.001071447351527685, energy_consumed=0.003514793889035024, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 26/30 completed. Throughput: 541.73, Energy: 0.990548. Pareto size: 2.
INFO 12-28 18:44:00 [utils.py:233] non-default args: {'seed': 42, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 6533, 'max_num_seqs': 186, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:44:00 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:44:00 [model.py:1510] Using max model len 32768
INFO 12-28 18:44:00 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6533.
INFO 12-28 18:44:00 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:44:08 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:12 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:12 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:14 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=4050631)[0;0m WARNING 12-28 18:44:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:18 [default_loader.py:267] Loading weights took 1.71 seconds
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:18 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.851061 seconds
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:21 [gpu_worker.py:298] Available KV cache memory: 5.62 GiB
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:21 [kv_cache_utils.py:1087] GPU KV cache size: 45,952 tokens
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:21 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 4.22x
[1;36m(EngineCore_DP0 pid=4050631)[0;0m WARNING 12-28 18:44:21 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:21 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.96 seconds
[1;36m(EngineCore_DP0 pid=4050631)[0;0m INFO 12-28 18:44:22 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:44:22 [llm.py:306] Supported_tasks: ['generate']
EmissionsData(timestamp='2025-12-28T18:44:47', project_name='codecarbon', run_id='a1035349-a038-4e92-9dbc-b04e366bf8bd', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=47.430847081006505, emissions=0.0004611212280999285, emissions_rate=9.721969066088695e-06, cpu_power=30.001776330000006, gpu_power=59.345723523968815, ram_power=70.0, cpu_energy=0.0003777290719807823, gpu_energy=0.0006819005455200866, ram_energy=0.0008812061455181619, energy_consumed=0.0019408357630190307, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 27/30 completed. Throughput: 283.05, Energy: 1.037110. Pareto size: 2.
INFO 12-28 18:44:54 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9703, 'max_num_seqs': 143, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:44:54 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:44:54 [model.py:1510] Using max model len 32768
INFO 12-28 18:44:54 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9703.
INFO 12-28 18:44:54 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:45:01 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:04 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:04 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4055083)[0;0m WARNING 12-28 18:45:04 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_c23376a0'), local_subscribe_addr='ipc:///tmp/7d7bee98-9d97-4096-a45f-6c2e3c3ab1b0', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:45:10 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:45:10 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:45:10 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:45:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_269b24b7'), local_subscribe_addr='ipc:///tmp/2fa7f5bf-882c-4c5d-836e-9614e8e19ac8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:45:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_89583813'), local_subscribe_addr='ipc:///tmp/1d7dd8df-6cd5-428b-ab66-63b30728b021', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:45:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d2929b2e'), local_subscribe_addr='ipc:///tmp/a89f9283-8e21-45df-8401-65a4d12c0dd3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:45:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:45:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:45:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:45:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:45:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:45:16 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:45:17 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:45:17 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:45:17 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
WARNING 12-28 18:45:17 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:45:17 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:45:17 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:17 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:17 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:17 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:17 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:17 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:18 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:18 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:18 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:18 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:18 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:18 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:18 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:18 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:18 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:18 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:19 [default_loader.py:267] Loading weights took 0.60 seconds
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:20 [default_loader.py:267] Loading weights took 0.72 seconds
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:20 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.680758 seconds
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:20 [default_loader.py:267] Loading weights took 0.65 seconds
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:20 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 2.218845 seconds
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:21 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 2.641464 seconds
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:22 [gpu_worker.py:298] Available KV cache memory: 14.24 GiB
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:23 [gpu_worker.py:298] Available KV cache memory: 14.40 GiB
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:23 [gpu_worker.py:298] Available KV cache memory: 14.07 GiB
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:23 [kv_cache_utils.py:1087] GPU KV cache size: 335,232 tokens
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.03x
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:23 [kv_cache_utils.py:1087] GPU KV cache size: 339,200 tokens
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.31x
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:23 [kv_cache_utils.py:1087] GPU KV cache size: 377,344 tokens
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 27.05x
[1;36m(Worker_PP2 pid=4055342)[0;0m WARNING 12-28 18:45:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:23 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.28 seconds
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:24 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4055083)[0;0m INFO 12-28 18:45:24 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:45:24 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4055340)[0;0m WARNING 12-28 18:45:24 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4055341)[0;0m WARNING 12-28 18:45:24 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:48 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4055340)[0;0m INFO 12-28 18:45:48 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:48 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4055341)[0;0m INFO 12-28 18:45:48 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4055342)[0;0m INFO 12-28 18:45:48 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:45:48', project_name='codecarbon', run_id='92ad0e9d-b18c-4af5-95b8-c67effc4d22c', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=54.90369703702163, emissions=0.0008070572908304797, emissions_rate=1.4699507216905266e-05, cpu_power=30.040517928000003, gpu_power=202.98400590122424, ram_power=70.0, cpu_energy=0.00044069793324383246, gpu_energy=0.0019296987659798148, ram_energy=0.0010264663324422954, energy_consumed=0.0033968630316659433, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 28/30 completed. Throughput: 511.34, Energy: 1.015167. Pareto size: 2.
INFO 12-28 18:45:59 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'pipeline_parallel_size': 2, 'block_size': 128, 'enable_prefix_caching': True, 'max_num_batched_tokens': 7507, 'max_num_seqs': 79, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:45:59 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:45:59 [model.py:1510] Using max model len 32768
INFO 12-28 18:45:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=7507.
INFO 12-28 18:45:59 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:46:06 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:09 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:09 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4061603)[0;0m WARNING 12-28 18:46:09 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_cd82d21c'), local_subscribe_addr='ipc:///tmp/1e1d00ad-d155-4afb-822f-ed7b6944482c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:46:16 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:46:16 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:46:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cf3ce8c8'), local_subscribe_addr='ipc:///tmp/11aa0305-a83b-4191-a55e-2031d4d5f5a8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:46:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ee744872'), local_subscribe_addr='ipc:///tmp/fdaf8245-88e4-4202-8726-6d64e7e6c399', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 18:46:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:46:23 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:46:23 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:46:23 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:46:23 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:46:23 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
WARNING 12-28 18:46:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:46:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4061820)[0;0m INFO 12-28 18:46:23 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:23 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4061820)[0;0m INFO 12-28 18:46:24 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:24 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4061820)[0;0m INFO 12-28 18:46:24 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:24 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=4061820)[0;0m INFO 12-28 18:46:24 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:24 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4061820)[0;0m INFO 12-28 18:46:25 [default_loader.py:267] Loading weights took 1.02 seconds
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:26 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(Worker_PP1 pid=4061820)[0;0m INFO 12-28 18:46:26 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 1.715103 seconds
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:26 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 1.999690 seconds
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:28 [gpu_worker.py:298] Available KV cache memory: 12.26 GiB
[1;36m(Worker_PP1 pid=4061820)[0;0m INFO 12-28 18:46:28 [gpu_worker.py:298] Available KV cache memory: 12.20 GiB
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:29 [kv_cache_utils.py:1087] GPU KV cache size: 200,832 tokens
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:29 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 17.05x
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:29 [kv_cache_utils.py:1087] GPU KV cache size: 199,936 tokens
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:29 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 16.98x
[1;36m(Worker_PP1 pid=4061820)[0;0m WARNING 12-28 18:46:29 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:29 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.41 seconds
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:29 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=4061603)[0;0m INFO 12-28 18:46:29 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:46:30 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4061819)[0;0m WARNING 12-28 18:46:30 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
WARNING 12-28 18:46:50 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:52 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4061819)[0;0m INFO 12-28 18:46:52 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4061820)[0;0m INFO 12-28 18:46:52 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:46:52', project_name='codecarbon', run_id='cf8e502d-a5da-4cd2-ae9b-41959d26a2f2', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=53.68688515096437, emissions=0.0006437835043559074, emissions_rate=1.1991448238161441e-05, cpu_power=30.027966210000002, gpu_power=136.9630767122168, ram_power=70.0, cpu_energy=0.0004300792706211532, gpu_energy=0.0012767049102517802, ram_energy=0.001002867799462466, energy_consumed=0.0027096519803354, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 29/30 completed. Throughput: 296.60, Energy: 1.494980. Pareto size: 2.
INFO 12-28 18:47:02 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 7184, 'max_num_seqs': 85, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 18:47:02 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 18:47:02 [model.py:1510] Using max model len 32768
INFO 12-28 18:47:02 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=7184.
INFO 12-28 18:47:02 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:47:08 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:11 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:11 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=4066237)[0;0m WARNING 12-28 18:47:11 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_a2c7d48c'), local_subscribe_addr='ipc:///tmp/6bbaae96-bcfb-4120-bbd2-bf12fe385a3f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:47:17 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:47:17 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:47:22 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 18:47:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a34f5858'), local_subscribe_addr='ipc:///tmp/94aaabf9-12d1-40ab-a178-32deb63d32b1', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:47:22 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bf27329b'), local_subscribe_addr='ipc:///tmp/4ab4505d-cc35-42f6-b764-c9f8445301df', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 18:47:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6854f840'), local_subscribe_addr='ipc:///tmp/26ea523e-94ac-411c-9c8c-0930ebd2530d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
INFO 12-28 18:47:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:47:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:47:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:47:29 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 18:47:29 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 18:47:29 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:47:29 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:47:29 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 18:47:29 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 18:47:29 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 18:47:29 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
WARNING 12-28 18:47:29 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:30 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:30 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:30 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:30 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:30 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:30 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:30 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:30 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:30 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:30 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:30 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:30 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:31 [default_loader.py:267] Loading weights took 0.58 seconds
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:31 [default_loader.py:267] Loading weights took 0.74 seconds
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:31 [default_loader.py:267] Loading weights took 0.60 seconds
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:31 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.169377 seconds
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:32 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.502740 seconds
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:32 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.697871 seconds
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:33 [gpu_worker.py:298] Available KV cache memory: 14.52 GiB
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:33 [gpu_worker.py:298] Available KV cache memory: 14.68 GiB
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:33 [gpu_worker.py:298] Available KV cache memory: 14.33 GiB
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:34 [kv_cache_utils.py:1087] GPU KV cache size: 341,440 tokens
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:34 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.14x
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:34 [kv_cache_utils.py:1087] GPU KV cache size: 345,952 tokens
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:34 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.54x
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:34 [kv_cache_utils.py:1087] GPU KV cache size: 384,768 tokens
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:34 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 33.97x
[1;36m(Worker_PP2 pid=4066472)[0;0m WARNING 12-28 18:47:34 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:34 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.00 seconds
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:35 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=4066237)[0;0m INFO 12-28 18:47:35 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 18:47:35 [llm.py:306] Supported_tasks: ['generate']
[1;36m(Worker_PP0 pid=4066470)[0;0m WARNING 12-28 18:47:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1 pid=4066471)[0;0m WARNING 12-28 18:47:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:58 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=4066470)[0;0m INFO 12-28 18:47:58 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:58 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=4066471)[0;0m INFO 12-28 18:47:58 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=4066472)[0;0m INFO 12-28 18:47:58 [multiproc_executor.py:558] Parent process exited, terminating worker
EmissionsData(timestamp='2025-12-28T18:47:58', project_name='codecarbon', run_id='9a013b64-0e8d-4d19-b4fb-71aabd8e9f6f', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=56.743642852059565, emissions=0.0008252506826144194, emissions_rate=1.454349141393671e-05, cpu_power=30.033940170000005, gpu_power=205.53091521859193, ram_power=70.0, cpu_energy=0.00045588155142459375, gpu_energy=0.0019554171198885584, ram_energy=0.0010621394197365993, energy_consumed=0.0034734380910497516, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 30/30 completed. Throughput: 440.50, Energy: 1.265497. Pareto size: 2.
======================================================================
Optimization Complete.
======================================================================

📊 Found 2 Pareto-optimal configurations:
  - 30: ({'block_size': 84.58317657956584, 'max_num_seqs': 153.5423927029267, 'max_num_batched_tokens': 9273.203080502642, 'tensor_parallel_size': 1.141974168439276, 'pipeline_parallel_size': 3.4964117137343034, 'enable_chunked_prefill': 0.5324896662625623, 'enable_prefix_caching': 0.20714085858985823, 'dtype_idx': 0.24360101628823755, 'data_parallel_size': 1}, ({'energy(J/token)': np.float64(0.9363144451296628), 'throughput(token/s)': np.float64(544.9070966613932)}, {'energy(J/token)': {'energy(J/token)': 4.76862288917663e-07, 'throughput(token/s)': 0.0}, 'throughput(token/s)': {'energy(J/token)': 0.0, 'throughput(token/s)': 0.03460941885392757}}))
  - 24: ({'block_size': 45.024905492367196, 'max_num_seqs': 91.34718240044911, 'max_num_batched_tokens': 6595.58961697875, 'tensor_parallel_size': 1.2711893981218145, 'pipeline_parallel_size': 1.4030382795981888, 'enable_chunked_prefill': 0.8078366072927925, 'enable_prefix_caching': 1.0, 'dtype_idx': 0.47673736019320595, 'data_parallel_size': 1}, ({'energy(J/token)': np.float64(0.9122926938884778), 'throughput(token/s)': np.float64(307.2247397017348)}, {'energy(J/token)': {'energy(J/token)': 4.769063756567887e-07, 'throughput(token/s)': 0.0}, 'throughput(token/s)': {'energy(J/token)': 0.0, 'throughput(token/s)': 0.03461220351497268}}))
