========================================
Command 1: python vllm_evaluation.py --num_prompts 100 --block_size 48 --max_num_seqs 91 --max_num_batched_tokens 6596 --tensor_parallel_size 1 --enable_chunked_prefill --enable_prefix_caching
Required GPUs: 1
========================================
--- Run 1 of 5 ---
Using GPUs: 1
INFO 01-19 00:14:04 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:14:11] offline tracker init
[codecarbon WARNING @ 00:14:11] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:14:12] [setup] RAM Tracking...
[codecarbon INFO @ 00:14:12] [setup] CPU Tracking...
[codecarbon WARNING @ 00:14:13] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:14:13] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:14:13] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:14:13] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:14:13] [setup] GPU Tracking...
[codecarbon INFO @ 00:14:13] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:14:13] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:14:13] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:14:13] >>> Tracker's metadata:
[codecarbon INFO @ 00:14:13]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:14:13]   Python version: 3.12.3
[codecarbon INFO @ 00:14:13]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:14:13]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:14:13]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:14:13]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:14:13]   GPU count: 1
[codecarbon INFO @ 00:14:13]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:14:13] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 48
  Batch Size: 91
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:14:13 [utils.py:233] non-default args: {'seed': 42, 'block_size': 48, 'enable_prefix_caching': True, 'max_num_batched_tokens': 6596, 'max_num_seqs': 91, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:14:14 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:14:14 [model.py:1510] Using max model len 32768
INFO 01-19 00:14:16 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6596.
INFO 01-19 00:14:16 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:17 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:17 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:21 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2209919)[0;0m WARNING 01-19 00:14:22 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:22 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:22 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:22 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:23 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2209919)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2209919)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.61s/it]
[1;36m(EngineCore_DP0 pid=2209919)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.19s/it]
[1;36m(EngineCore_DP0 pid=2209919)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.26s/it]
[1;36m(EngineCore_DP0 pid=2209919)[0;0m 
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:25 [default_loader.py:267] Loading weights took 2.58 seconds
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:26 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 3.304248 seconds
[codecarbon INFO @ 00:14:28] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:14:29] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.005599872 W
[codecarbon INFO @ 00:14:29] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:14:29] Energy consumed for all GPUs : 0.000114 kWh. Total GPU Power : 25.703886525405476 W
[codecarbon INFO @ 00:14:29] 0.000546 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:29 [gpu_worker.py:298] Available KV cache memory: 5.61 GiB
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:30 [kv_cache_utils.py:1087] GPU KV cache size: 45,936 tokens
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:30 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 4.27x
[1;36m(EngineCore_DP0 pid=2209919)[0;0m WARNING 01-19 00:14:30 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:30 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.62 seconds
[1;36m(EngineCore_DP0 pid=2209919)[0;0m INFO 01-19 00:14:30 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:14:30 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 957
INFO 01-19 00:14:31 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1316.84it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:43,  1.65s/it, est. speed input: 35.16 toks/s, output: 1.21 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:03<01:41,  1.04s/it, est. speed input: 147.32 toks/s, output: 1.21 toks/s]Processed prompts:   4%|â–         | 4/100 [00:04<02:00,  1.26s/it, est. speed input: 204.56 toks/s, output: 1.41 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:05<01:08,  1.38it/s, est. speed input: 251.59 toks/s, output: 2.05 toks/s]Processed prompts:  10%|â–ˆ         | 10/100 [00:05<00:33,  2.72it/s, est. speed input: 599.32 toks/s, output: 3.79 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 15/100 [00:05<00:16,  5.05it/s, est. speed input: 761.42 toks/s, output: 7.68 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:06<00:07,  9.68it/s, est. speed input: 1103.22 toks/s, output: 15.23 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:06<00:07, 10.02it/s, est. speed input: 1193.25 toks/s, output: 19.58 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:06<00:05, 12.99it/s, est. speed input: 1365.85 toks/s, output: 27.86 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:06<00:05, 12.65it/s, est. speed input: 1376.90 toks/s, output: 33.66 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:06,  9.73it/s, est. speed input: 1306.81 toks/s, output: 37.33 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:07<00:04, 12.23it/s, est. speed input: 1370.28 toks/s, output: 48.46 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:07<00:04, 13.48it/s, est. speed input: 1467.48 toks/s, output: 56.78 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:05, 10.02it/s, est. speed input: 1404.85 toks/s, output: 60.79 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:08<00:05, 10.47it/s, est. speed input: 1422.53 toks/s, output: 67.30 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:08<00:05,  9.88it/s, est. speed input: 1397.60 toks/s, output: 73.24 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:08<00:03, 11.93it/s, est. speed input: 1442.42 toks/s, output: 87.91 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:08<00:04,  9.98it/s, est. speed input: 1407.80 toks/s, output: 94.11 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:09<00:03, 11.86it/s, est. speed input: 1476.92 toks/s, output: 106.78 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:09<00:04,  9.12it/s, est. speed input: 1490.32 toks/s, output: 111.86 toks/s]INFO 01-19 00:14:41 [loggers.py:127] Engine 000: Avg prompt throughput: 2180.1 tokens/s, Avg generation throughput: 315.8 tokens/s, Running: 39 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.8%, Prefix cache hit rate: 0.0%
Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:10<00:06,  5.61it/s, est. speed input: 1391.73 toks/s, output: 114.28 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:11<00:10,  3.58it/s, est. speed input: 1291.30 toks/s, output: 112.05 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:11<00:09,  3.90it/s, est. speed input: 1280.53 toks/s, output: 117.02 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:11<00:08,  4.25it/s, est. speed input: 1305.24 toks/s, output: 121.95 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:11<00:06,  5.26it/s, est. speed input: 1443.84 toks/s, output: 132.49 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:11<00:05,  6.15it/s, est. speed input: 1502.44 toks/s, output: 143.44 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:11<00:03,  7.51it/s, est. speed input: 1520.68 toks/s, output: 154.95 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:12<00:04,  5.45it/s, est. speed input: 1457.70 toks/s, output: 161.31 toks/s][codecarbon INFO @ 00:14:43] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:12<00:04,  5.26it/s, est. speed input: 1434.35 toks/s, output: 165.76 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:05,  4.75it/s, est. speed input: 1412.77 toks/s, output: 169.44 toks/s][codecarbon INFO @ 00:14:44] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.003844800000003 W
[codecarbon INFO @ 00:14:44] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:14:44] Energy consumed for all GPUs : 0.000394 kWh. Total GPU Power : 67.32318679934606 W
[codecarbon INFO @ 00:14:44] 0.001228 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:07,  3.39it/s, est. speed input: 1357.81 toks/s, output: 169.98 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:14<00:07,  3.04it/s, est. speed input: 1329.14 toks/s, output: 172.58 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:15<00:11,  1.91it/s, est. speed input: 1258.14 toks/s, output: 168.60 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:15<00:09,  2.18it/s, est. speed input: 1240.96 toks/s, output: 173.96 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:15<00:07,  2.70it/s, est. speed input: 1231.41 toks/s, output: 180.77 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:16<00:07,  2.59it/s, est. speed input: 1202.97 toks/s, output: 184.53 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:16<00:04,  4.09it/s, est. speed input: 1214.20 toks/s, output: 199.99 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:16<00:04,  3.48it/s, est. speed input: 1244.41 toks/s, output: 203.79 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:17<00:05,  2.64it/s, est. speed input: 1201.83 toks/s, output: 205.19 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:17<00:04,  2.89it/s, est. speed input: 1187.45 toks/s, output: 211.29 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:17<00:04,  2.81it/s, est. speed input: 1166.85 toks/s, output: 216.12 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:17<00:02,  4.42it/s, est. speed input: 1167.56 toks/s, output: 233.07 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:18<00:02,  3.60it/s, est. speed input: 1142.97 toks/s, output: 237.05 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:18<00:02,  4.19it/s, est. speed input: 1138.47 toks/s, output: 245.04 toks/s]INFO 01-19 00:14:51 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 306.5 tokens/s, Running: 9 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.7%, Prefix cache hit rate: 0.0%
Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:23<00:11,  1.44s/it, est. speed input: 919.25 toks/s, output: 205.96 toks/s] Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:23<00:07,  1.08s/it, est. speed input: 915.31 toks/s, output: 215.78 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:23<00:00,  2.99it/s, est. speed input: 956.66 toks/s, output: 279.91 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  2.99it/s, est. speed input: 956.98 toks/s, output: 289.26 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.23it/s, est. speed input: 956.98 toks/s, output: 289.26 toks/s]
[codecarbon INFO @ 00:14:54] Energy consumed for RAM : 0.000785 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:14:55] Delta energy consumed for CPU with cpu_load : 0.000086 kWh, power : 30.004740570000006 W
[codecarbon INFO @ 00:14:55] Energy consumed for All CPU : 0.000336 kWh
[codecarbon INFO @ 00:14:55] Energy consumed for all GPUs : 0.000606 kWh. Total GPU Power : 70.14454110446344 W
[codecarbon INFO @ 00:14:55] 0.001727 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:14:55] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:14:55', project_name='codecarbon', run_id='3656dbc1-0eea-427a-93fb-7c6c90a67e7b', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=41.936277816072106, emissions=0.0004103832750931235, emissions_rate=9.785877442271327e-06, cpu_power=30.004740570000006, gpu_power=70.14454110446344, ram_power=70.0, cpu_energy=0.0003363956254497317, gpu_energy=0.0006061468738067788, ram_energy=0.0007847398037672974, energy_consumed=0.001727282303023808, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.14s
  Throughput: 288.30 tokens/sec
  Energy per token: 0.91 J/token
  Requests/sec: 4.21
  Total time: 23.75s
  Emissions: 0.000410 kg CO2
  Total energy consumed: 0.001727 kWh

Results saved to: results/run_20260119_001455/results.json
Run 1 completed successfully
--- Run 2 of 5 ---
Using GPUs: 1
INFO 01-19 00:15:02 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:15:09] offline tracker init
[codecarbon WARNING @ 00:15:09] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:15:09] [setup] RAM Tracking...
[codecarbon INFO @ 00:15:09] [setup] CPU Tracking...
[codecarbon WARNING @ 00:15:10] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:15:10] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:15:10] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:15:10] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:15:10] [setup] GPU Tracking...
[codecarbon INFO @ 00:15:10] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:15:10] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:15:10] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:15:10] >>> Tracker's metadata:
[codecarbon INFO @ 00:15:10]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:15:10]   Python version: 3.12.3
[codecarbon INFO @ 00:15:10]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:15:10]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:15:10]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:15:10]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:15:10]   GPU count: 1
[codecarbon INFO @ 00:15:10]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:15:10] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 48
  Batch Size: 91
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:15:11 [utils.py:233] non-default args: {'seed': 42, 'block_size': 48, 'enable_prefix_caching': True, 'max_num_batched_tokens': 6596, 'max_num_seqs': 91, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:15:12 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:15:12 [model.py:1510] Using max model len 32768
INFO 01-19 00:15:13 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6596.
INFO 01-19 00:15:13 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:14 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:14 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:18 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2211655)[0;0m WARNING 01-19 00:15:18 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:18 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:19 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2211655)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2211655)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.63s/it]
[1;36m(EngineCore_DP0 pid=2211655)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.18s/it]
[1;36m(EngineCore_DP0 pid=2211655)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
[1;36m(EngineCore_DP0 pid=2211655)[0;0m 
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:22 [default_loader.py:267] Loading weights took 2.55 seconds
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:23 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 3.375024 seconds
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:25 [gpu_worker.py:298] Available KV cache memory: 5.61 GiB
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:25 [kv_cache_utils.py:1087] GPU KV cache size: 45,936 tokens
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:25 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 4.27x
[1;36m(EngineCore_DP0 pid=2211655)[0;0m WARNING 01-19 00:15:25 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:25 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.93 seconds
[1;36m(EngineCore_DP0 pid=2211655)[0;0m INFO 01-19 00:15:26 [__init__.py:381] Cudagraph is disabled under eager mode
[codecarbon INFO @ 00:15:26] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
INFO 01-19 00:15:26 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 957
INFO 01-19 00:15:26 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:00<00:00, 92.90it/s][codecarbon INFO @ 00:15:27] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.012247920000007 W
[codecarbon INFO @ 00:15:27] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:15:27] Energy consumed for all GPUs : 0.000125 kWh. Total GPU Power : 27.646328535057616 W
[codecarbon INFO @ 00:15:27] 0.000556 kWh of electricity and 0.000000 L of water were used since the beginning.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 253.68it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:14,  1.36s/it, est. speed input: 114.90 toks/s, output: 0.74 toks/s]Processed prompts:   2%|â–         | 2/100 [00:03<02:30,  1.54s/it, est. speed input: 70.72 toks/s, output: 0.99 toks/s] Processed prompts:   3%|â–Ž         | 3/100 [00:04<02:35,  1.60s/it, est. speed input: 158.12 toks/s, output: 1.28 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:05<01:15,  1.26it/s, est. speed input: 343.89 toks/s, output: 1.98 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:05<00:43,  2.16it/s, est. speed input: 389.99 toks/s, output: 3.10 toks/s]Processed prompts:   8%|â–Š         | 8/100 [00:05<00:34,  2.65it/s, est. speed input: 419.63 toks/s, output: 4.18 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:05<00:22,  3.92it/s, est. speed input: 466.22 toks/s, output: 6.69 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:05<00:08,  9.89it/s, est. speed input: 1085.82 toks/s, output: 15.96 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:06<00:06, 11.01it/s, est. speed input: 1236.46 toks/s, output: 20.22 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:06<00:06, 12.08it/s, est. speed input: 1297.38 toks/s, output: 26.74 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:04, 15.29it/s, est. speed input: 1405.89 toks/s, output: 37.75 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:06<00:05, 11.94it/s, est. speed input: 1470.57 toks/s, output: 41.36 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:05, 11.93it/s, est. speed input: 1454.38 toks/s, output: 51.45 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:07<00:04, 14.11it/s, est. speed input: 1565.73 toks/s, output: 66.32 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:07<00:04, 12.56it/s, est. speed input: 1594.36 toks/s, output: 71.61 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:04, 11.49it/s, est. speed input: 1585.10 toks/s, output: 80.06 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:04, 11.69it/s, est. speed input: 1611.36 toks/s, output: 86.90 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:08<00:04, 11.89it/s, est. speed input: 1594.81 toks/s, output: 94.09 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:08<00:03, 12.16it/s, est. speed input: 1584.84 toks/s, output: 105.35 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:09<00:05,  7.30it/s, est. speed input: 1503.21 toks/s, output: 108.50 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:09<00:06,  6.16it/s, est. speed input: 1464.36 toks/s, output: 110.52 toks/s]INFO 01-19 00:15:37 [loggers.py:127] Engine 000: Avg prompt throughput: 2119.2 tokens/s, Avg generation throughput: 328.1 tokens/s, Running: 41 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.8%, Prefix cache hit rate: 0.0%
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:10<00:08,  4.65it/s, est. speed input: 1402.79 toks/s, output: 111.24 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:10<00:13,  3.07it/s, est. speed input: 1309.55 toks/s, output: 109.70 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:11<00:12,  3.11it/s, est. speed input: 1280.05 toks/s, output: 113.32 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:11<00:11,  3.32it/s, est. speed input: 1233.74 toks/s, output: 121.23 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:12<00:09,  3.63it/s, est. speed input: 1297.50 toks/s, output: 129.95 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:12<00:11,  3.06it/s, est. speed input: 1248.42 toks/s, output: 131.80 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:13<00:10,  3.03it/s, est. speed input: 1266.77 toks/s, output: 139.87 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:13<00:06,  4.60it/s, est. speed input: 1323.89 toks/s, output: 160.50 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:03,  7.41it/s, est. speed input: 1320.38 toks/s, output: 189.82 toks/s][codecarbon INFO @ 00:15:41] Energy consumed for RAM : 0.000579 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:15:41] Delta energy consumed for CPU with cpu_load : 0.000119 kWh, power : 30.006969661875 W
[codecarbon INFO @ 00:15:41] Energy consumed for All CPU : 0.000248 kWh
[codecarbon INFO @ 00:15:41] Energy consumed for all GPUs : 0.000420 kWh. Total GPU Power : 71.99947210313583 W
[codecarbon INFO @ 00:15:41] 0.001247 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:14<00:06,  3.82it/s, est. speed input: 1215.78 toks/s, output: 189.74 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:15<00:04,  4.33it/s, est. speed input: 1247.23 toks/s, output: 202.69 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:15<00:04,  4.24it/s, est. speed input: 1212.01 toks/s, output: 213.06 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:16<00:06,  2.98it/s, est. speed input: 1154.95 toks/s, output: 210.95 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:16<00:04,  3.98it/s, est. speed input: 1208.82 toks/s, output: 226.99 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:17<00:04,  3.62it/s, est. speed input: 1190.91 toks/s, output: 231.02 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:17<00:03,  4.05it/s, est. speed input: 1185.03 toks/s, output: 243.92 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:19<00:07,  1.67it/s, est. speed input: 1071.15 toks/s, output: 228.44 toks/s]INFO 01-19 00:15:47 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 336.1 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.3%, Prefix cache hit rate: 0.0%
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:20<00:06,  1.57it/s, est. speed input: 1034.45 toks/s, output: 229.92 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:20<00:05,  1.74it/s, est. speed input: 1016.78 toks/s, output: 235.92 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:22<00:07,  1.21it/s, est. speed input: 948.79 toks/s, output: 229.94 toks/s] Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:23<00:07,  1.10it/s, est. speed input: 904.02 toks/s, output: 229.80 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:23<00:00,  3.04it/s, est. speed input: 934.97 toks/s, output: 283.05 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.33it/s, est. speed input: 956.59 toks/s, output: 313.43 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.33it/s, est. speed input: 956.59 toks/s, output: 313.43 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.22it/s, est. speed input: 956.59 toks/s, output: 313.43 toks/s]
[codecarbon INFO @ 00:15:50] Energy consumed for RAM : 0.000752 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:15:51] Delta energy consumed for CPU with cpu_load : 0.000074 kWh, power : 30.002479189090916 W
[codecarbon INFO @ 00:15:51] Energy consumed for All CPU : 0.000323 kWh
[codecarbon INFO @ 00:15:51] Energy consumed for all GPUs : 0.000604 kWh. Total GPU Power : 70.35460384764467 W
[codecarbon INFO @ 00:15:51] 0.001679 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:15:51', project_name='codecarbon', run_id='22898401-90fc-43ec-a539-9bfad23e7559', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.48108625924215, emissions=0.0003987966681356991, emissions_rate=9.851431989294772e-06, cpu_power=30.002479189090916, gpu_power=70.35460384764467, ram_power=70.0, cpu_energy=0.00032250309707630205, gpu_energy=0.0006037585385598732, ram_energy=0.0007522532288386074, energy_consumed=0.0016785148644747824, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 15.36s
  Throughput: 308.26 tokens/sec
  Energy per token: 0.81 J/token
  Requests/sec: 4.15
  Total time: 24.07s
  Emissions: 0.000399 kg CO2
  Total energy consumed: 0.001679 kWh

Results saved to: results/run_20260119_001551/results.json
Run 2 completed successfully
--- Run 3 of 5 ---
Using GPUs: 1
INFO 01-19 00:15:59 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:16:04] offline tracker init
[codecarbon WARNING @ 00:16:04] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:16:04] [setup] RAM Tracking...
[codecarbon INFO @ 00:16:04] [setup] CPU Tracking...
[codecarbon WARNING @ 00:16:06] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:16:06] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:16:06] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:16:06] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:16:06] [setup] GPU Tracking...
[codecarbon INFO @ 00:16:06] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:16:06] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:16:06] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:16:06] >>> Tracker's metadata:
[codecarbon INFO @ 00:16:06]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:16:06]   Python version: 3.12.3
[codecarbon INFO @ 00:16:06]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:16:06]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:16:06]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:16:06]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:16:06]   GPU count: 1
[codecarbon INFO @ 00:16:06]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:16:06] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 48
  Batch Size: 91
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:16:06 [utils.py:233] non-default args: {'seed': 42, 'block_size': 48, 'enable_prefix_caching': True, 'max_num_batched_tokens': 6596, 'max_num_seqs': 91, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:16:07 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:16:07 [model.py:1510] Using max model len 32768
INFO 01-19 00:16:09 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6596.
INFO 01-19 00:16:09 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:09 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:09 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:13 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2213777)[0;0m WARNING 01-19 00:16:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:14 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:14 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:14 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:14 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2213777)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2213777)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.41s/it]
[1;36m(EngineCore_DP0 pid=2213777)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.02s/it]
[1;36m(EngineCore_DP0 pid=2213777)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]
[1;36m(EngineCore_DP0 pid=2213777)[0;0m 
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:17 [default_loader.py:267] Loading weights took 2.21 seconds
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:17 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.943254 seconds
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:20 [gpu_worker.py:298] Available KV cache memory: 5.61 GiB
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:20 [kv_cache_utils.py:1087] GPU KV cache size: 45,936 tokens
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:20 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 4.27x
[1;36m(EngineCore_DP0 pid=2213777)[0;0m WARNING 01-19 00:16:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:20 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.89 seconds
[1;36m(EngineCore_DP0 pid=2213777)[0;0m INFO 01-19 00:16:21 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:16:21 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 957
INFO 01-19 00:16:21 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1417.46it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][codecarbon INFO @ 00:16:21] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:16:22] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.003793536000003 W
[codecarbon INFO @ 00:16:22] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:16:22] Energy consumed for all GPUs : 0.000132 kWh. Total GPU Power : 29.608558867873764 W
[codecarbon INFO @ 00:16:22] 0.000563 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:   1%|          | 1/100 [00:01<02:44,  1.67s/it, est. speed input: 93.64 toks/s, output: 0.60 toks/s]Processed prompts:   2%|â–         | 2/100 [00:03<02:44,  1.67s/it, est. speed input: 63.96 toks/s, output: 0.90 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:05<02:42,  1.67s/it, est. speed input: 148.14 toks/s, output: 1.20 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:05<01:18,  1.21it/s, est. speed input: 323.68 toks/s, output: 1.86 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:05<00:44,  2.08it/s, est. speed input: 367.45 toks/s, output: 2.92 toks/s]Processed prompts:   8%|â–Š         | 8/100 [00:05<00:35,  2.56it/s, est. speed input: 395.73 toks/s, output: 3.94 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:05<00:23,  3.83it/s, est. speed input: 441.62 toks/s, output: 6.34 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:06<00:08,  9.73it/s, est. speed input: 1031.63 toks/s, output: 15.16 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:06<00:07, 10.88it/s, est. speed input: 1176.95 toks/s, output: 19.25 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:06<00:06, 12.00it/s, est. speed input: 1237.74 toks/s, output: 25.51 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:04, 15.23it/s, est. speed input: 1343.19 toks/s, output: 36.07 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:05, 12.98it/s, est. speed input: 1450.41 toks/s, output: 42.40 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:05, 11.71it/s, est. speed input: 1397.05 toks/s, output: 49.42 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:07<00:04, 13.98it/s, est. speed input: 1506.33 toks/s, output: 63.80 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:04, 12.49it/s, est. speed input: 1536.10 toks/s, output: 69.00 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:04, 11.50it/s, est. speed input: 1530.36 toks/s, output: 77.29 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:04, 11.73it/s, est. speed input: 1557.21 toks/s, output: 83.98 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:08<00:04, 11.96it/s, est. speed input: 1542.48 toks/s, output: 91.01 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:08<00:03, 12.23it/s, est. speed input: 1534.44 toks/s, output: 102.00 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:09<00:05,  7.35it/s, est. speed input: 1459.62 toks/s, output: 105.35 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:09<00:06,  6.21it/s, est. speed input: 1423.60 toks/s, output: 107.44 toks/s]INFO 01-19 00:16:31 [loggers.py:127] Engine 000: Avg prompt throughput: 2204.5 tokens/s, Avg generation throughput: 329.1 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.7%, Prefix cache hit rate: 0.0%
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:10<00:08,  4.69it/s, est. speed input: 1366.18 toks/s, output: 108.34 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:11<00:12,  3.09it/s, est. speed input: 1278.59 toks/s, output: 107.11 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:11<00:12,  3.12it/s, est. speed input: 1250.54 toks/s, output: 110.71 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:11<00:11,  3.32it/s, est. speed input: 1206.31 toks/s, output: 118.53 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:12<00:09,  3.63it/s, est. speed input: 1269.68 toks/s, output: 127.16 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:12<00:11,  3.06it/s, est. speed input: 1222.59 toks/s, output: 129.07 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:13<00:10,  3.02it/s, est. speed input: 1241.25 toks/s, output: 137.05 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:13<00:06,  4.58it/s, est. speed input: 1297.47 toks/s, output: 157.30 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:03,  7.37it/s, est. speed input: 1294.12 toks/s, output: 186.04 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:15<00:06,  3.79it/s, est. speed input: 1192.60 toks/s, output: 186.12 toks/s][codecarbon INFO @ 00:16:36] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:15<00:04,  4.29it/s, est. speed input: 1223.63 toks/s, output: 198.86 toks/s][codecarbon INFO @ 00:16:37] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.004466238750002 W
[codecarbon INFO @ 00:16:37] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:16:37] Energy consumed for all GPUs : 0.000431 kWh. Total GPU Power : 71.912579130328 W
[codecarbon INFO @ 00:16:37] 0.001264 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:16<00:04,  4.20it/s, est. speed input: 1189.38 toks/s, output: 209.08 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:16<00:06,  2.96it/s, est. speed input: 1133.90 toks/s, output: 207.11 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:17<00:04,  3.94it/s, est. speed input: 1186.87 toks/s, output: 222.86 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:17<00:04,  3.58it/s, est. speed input: 1169.48 toks/s, output: 226.87 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:17<00:03,  4.01it/s, est. speed input: 1163.92 toks/s, output: 239.57 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:19<00:07,  1.65it/s, est. speed input: 1053.31 toks/s, output: 224.63 toks/s]INFO 01-19 00:16:41 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 343.5 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.3%, Prefix cache hit rate: 0.0%
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:20<00:07,  1.57it/s, est. speed input: 1017.87 toks/s, output: 226.24 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:21<00:05,  1.73it/s, est. speed input: 1000.76 toks/s, output: 232.20 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:22<00:07,  1.20it/s, est. speed input: 934.95 toks/s, output: 226.59 toks/s] Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:23<00:07,  1.10it/s, est. speed input: 891.47 toks/s, output: 226.61 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:23<00:00,  3.04it/s, est. speed input: 922.05 toks/s, output: 279.14 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  4.32it/s, est. speed input: 943.48 toks/s, output: 309.13 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  4.32it/s, est. speed input: 943.48 toks/s, output: 309.13 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  4.17it/s, est. speed input: 943.48 toks/s, output: 309.13 toks/s]
[codecarbon INFO @ 00:16:45] Energy consumed for RAM : 0.000741 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:16:46] Delta energy consumed for CPU with cpu_load : 0.000067 kWh, power : 30.003553659000005 W
[codecarbon INFO @ 00:16:46] Energy consumed for All CPU : 0.000318 kWh
[codecarbon INFO @ 00:16:46] Energy consumed for all GPUs : 0.000599 kWh. Total GPU Power : 70.28293230396639 W
[codecarbon INFO @ 00:16:46] 0.001657 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:16:46] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:16:46', project_name='codecarbon', run_id='97b6a7cd-d28d-422a-aac0-4ee55e92e725', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=39.67551059508696, emissions=0.0003937006938160989, emissions_rate=9.923015177650948e-06, cpu_power=30.003553659000005, gpu_power=70.28293230396639, ram_power=70.0, cpu_energy=0.00031751398318370205, gpu_energy=0.0005988352012913367, ram_energy=0.0007407169836392213, energy_consumed=0.00165706616811426, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 14.55s
  Throughput: 308.20 tokens/sec
  Energy per token: 0.80 J/token
  Requests/sec: 4.15
  Total time: 24.08s
  Emissions: 0.000394 kg CO2
  Total energy consumed: 0.001657 kWh

Results saved to: results/run_20260119_001646/results.json
Run 3 completed successfully
--- Run 4 of 5 ---
Using GPUs: 1
INFO 01-19 00:16:53 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:16:58] offline tracker init
[codecarbon WARNING @ 00:16:58] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:16:58] [setup] RAM Tracking...
[codecarbon INFO @ 00:16:58] [setup] CPU Tracking...
[codecarbon WARNING @ 00:17:00] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:17:00] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:17:00] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:17:00] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:17:00] [setup] GPU Tracking...
[codecarbon INFO @ 00:17:00] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:17:00] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:17:00] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:17:00] >>> Tracker's metadata:
[codecarbon INFO @ 00:17:00]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:17:00]   Python version: 3.12.3
[codecarbon INFO @ 00:17:00]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:17:00]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:17:00]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:17:00]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:17:00]   GPU count: 1
[codecarbon INFO @ 00:17:00]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:17:00] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 48
  Batch Size: 91
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:17:00 [utils.py:233] non-default args: {'seed': 42, 'block_size': 48, 'enable_prefix_caching': True, 'max_num_batched_tokens': 6596, 'max_num_seqs': 91, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:17:01 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:17:01 [model.py:1510] Using max model len 32768
INFO 01-19 00:17:02 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6596.
INFO 01-19 00:17:02 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:03 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:03 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:07 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2214883)[0;0m WARNING 01-19 00:17:07 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:07 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:07 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:07 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:08 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2214883)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2214883)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.37s/it]
[1;36m(EngineCore_DP0 pid=2214883)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.01s/it]
[1;36m(EngineCore_DP0 pid=2214883)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
[1;36m(EngineCore_DP0 pid=2214883)[0;0m 
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:10 [default_loader.py:267] Loading weights took 2.18 seconds
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:11 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.890737 seconds
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:13 [gpu_worker.py:298] Available KV cache memory: 5.61 GiB
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:14 [kv_cache_utils.py:1087] GPU KV cache size: 45,936 tokens
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:14 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 4.27x
[1;36m(EngineCore_DP0 pid=2214883)[0;0m WARNING 01-19 00:17:14 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:14 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.89 seconds
[1;36m(EngineCore_DP0 pid=2214883)[0;0m INFO 01-19 00:17:14 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:17:14 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 957
INFO 01-19 00:17:14 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1946.72it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][codecarbon INFO @ 00:17:15] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:17:16] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.003691044 W
[codecarbon INFO @ 00:17:16] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:17:16] Energy consumed for all GPUs : 0.000137 kWh. Total GPU Power : 30.856126461829493 W
[codecarbon INFO @ 00:17:16] 0.000569 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:   2%|â–         | 2/100 [00:01<01:24,  1.16it/s, est. speed input: 124.18 toks/s, output: 1.74 toks/s]Processed prompts:   4%|â–         | 4/100 [00:03<01:21,  1.18it/s, est. speed input: 191.04 toks/s, output: 1.47 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:05<01:43,  1.09s/it, est. speed input: 218.93 toks/s, output: 1.18 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:05<01:21,  1.15it/s, est. speed input: 213.10 toks/s, output: 1.85 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:05<00:30,  2.94it/s, est. speed input: 571.85 toks/s, output: 4.43 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 15/100 [00:06<00:18,  4.72it/s, est. speed input: 691.67 toks/s, output: 7.77 toks/s]Processed prompts:  21%|â–ˆâ–ˆ        | 21/100 [00:06<00:09,  8.07it/s, est. speed input: 903.08 toks/s, output: 13.96 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:06<00:08,  9.33it/s, est. speed input: 1013.05 toks/s, output: 17.01 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:05, 12.53it/s, est. speed input: 1242.46 toks/s, output: 24.45 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:05, 13.45it/s, est. speed input: 1352.80 toks/s, output: 29.60 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:05, 11.82it/s, est. speed input: 1408.28 toks/s, output: 33.36 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:04, 14.39it/s, est. speed input: 1509.63 toks/s, output: 42.62 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:07<00:05, 11.35it/s, est. speed input: 1523.04 toks/s, output: 46.24 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:07<00:06,  9.57it/s, est. speed input: 1587.50 toks/s, output: 50.71 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:06,  9.24it/s, est. speed input: 1591.55 toks/s, output: 56.65 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:04, 12.49it/s, est. speed input: 1653.53 toks/s, output: 70.88 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:08<00:03, 13.87it/s, est. speed input: 1735.48 toks/s, output: 84.41 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:08<00:03, 13.68it/s, est. speed input: 1765.88 toks/s, output: 91.38 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:09<00:05,  8.49it/s, est. speed input: 1677.48 toks/s, output: 95.54 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:09<00:04,  8.90it/s, est. speed input: 1683.60 toks/s, output: 107.18 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:09<00:05,  7.66it/s, est. speed input: 1630.18 toks/s, output: 113.53 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:10<00:05,  6.92it/s, est. speed input: 1599.20 toks/s, output: 116.46 toks/s]INFO 01-19 00:17:25 [loggers.py:127] Engine 000: Avg prompt throughput: 2187.2 tokens/s, Avg generation throughput: 320.6 tokens/s, Running: 38 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.7%, Prefix cache hit rate: 0.0%
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:10<00:06,  5.43it/s, est. speed input: 1545.22 toks/s, output: 118.13 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:11<00:09,  3.75it/s, est. speed input: 1466.55 toks/s, output: 117.94 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:11<00:09,  3.65it/s, est. speed input: 1458.76 toks/s, output: 121.17 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:11<00:10,  3.36it/s, est. speed input: 1417.93 toks/s, output: 123.87 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:11<00:08,  3.84it/s, est. speed input: 1410.10 toks/s, output: 129.06 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:12<00:09,  3.23it/s, est. speed input: 1382.24 toks/s, output: 131.29 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:12<00:04,  5.86it/s, est. speed input: 1454.22 toks/s, output: 150.38 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:12<00:05,  4.76it/s, est. speed input: 1416.52 toks/s, output: 153.35 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:13<00:04,  5.37it/s, est. speed input: 1421.13 toks/s, output: 164.38 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:05,  4.53it/s, est. speed input: 1435.91 toks/s, output: 172.24 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:14<00:05,  4.04it/s, est. speed input: 1419.57 toks/s, output: 175.64 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:14<00:06,  3.31it/s, est. speed input: 1376.89 toks/s, output: 177.69 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:14<00:05,  3.79it/s, est. speed input: 1368.39 toks/s, output: 183.96 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:14<00:04,  4.30it/s, est. speed input: 1412.53 toks/s, output: 190.30 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:14<00:03,  4.82it/s, est. speed input: 1405.88 toks/s, output: 196.76 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:15<00:04,  4.41it/s, est. speed input: 1382.04 toks/s, output: 201.46 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:15<00:04,  3.84it/s, est. speed input: 1356.12 toks/s, output: 205.41 toks/s][codecarbon INFO @ 00:17:30] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:15<00:03,  4.09it/s, est. speed input: 1346.74 toks/s, output: 211.21 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:16<00:03,  3.76it/s, est. speed input: 1331.46 toks/s, output: 215.68 toks/s][codecarbon INFO @ 00:17:31] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.002945176875002 W
[codecarbon INFO @ 00:17:31] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:17:31] Energy consumed for all GPUs : 0.000437 kWh. Total GPU Power : 71.95287886744087 W
[codecarbon INFO @ 00:17:31] 0.001270 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:16<00:05,  2.44it/s, est. speed input: 1277.33 toks/s, output: 214.99 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:16<00:04,  3.06it/s, est. speed input: 1269.21 toks/s, output: 222.53 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:17<00:03,  3.51it/s, est. speed input: 1260.80 toks/s, output: 229.29 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:17<00:02,  3.90it/s, est. speed input: 1248.81 toks/s, output: 235.98 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:18<00:03,  2.61it/s, est. speed input: 1204.38 toks/s, output: 236.57 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:18<00:02,  3.27it/s, est. speed input: 1199.63 toks/s, output: 244.48 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:19<00:04,  1.96it/s, est. speed input: 1140.97 toks/s, output: 241.80 toks/s]INFO 01-19 00:17:35 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 294.6 tokens/s, Running: 8 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.8%, Prefix cache hit rate: 0.0%
Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:20<00:06,  1.14it/s, est. speed input: 1061.02 toks/s, output: 232.26 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:23<00:07,  1.31s/it, est. speed input: 957.47 toks/s, output: 220.07 toks/s] Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:23<00:01,  1.68it/s, est. speed input: 961.77 toks/s, output: 251.86 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  2.87it/s, est. speed input: 966.19 toks/s, output: 283.30 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  2.87it/s, est. speed input: 966.19 toks/s, output: 283.30 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.27it/s, est. speed input: 966.19 toks/s, output: 283.30 toks/s]
[codecarbon INFO @ 00:17:38] Energy consumed for RAM : 0.000722 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:17:38] Delta energy consumed for CPU with cpu_load : 0.000060 kWh, power : 30.003194520000005 W
[codecarbon INFO @ 00:17:38] Energy consumed for All CPU : 0.000310 kWh
[codecarbon INFO @ 00:17:38] Energy consumed for all GPUs : 0.000585 kWh. Total GPU Power : 69.7385584180197 W
[codecarbon INFO @ 00:17:38] 0.001617 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:17:38', project_name='codecarbon', run_id='049d2b8b-2d35-45d7-8b2a-bb9f033f4be3', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=38.7235477687791, emissions=0.0003840656144611759, emissions_rate=9.918141198075586e-06, cpu_power=30.003194520000005, gpu_power=69.7385584180197, ram_power=70.0, cpu_energy=0.0003095603297457677, gpu_energy=0.0005847699122618621, ram_energy=0.0007221823678403679, energy_consumed=0.0016165126098479977, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 14.18s
  Throughput: 282.65 tokens/sec
  Energy per token: 0.88 J/token
  Requests/sec: 4.26
  Total time: 23.50s
  Emissions: 0.000384 kg CO2
  Total energy consumed: 0.001617 kWh

Results saved to: results/run_20260119_001739/results.json
Run 4 completed successfully
--- Run 5 of 5 ---
Using GPUs: 1
INFO 01-19 00:17:44 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:17:49] offline tracker init
[codecarbon WARNING @ 00:17:49] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:17:49] [setup] RAM Tracking...
[codecarbon INFO @ 00:17:49] [setup] CPU Tracking...
[codecarbon WARNING @ 00:17:50] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:17:50] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:17:50] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:17:50] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:17:50] [setup] GPU Tracking...
[codecarbon INFO @ 00:17:50] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:17:50] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:17:50] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:17:50] >>> Tracker's metadata:
[codecarbon INFO @ 00:17:50]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:17:50]   Python version: 3.12.3
[codecarbon INFO @ 00:17:50]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:17:50]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:17:50]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:17:50]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:17:50]   GPU count: 1
[codecarbon INFO @ 00:17:50]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:17:50] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 48
  Batch Size: 91
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:17:51 [utils.py:233] non-default args: {'seed': 42, 'block_size': 48, 'enable_prefix_caching': True, 'max_num_batched_tokens': 6596, 'max_num_seqs': 91, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:17:52 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:17:52 [model.py:1510] Using max model len 32768
INFO 01-19 00:17:54 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=6596.
INFO 01-19 00:17:54 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:17:55 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:17:55 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:17:58 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2215960)[0;0m WARNING 01-19 00:17:59 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:17:59 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:17:59 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:17:59 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:18:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2215960)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2215960)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.60s/it]
[1;36m(EngineCore_DP0 pid=2215960)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
[1;36m(EngineCore_DP0 pid=2215960)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.27s/it]
[1;36m(EngineCore_DP0 pid=2215960)[0;0m 
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:18:03 [default_loader.py:267] Loading weights took 2.61 seconds
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:18:04 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 3.536754 seconds
[codecarbon INFO @ 00:18:06] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:18:06 [gpu_worker.py:298] Available KV cache memory: 5.61 GiB
[codecarbon INFO @ 00:18:06] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.003132144000002 W
[codecarbon INFO @ 00:18:06] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:18:06] Energy consumed for all GPUs : 0.000118 kWh. Total GPU Power : 26.531479341399294 W
[codecarbon INFO @ 00:18:06] 0.000549 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:18:06 [kv_cache_utils.py:1087] GPU KV cache size: 45,936 tokens
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:18:06 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 4.27x
[1;36m(EngineCore_DP0 pid=2215960)[0;0m WARNING 01-19 00:18:06 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:18:07 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.02 seconds
[1;36m(EngineCore_DP0 pid=2215960)[0;0m INFO 01-19 00:18:07 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:18:07 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 957
INFO 01-19 00:18:07 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1385.28it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:44,  1.66s/it, est. speed input: 93.96 toks/s, output: 0.60 toks/s]Processed prompts:   2%|â–         | 2/100 [00:03<02:43,  1.67s/it, est. speed input: 64.20 toks/s, output: 0.90 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:05<02:42,  1.67s/it, est. speed input: 148.38 toks/s, output: 1.20 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:05<01:18,  1.21it/s, est. speed input: 324.07 toks/s, output: 1.87 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:05<00:44,  2.08it/s, est. speed input: 367.84 toks/s, output: 2.93 toks/s]Processed prompts:   8%|â–Š         | 8/100 [00:05<00:35,  2.56it/s, est. speed input: 396.27 toks/s, output: 3.94 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:05<00:23,  3.83it/s, est. speed input: 442.18 toks/s, output: 6.35 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:06<00:08,  9.73it/s, est. speed input: 1032.66 toks/s, output: 15.17 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:06<00:07, 10.86it/s, est. speed input: 1177.81 toks/s, output: 19.26 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:06<00:06, 11.98it/s, est. speed input: 1238.62 toks/s, output: 25.53 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:04, 15.22it/s, est. speed input: 1344.16 toks/s, output: 36.09 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:05, 11.94it/s, est. speed input: 1409.71 toks/s, output: 39.65 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:05, 12.00it/s, est. speed input: 1397.85 toks/s, output: 49.45 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:07<00:04, 14.21it/s, est. speed input: 1507.32 toks/s, output: 63.84 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:04, 12.64it/s, est. speed input: 1536.82 toks/s, output: 69.03 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:04, 11.59it/s, est. speed input: 1530.95 toks/s, output: 77.32 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:04, 11.81it/s, est. speed input: 1557.86 toks/s, output: 84.01 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:08<00:03, 12.02it/s, est. speed input: 1543.10 toks/s, output: 91.04 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:08<00:03, 12.30it/s, est. speed input: 1535.28 toks/s, output: 102.05 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:09<00:05,  7.37it/s, est. speed input: 1460.03 toks/s, output: 105.38 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:09<00:06,  6.22it/s, est. speed input: 1423.97 toks/s, output: 107.47 toks/s]INFO 01-19 00:18:17 [loggers.py:127] Engine 000: Avg prompt throughput: 2203.3 tokens/s, Avg generation throughput: 328.9 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.7%, Prefix cache hit rate: 0.0%
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:10<00:08,  4.70it/s, est. speed input: 1366.51 toks/s, output: 108.36 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:11<00:12,  3.10it/s, est. speed input: 1278.79 toks/s, output: 107.12 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:11<00:12,  3.12it/s, est. speed input: 1250.60 toks/s, output: 110.71 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:11<00:11,  3.33it/s, est. speed input: 1206.58 toks/s, output: 118.56 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:12<00:09,  3.63it/s, est. speed input: 1269.89 toks/s, output: 127.18 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:12<00:11,  3.06it/s, est. speed input: 1223.03 toks/s, output: 129.12 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:13<00:10,  3.03it/s, est. speed input: 1242.28 toks/s, output: 137.16 toks/s][codecarbon INFO @ 00:18:21] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:13<00:06,  4.60it/s, est. speed input: 1298.70 toks/s, output: 157.45 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:03,  7.41it/s, est. speed input: 1295.49 toks/s, output: 186.24 toks/s][codecarbon INFO @ 00:18:21] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.003288650625002 W
[codecarbon INFO @ 00:18:21] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:18:21] Energy consumed for all GPUs : 0.000409 kWh. Total GPU Power : 69.97239362604742 W
[codecarbon INFO @ 00:18:21] 0.001242 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:15<00:06,  3.82it/s, est. speed input: 1194.91 toks/s, output: 186.48 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:15<00:04,  4.33it/s, est. speed input: 1226.20 toks/s, output: 199.27 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:16<00:04,  4.24it/s, est. speed input: 1192.22 toks/s, output: 209.58 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:16<00:06,  2.99it/s, est. speed input: 1137.07 toks/s, output: 207.69 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:17<00:04,  3.98it/s, est. speed input: 1190.25 toks/s, output: 223.50 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:17<00:04,  3.62it/s, est. speed input: 1173.07 toks/s, output: 227.56 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:17<00:03,  4.05it/s, est. speed input: 1167.66 toks/s, output: 240.34 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:19<00:07,  1.67it/s, est. speed input: 1057.21 toks/s, output: 225.46 toks/s]INFO 01-19 00:18:27 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 345.0 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 9.3%, Prefix cache hit rate: 0.0%
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:20<00:06,  1.58it/s, est. speed input: 1021.44 toks/s, output: 227.03 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:20<00:05,  1.74it/s, est. speed input: 1004.00 toks/s, output: 232.95 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:22<00:07,  1.20it/s, est. speed input: 937.02 toks/s, output: 227.09 toks/s] Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:23<00:07,  1.10it/s, est. speed input: 893.27 toks/s, output: 227.07 toks/s]Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:23<00:00,  3.03it/s, est. speed input: 923.89 toks/s, output: 279.70 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.31it/s, est. speed input: 945.34 toks/s, output: 309.74 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.31it/s, est. speed input: 945.34 toks/s, output: 309.74 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.17it/s, est. speed input: 945.34 toks/s, output: 309.74 toks/s]
[codecarbon INFO @ 00:18:31] Energy consumed for RAM : 0.000773 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:18:32] Delta energy consumed for CPU with cpu_load : 0.000082 kWh, power : 30.00339925500001 W
[codecarbon INFO @ 00:18:32] Energy consumed for All CPU : 0.000332 kWh
[codecarbon INFO @ 00:18:32] Energy consumed for all GPUs : 0.000609 kWh. Total GPU Power : 70.08589895505041 W
[codecarbon INFO @ 00:18:32] 0.001714 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:18:32] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:18:32', project_name='codecarbon', run_id='1235f894-5e69-4bbe-a2a7-69a56089559f', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=41.37792233610526, emissions=0.00040733492076863546, emissions_rate=9.844257463193264e-06, cpu_power=30.00339925500001, gpu_power=70.08589895505041, ram_power=70.0, cpu_energy=0.0003315563639538597, gpu_energy=0.0006093979875174682, ram_energy=0.0007734975834609941, energy_consumed=0.0017144519349323222, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 16.29s
  Throughput: 308.77 tokens/sec
  Energy per token: 0.83 J/token
  Requests/sec: 4.16
  Total time: 24.03s
  Emissions: 0.000407 kg CO2
  Total energy consumed: 0.001714 kWh

Results saved to: results/run_20260119_001832/results.json
Run 5 completed successfully
========================================
Command 2: python vllm_evaluation.py --num_prompts 100 --block_size 80 --max_num_seqs 154 --max_num_batched_tokens 9273 --pipeline_parallel_size 3 --enable_chunked_prefill
Required GPUs: 3
========================================
--- Run 1 of 5 ---
Using GPUs: 1,2,3
INFO 01-19 00:18:39 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:18:46] offline tracker init
[codecarbon WARNING @ 00:18:46] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:18:46] [setup] RAM Tracking...
[codecarbon INFO @ 00:18:46] [setup] CPU Tracking...
[codecarbon WARNING @ 00:18:47] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:18:47] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:18:47] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:18:47] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:18:47] [setup] GPU Tracking...
[codecarbon INFO @ 00:18:47] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:18:47] You have 8 GPUs but we will monitor only 3 ([1, 2, 3]) of them. Check your configuration.
[codecarbon INFO @ 00:18:47] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:18:47] >>> Tracker's metadata:
[codecarbon INFO @ 00:18:47]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:18:47]   Python version: 3.12.3
[codecarbon INFO @ 00:18:47]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:18:47]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:18:47]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:18:47]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:18:47]   GPU count: 3
[codecarbon INFO @ 00:18:47]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3']
[codecarbon INFO @ 00:18:47] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 3
  Total GPUs: 3
  Block Size: 80
  Batch Size: 154
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:18:48 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9273, 'max_num_seqs': 154, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:18:49 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:18:49 [model.py:1510] Using max model len 32768
INFO 01-19 00:18:51 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9273.
INFO 01-19 00:18:51 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:52 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:52 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2217211)[0;0m WARNING 01-19 00:18:52 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_6adcc3e5'), local_subscribe_addr='ipc:///tmp/41bc7fd6-144e-4116-a1af-d9b6465e7650', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b7c0c143'), local_subscribe_addr='ipc:///tmp/945ad522-c978-4367-9d1a-20c8dd367326', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5f870a5f'), local_subscribe_addr='ipc:///tmp/3b71a93a-4868-4785-9c32-9c14ea98abeb', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6083cfeb'), local_subscribe_addr='ipc:///tmp/f2219261-f871-461c-9ee0-35725da103e9', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:55 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:56 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:56 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:18:56 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2217211)[0;0m WARNING 01-19 00:18:56 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m WARNING 01-19 00:18:56 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m WARNING 01-19 00:18:56 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m INFO 01-19 00:18:56 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m INFO 01-19 00:18:56 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m INFO 01-19 00:18:56 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m INFO 01-19 00:18:57 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m INFO 01-19 00:18:57 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m INFO 01-19 00:18:57 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m INFO 01-19 00:18:57 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m INFO 01-19 00:18:57 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m INFO 01-19 00:18:57 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m INFO 01-19 00:18:57 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m INFO 01-19 00:18:57 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m INFO 01-19 00:18:57 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m INFO 01-19 00:18:57 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m INFO 01-19 00:18:57 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m INFO 01-19 00:18:57 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m INFO 01-19 00:18:58 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m INFO 01-19 00:18:58 [default_loader.py:267] Loading weights took 0.88 seconds
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.25it/s]
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.54it/s]
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.20it/s]
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m 
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m INFO 01-19 00:18:59 [default_loader.py:267] Loading weights took 0.91 seconds
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m INFO 01-19 00:18:59 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.605368 seconds
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m INFO 01-19 00:18:59 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.856617 seconds
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m INFO 01-19 00:18:59 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 2.179025 seconds
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m INFO 01-19 00:19:01 [gpu_worker.py:298] Available KV cache memory: 14.29 GiB
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m INFO 01-19 00:19:01 [gpu_worker.py:298] Available KV cache memory: 14.11 GiB
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m INFO 01-19 00:19:01 [gpu_worker.py:298] Available KV cache memory: 14.45 GiB
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:02 [kv_cache_utils.py:1087] GPU KV cache size: 336,240 tokens
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:02 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.87x
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:02 [kv_cache_utils.py:1087] GPU KV cache size: 340,400 tokens
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:02 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.18x
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:02 [kv_cache_utils.py:1087] GPU KV cache size: 378,640 tokens
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:02 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.01x
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP2 pid=2217221)[0;0m WARNING 01-19 00:19:02 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:02 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.29 seconds
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:03 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=2217211)[0;0m INFO 01-19 00:19:03 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:19:03 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 4203
[codecarbon INFO @ 00:19:03] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
INFO 01-19 00:19:03 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m WARNING 01-19 00:19:03 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP0 pid=2217217)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1358.52it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m WARNING 01-19 00:19:03 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2217211)[0;0m [1;36m(Worker_PP1 pid=2217219)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[codecarbon INFO @ 00:19:03] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.036727200000005 W
[codecarbon INFO @ 00:19:03] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:19:03] Energy consumed for all GPUs : 0.000317 kWh. Total GPU Power : 71.21276678231078 W
[codecarbon INFO @ 00:19:03] 0.000748 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:   1%|          | 1/100 [00:02<03:58,  2.41s/it, est. speed input: 64.80 toks/s, output: 0.42 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:03<00:57,  1.65it/s, est. speed input: 361.33 toks/s, output: 1.68 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:37,  2.46it/s, est. speed input: 523.04 toks/s, output: 2.95 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:03<00:25,  3.55it/s, est. speed input: 551.04 toks/s, output: 4.96 toks/s]Processed prompts:  12%|â–ˆâ–        | 12/100 [00:03<00:15,  5.54it/s, est. speed input: 745.62 toks/s, output: 8.83 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:04<00:10,  7.95it/s, est. speed input: 987.81 toks/s, output: 15.23 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:04<00:11,  7.36it/s, est. speed input: 939.59 toks/s, output: 19.40 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:04<00:09,  8.26it/s, est. speed input: 974.06 toks/s, output: 25.15 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:04<00:07, 10.38it/s, est. speed input: 1089.82 toks/s, output: 34.46 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:09,  7.99it/s, est. speed input: 1106.61 toks/s, output: 40.25 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:05<00:07,  9.74it/s, est. speed input: 1185.83 toks/s, output: 52.29 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:05<00:07,  8.87it/s, est. speed input: 1154.50 toks/s, output: 59.28 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:10,  6.45it/s, est. speed input: 1089.99 toks/s, output: 64.94 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:10,  6.64it/s, est. speed input: 1076.61 toks/s, output: 69.43 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:18,  3.60it/s, est. speed input: 961.76 toks/s, output: 68.00 toks/s] Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:16,  4.05it/s, est. speed input: 948.62 toks/s, output: 73.60 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:16,  3.82it/s, est. speed input: 914.62 toks/s, output: 77.62 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:11,  5.57it/s, est. speed input: 942.24 toks/s, output: 90.65 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:11,  5.39it/s, est. speed input: 903.58 toks/s, output: 100.48 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:08<00:10,  5.60it/s, est. speed input: 999.79 toks/s, output: 111.24 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:09,  5.97it/s, est. speed input: 974.63 toks/s, output: 122.68 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:09,  6.04it/s, est. speed input: 1052.10 toks/s, output: 128.35 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:09<00:11,  4.85it/s, est. speed input: 1016.74 toks/s, output: 131.49 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:09<00:09,  5.32it/s, est. speed input: 1048.49 toks/s, output: 137.75 toks/s]INFO 01-19 00:19:13 [loggers.py:127] Engine 000: Avg prompt throughput: 2135.2 tokens/s, Avg generation throughput: 546.3 tokens/s, Running: 52 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:10<00:17,  2.85it/s, est. speed input: 964.65 toks/s, output: 138.18 toks/s] Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:10<00:14,  3.36it/s, est. speed input: 999.12 toks/s, output: 145.43 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:11<00:18,  2.59it/s, est. speed input: 993.69 toks/s, output: 145.90 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:13<00:33,  1.42it/s, est. speed input: 892.39 toks/s, output: 137.07 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:13<00:30,  1.54it/s, est. speed input: 894.14 toks/s, output: 141.50 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:14<00:30,  1.52it/s, est. speed input: 907.01 toks/s, output: 144.54 toks/s][codecarbon INFO @ 00:19:18] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:14<00:21,  2.09it/s, est. speed input: 900.95 toks/s, output: 158.83 toks/s][codecarbon INFO @ 00:19:18] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.030268890000002 W
[codecarbon INFO @ 00:19:18] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:19:18] Energy consumed for all GPUs : 0.001150 kWh. Total GPU Power : 200.1875036492776 W
[codecarbon INFO @ 00:19:18] 0.001984 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:15<00:19,  2.17it/s, est. speed input: 899.97 toks/s, output: 164.40 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:16<00:25,  1.65it/s, est. speed input: 863.40 toks/s, output: 163.96 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:17<00:33,  1.22it/s, est. speed input: 851.61 toks/s, output: 161.10 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:17<00:25,  1.60it/s, est. speed input: 868.98 toks/s, output: 170.37 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:18<00:21,  1.84it/s, est. speed input: 856.67 toks/s, output: 177.51 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:18<00:17,  2.16it/s, est. speed input: 853.49 toks/s, output: 185.41 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:19<00:20,  1.80it/s, est. speed input: 821.38 toks/s, output: 188.33 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:19<00:20,  1.76it/s, est. speed input: 822.88 toks/s, output: 193.07 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:20<00:18,  1.88it/s, est. speed input: 818.78 toks/s, output: 199.36 toks/s]INFO 01-19 00:19:23 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 566.7 tokens/s, Running: 35 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:21<00:21,  1.56it/s, est. speed input: 786.62 toks/s, output: 201.53 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:21<00:19,  1.69it/s, est. speed input: 783.37 toks/s, output: 207.88 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:21<00:16,  1.97it/s, est. speed input: 778.85 toks/s, output: 215.67 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:23<00:26,  1.18it/s, est. speed input: 727.99 toks/s, output: 211.39 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.18it/s, est. speed input: 963.33 toks/s, output: 548.42 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.25it/s, est. speed input: 963.33 toks/s, output: 548.42 toks/s]
[codecarbon INFO @ 00:19:27] Energy consumed for RAM : 0.000747 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:19:27] Delta energy consumed for CPU with cpu_load : 0.000070 kWh, power : 30.032336226 W
[codecarbon INFO @ 00:19:27] Energy consumed for All CPU : 0.000320 kWh
[codecarbon INFO @ 00:19:27] Energy consumed for all GPUs : 0.001650 kWh. Total GPU Power : 202.24466649090056 W
[codecarbon INFO @ 00:19:27] 0.002717 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:19:27', project_name='codecarbon', run_id='e268eb6a-f4c0-4fc9-9522-bf6e1185fb5d', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=39.984274259768426, emissions=0.0006455198676486038, emissions_rate=1.6144343735109783e-05, cpu_power=30.032336226, gpu_power=202.24466649090056, ram_power=70.0, cpu_energy=0.00032036287876232526, gpu_energy=0.0016499618755219103, ram_energy=0.0007466354907128132, energy_consumed=0.0027169602449970487, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 15.07s
  Throughput: 540.34 tokens/sec
  Energy per token: 0.76 J/token
  Requests/sec: 4.19
  Total time: 23.86s
  Emissions: 0.000646 kg CO2
  Total energy consumed: 0.002717 kWh

Results saved to: results/run_20260119_001928/results.json
Run 1 completed successfully
--- Run 2 of 5 ---
Using GPUs: 1,2,3
INFO 01-19 00:19:35 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:19:42] offline tracker init
[codecarbon WARNING @ 00:19:42] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:19:42] [setup] RAM Tracking...
[codecarbon INFO @ 00:19:42] [setup] CPU Tracking...
[codecarbon WARNING @ 00:19:43] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:19:43] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:19:43] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:19:43] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:19:43] [setup] GPU Tracking...
[codecarbon INFO @ 00:19:43] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:19:43] You have 8 GPUs but we will monitor only 3 ([1, 2, 3]) of them. Check your configuration.
[codecarbon INFO @ 00:19:43] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:19:43] >>> Tracker's metadata:
[codecarbon INFO @ 00:19:43]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:19:43]   Python version: 3.12.3
[codecarbon INFO @ 00:19:43]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:19:43]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:19:43]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:19:43]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:19:43]   GPU count: 3
[codecarbon INFO @ 00:19:43]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3']
[codecarbon INFO @ 00:19:43] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 3
  Total GPUs: 3
  Block Size: 80
  Batch Size: 154
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:19:44 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9273, 'max_num_seqs': 154, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:19:44 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:19:44 [model.py:1510] Using max model len 32768
INFO 01-19 00:19:46 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9273.
INFO 01-19 00:19:46 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:47 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:47 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2218478)[0;0m WARNING 01-19 00:19:47 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_d0d85520'), local_subscribe_addr='ipc:///tmp/482cd9e4-c8b9-44e0-896a-b00060ac4a8d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3668fa20'), local_subscribe_addr='ipc:///tmp/905373a3-c9f2-4711-9f18-659e16ff88f8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_344926d8'), local_subscribe_addr='ipc:///tmp/893f61e0-e0ce-4e61-b0b7-a7300d876200', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_69f94721'), local_subscribe_addr='ipc:///tmp/3ca1bda6-454b-4ae0-9b5b-099507b62f7d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:50 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:51 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:51 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:51 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2218478)[0;0m WARNING 01-19 00:19:51 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m WARNING 01-19 00:19:51 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m WARNING 01-19 00:19:51 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m INFO 01-19 00:19:51 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m INFO 01-19 00:19:51 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m INFO 01-19 00:19:51 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m INFO 01-19 00:19:51 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m INFO 01-19 00:19:51 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m INFO 01-19 00:19:51 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m INFO 01-19 00:19:51 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m INFO 01-19 00:19:51 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m INFO 01-19 00:19:51 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m INFO 01-19 00:19:51 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m INFO 01-19 00:19:51 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m INFO 01-19 00:19:51 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m INFO 01-19 00:19:52 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m INFO 01-19 00:19:52 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m INFO 01-19 00:19:52 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m INFO 01-19 00:19:53 [default_loader.py:267] Loading weights took 0.72 seconds
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.24it/s]
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.53it/s]
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.19it/s]
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m 
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m INFO 01-19 00:19:53 [default_loader.py:267] Loading weights took 0.91 seconds
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m INFO 01-19 00:19:53 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.421642 seconds
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m INFO 01-19 00:19:53 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m INFO 01-19 00:19:54 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.818194 seconds
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m INFO 01-19 00:19:54 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 2.015232 seconds
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m INFO 01-19 00:19:56 [gpu_worker.py:298] Available KV cache memory: 14.29 GiB
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m INFO 01-19 00:19:56 [gpu_worker.py:298] Available KV cache memory: 14.45 GiB
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m INFO 01-19 00:19:56 [gpu_worker.py:298] Available KV cache memory: 14.11 GiB
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:56 [kv_cache_utils.py:1087] GPU KV cache size: 336,240 tokens
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:56 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.87x
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:56 [kv_cache_utils.py:1087] GPU KV cache size: 340,400 tokens
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:56 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.18x
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:56 [kv_cache_utils.py:1087] GPU KV cache size: 378,640 tokens
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:56 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.01x
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP2 pid=2218490)[0;0m WARNING 01-19 00:19:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:56 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.17 seconds
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:56 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=2218478)[0;0m INFO 01-19 00:19:57 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:19:57 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 4203
INFO 01-19 00:19:57 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m WARNING 01-19 00:19:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP0 pid=2218486)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m WARNING 01-19 00:19:57 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1378.62it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2218478)[0;0m [1;36m(Worker_PP1 pid=2218488)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[codecarbon INFO @ 00:19:59] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:19:59] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.033789060000004 W
[codecarbon INFO @ 00:19:59] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:19:59] Energy consumed for all GPUs : 0.000376 kWh. Total GPU Power : 84.42612966022838 W
[codecarbon INFO @ 00:19:59] 0.000807 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:   1%|          | 1/100 [00:02<03:57,  2.40s/it, est. speed input: 65.09 toks/s, output: 0.42 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:03<00:57,  1.65it/s, est. speed input: 362.07 toks/s, output: 1.68 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:37,  2.48it/s, est. speed input: 526.05 toks/s, output: 2.97 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:03<00:25,  3.57it/s, est. speed input: 553.88 toks/s, output: 4.99 toks/s]Processed prompts:  12%|â–ˆâ–        | 12/100 [00:03<00:15,  5.58it/s, est. speed input: 749.62 toks/s, output: 8.87 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:04<00:10,  7.98it/s, est. speed input: 992.32 toks/s, output: 15.30 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:04<00:11,  7.36it/s, est. speed input: 942.83 toks/s, output: 19.47 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:04<00:09,  8.24it/s, est. speed input: 976.92 toks/s, output: 25.22 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:04<00:07, 10.36it/s, est. speed input: 1092.83 toks/s, output: 34.55 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:09,  7.97it/s, est. speed input: 1108.97 toks/s, output: 40.34 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:05<00:07,  9.70it/s, est. speed input: 1188.02 toks/s, output: 52.39 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:05<00:07,  8.87it/s, est. speed input: 1157.06 toks/s, output: 59.41 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:10,  6.46it/s, est. speed input: 1092.69 toks/s, output: 65.10 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:10,  6.63it/s, est. speed input: 1078.61 toks/s, output: 69.56 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:18,  3.60it/s, est. speed input: 963.48 toks/s, output: 68.12 toks/s] Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:16,  4.05it/s, est. speed input: 950.36 toks/s, output: 73.73 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:16,  3.82it/s, est. speed input: 916.24 toks/s, output: 77.75 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:11,  5.58it/s, est. speed input: 944.14 toks/s, output: 90.83 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:11,  5.41it/s, est. speed input: 905.50 toks/s, output: 100.69 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:08<00:10,  5.61it/s, est. speed input: 1001.82 toks/s, output: 111.47 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:09,  5.97it/s, est. speed input: 976.36 toks/s, output: 122.90 toks/s] Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:09,  6.06it/s, est. speed input: 1054.12 toks/s, output: 128.59 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:09<00:11,  4.87it/s, est. speed input: 1018.90 toks/s, output: 131.77 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:09<00:09,  5.34it/s, est. speed input: 1050.64 toks/s, output: 138.03 toks/s]INFO 01-19 00:20:07 [loggers.py:127] Engine 000: Avg prompt throughput: 2185.5 tokens/s, Avg generation throughput: 559.2 tokens/s, Running: 52 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:10<00:17,  2.85it/s, est. speed input: 966.55 toks/s, output: 138.45 toks/s] Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:10<00:14,  3.37it/s, est. speed input: 1001.08 toks/s, output: 145.71 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:11<00:18,  2.59it/s, est. speed input: 995.54 toks/s, output: 146.17 toks/s] Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:13<00:33,  1.42it/s, est. speed input: 893.94 toks/s, output: 137.31 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:13<00:30,  1.54it/s, est. speed input: 895.82 toks/s, output: 141.76 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:14<00:30,  1.53it/s, est. speed input: 908.72 toks/s, output: 144.81 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:14<00:20,  2.10it/s, est. speed input: 902.73 toks/s, output: 159.14 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:15<00:19,  2.17it/s, est. speed input: 901.62 toks/s, output: 164.70 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:16<00:25,  1.65it/s, est. speed input: 864.78 toks/s, output: 164.22 toks/s][codecarbon INFO @ 00:20:14] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:20:14] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.043737401250002 W
[codecarbon INFO @ 00:20:14] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:20:14] Energy consumed for all GPUs : 0.001236 kWh. Total GPU Power : 206.6996268105758 W
[codecarbon INFO @ 00:20:14] 0.002069 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:17<00:33,  1.22it/s, est. speed input: 852.90 toks/s, output: 161.34 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:17<00:25,  1.60it/s, est. speed input: 870.30 toks/s, output: 170.63 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:17<00:21,  1.85it/s, est. speed input: 858.25 toks/s, output: 177.84 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:18<00:17,  2.18it/s, est. speed input: 855.31 toks/s, output: 185.80 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:19<00:20,  1.81it/s, est. speed input: 822.90 toks/s, output: 188.68 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:19<00:20,  1.76it/s, est. speed input: 824.24 toks/s, output: 193.39 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:20<00:18,  1.87it/s, est. speed input: 820.04 toks/s, output: 199.67 toks/s]INFO 01-19 00:20:17 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 566.6 tokens/s, Running: 35 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:20<00:21,  1.55it/s, est. speed input: 787.45 toks/s, output: 201.74 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:21<00:19,  1.68it/s, est. speed input: 783.93 toks/s, output: 208.03 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:21<00:16,  1.95it/s, est. speed input: 779.28 toks/s, output: 215.79 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:23<00:26,  1.17it/s, est. speed input: 728.28 toks/s, output: 211.48 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.17it/s, est. speed input: 963.74 toks/s, output: 548.65 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.26it/s, est. speed input: 963.74 toks/s, output: 548.65 toks/s]
[codecarbon INFO @ 00:20:20] Energy consumed for RAM : 0.000704 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:20:21] Delta energy consumed for CPU with cpu_load : 0.000052 kWh, power : 30.06045661125 W
[codecarbon INFO @ 00:20:21] Energy consumed for All CPU : 0.000302 kWh
[codecarbon INFO @ 00:20:21] Energy consumed for all GPUs : 0.001611 kWh. Total GPU Power : 200.89154733329144 W
[codecarbon INFO @ 00:20:21] 0.002618 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:20:21] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:20:21', project_name='codecarbon', run_id='c6dcbc23-27a5-4004-85d6-12fa803ab1ea', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=37.81093237968162, emissions=0.0006219688287189833, emissions_rate=1.644944436898386e-05, cpu_power=30.06045661125, gpu_power=200.89154733329144, ram_power=70.0, cpu_energy=0.0003023128579098615, gpu_energy=0.0016111537889234029, ram_energy=0.0007043684748221292, energy_consumed=0.0026178351216553934, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 13.19s
  Throughput: 546.89 tokens/sec
  Energy per token: 0.73 J/token
  Requests/sec: 4.24
  Total time: 23.58s
  Emissions: 0.000622 kg CO2
  Total energy consumed: 0.002618 kWh

Results saved to: results/run_20260119_002022/results.json
Run 2 completed successfully
--- Run 3 of 5 ---
Using GPUs: 1,2,3
INFO 01-19 00:20:29 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:20:35] offline tracker init
[codecarbon WARNING @ 00:20:35] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:20:35] [setup] RAM Tracking...
[codecarbon INFO @ 00:20:35] [setup] CPU Tracking...
[codecarbon WARNING @ 00:20:36] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:20:36] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:20:36] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:20:36] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:20:36] [setup] GPU Tracking...
[codecarbon INFO @ 00:20:36] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:20:36] You have 8 GPUs but we will monitor only 3 ([1, 2, 3]) of them. Check your configuration.
[codecarbon INFO @ 00:20:36] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:20:36] >>> Tracker's metadata:
[codecarbon INFO @ 00:20:36]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:20:36]   Python version: 3.12.3
[codecarbon INFO @ 00:20:36]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:20:36]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:20:36]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:20:36]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:20:36]   GPU count: 3
[codecarbon INFO @ 00:20:36]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3']
[codecarbon INFO @ 00:20:36] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 3
  Total GPUs: 3
  Block Size: 80
  Batch Size: 154
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:20:37 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9273, 'max_num_seqs': 154, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:20:38 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:20:38 [model.py:1510] Using max model len 32768
INFO 01-19 00:20:40 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9273.
INFO 01-19 00:20:40 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:41 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:41 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2221256)[0;0m WARNING 01-19 00:20:41 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_5660ad5a'), local_subscribe_addr='ipc:///tmp/d0a5529f-5ea1-4469-a88d-595bb75e205e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_55a484da'), local_subscribe_addr='ipc:///tmp/ca21e942-0bcf-4852-897b-59275717f71e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_85778c66'), local_subscribe_addr='ipc:///tmp/0f588c76-7697-424f-a661-006718ba1797', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_69296fce'), local_subscribe_addr='ipc:///tmp/c555bb33-4322-432e-acb0-89e559cb4fb0', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:45 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:46 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:46 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:46 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2221256)[0;0m WARNING 01-19 00:20:46 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m WARNING 01-19 00:20:46 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m WARNING 01-19 00:20:46 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m INFO 01-19 00:20:46 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m INFO 01-19 00:20:46 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m INFO 01-19 00:20:46 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m INFO 01-19 00:20:47 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m INFO 01-19 00:20:47 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m INFO 01-19 00:20:47 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m INFO 01-19 00:20:47 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m INFO 01-19 00:20:47 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m INFO 01-19 00:20:47 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m INFO 01-19 00:20:47 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m INFO 01-19 00:20:47 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m INFO 01-19 00:20:47 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m INFO 01-19 00:20:47 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m INFO 01-19 00:20:47 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m INFO 01-19 00:20:47 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m INFO 01-19 00:20:48 [default_loader.py:267] Loading weights took 1.05 seconds
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.11it/s]
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.30it/s]
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.98it/s]
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m 
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m INFO 01-19 00:20:49 [default_loader.py:267] Loading weights took 1.01 seconds
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m INFO 01-19 00:20:49 [default_loader.py:267] Loading weights took 0.82 seconds
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m INFO 01-19 00:20:49 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.764146 seconds
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m INFO 01-19 00:20:49 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.994123 seconds
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m INFO 01-19 00:20:49 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.990604 seconds
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m INFO 01-19 00:20:51 [gpu_worker.py:298] Available KV cache memory: 14.29 GiB
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m INFO 01-19 00:20:51 [gpu_worker.py:298] Available KV cache memory: 14.11 GiB
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m INFO 01-19 00:20:51 [gpu_worker.py:298] Available KV cache memory: 14.45 GiB
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:52 [kv_cache_utils.py:1087] GPU KV cache size: 336,240 tokens
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:52 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.87x
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:52 [kv_cache_utils.py:1087] GPU KV cache size: 340,400 tokens
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:52 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.18x
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:52 [kv_cache_utils.py:1087] GPU KV cache size: 378,640 tokens
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:52 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.01x
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP2 pid=2221266)[0;0m WARNING 01-19 00:20:52 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[codecarbon INFO @ 00:20:52] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:52 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.38 seconds
[codecarbon INFO @ 00:20:52] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.032515794000005 W
[codecarbon INFO @ 00:20:52] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:20:52] Energy consumed for all GPUs : 0.000312 kWh. Total GPU Power : 69.98523288084658 W
[codecarbon INFO @ 00:20:52] 0.000743 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:53 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=2221256)[0;0m INFO 01-19 00:20:53 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:20:53 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 4203
INFO 01-19 00:20:53 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m WARNING 01-19 00:20:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP0 pid=2221262)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m WARNING 01-19 00:20:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1367.93it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2221256)[0;0m [1;36m(Worker_PP1 pid=2221264)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
Processed prompts:   1%|          | 1/100 [00:02<03:56,  2.39s/it, est. speed input: 65.25 toks/s, output: 0.42 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:03<00:57,  1.65it/s, est. speed input: 361.74 toks/s, output: 1.68 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:37,  2.46it/s, est. speed input: 523.83 toks/s, output: 2.96 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:03<00:25,  3.55it/s, est. speed input: 551.85 toks/s, output: 4.97 toks/s]Processed prompts:  12%|â–ˆâ–        | 12/100 [00:03<00:15,  5.55it/s, est. speed input: 747.00 toks/s, output: 8.84 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:04<00:10,  7.96it/s, est. speed input: 989.60 toks/s, output: 15.26 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:04<00:11,  7.35it/s, est. speed input: 940.63 toks/s, output: 19.43 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:04<00:09,  8.25it/s, est. speed input: 975.01 toks/s, output: 25.18 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:04<00:07, 10.33it/s, est. speed input: 1090.17 toks/s, output: 34.47 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:09,  7.93it/s, est. speed input: 1105.92 toks/s, output: 40.23 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:05<00:07,  9.66it/s, est. speed input: 1184.71 toks/s, output: 52.24 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:05<00:07,  8.84it/s, est. speed input: 1153.89 toks/s, output: 59.24 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:10,  6.45it/s, est. speed input: 1090.04 toks/s, output: 64.94 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:10,  6.64it/s, est. speed input: 1076.44 toks/s, output: 69.42 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:18,  3.59it/s, est. speed input: 961.62 toks/s, output: 67.99 toks/s] Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:16,  4.05it/s, est. speed input: 948.51 toks/s, output: 73.59 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:16,  3.82it/s, est. speed input: 914.67 toks/s, output: 77.62 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:11,  5.58it/s, est. speed input: 942.38 toks/s, output: 90.66 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:11,  5.40it/s, est. speed input: 903.72 toks/s, output: 100.49 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:08<00:10,  5.60it/s, est. speed input: 999.90 toks/s, output: 111.26 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:09,  5.95it/s, est. speed input: 974.36 toks/s, output: 122.65 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:09,  6.04it/s, est. speed input: 1051.90 toks/s, output: 128.32 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:09<00:11,  4.87it/s, est. speed input: 1017.12 toks/s, output: 131.54 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:09<00:09,  5.33it/s, est. speed input: 1048.57 toks/s, output: 137.76 toks/s]INFO 01-19 00:21:03 [loggers.py:127] Engine 000: Avg prompt throughput: 2140.2 tokens/s, Avg generation throughput: 547.6 tokens/s, Running: 52 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:10<00:17,  2.86it/s, est. speed input: 965.33 toks/s, output: 138.28 toks/s] Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:10<00:14,  3.37it/s, est. speed input: 999.71 toks/s, output: 145.51 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:11<00:18,  2.60it/s, est. speed input: 994.29 toks/s, output: 145.98 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:13<00:33,  1.42it/s, est. speed input: 893.12 toks/s, output: 137.18 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:13<00:30,  1.54it/s, est. speed input: 894.76 toks/s, output: 141.60 toks/s][codecarbon INFO @ 00:21:07] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:21:07] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.0312421725 W
[codecarbon INFO @ 00:21:07] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:21:07] Energy consumed for all GPUs : 0.001112 kWh. Total GPU Power : 192.36256721043742 W
[codecarbon INFO @ 00:21:07] 0.001946 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:14<00:30,  1.53it/s, est. speed input: 907.82 toks/s, output: 144.67 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:14<00:20,  2.10it/s, est. speed input: 901.73 toks/s, output: 158.97 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:15<00:19,  2.17it/s, est. speed input: 900.60 toks/s, output: 164.51 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:16<00:25,  1.65it/s, est. speed input: 863.94 toks/s, output: 164.06 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:17<00:33,  1.22it/s, est. speed input: 852.22 toks/s, output: 161.21 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:17<00:25,  1.60it/s, est. speed input: 869.43 toks/s, output: 170.46 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:18<00:21,  1.84it/s, est. speed input: 857.29 toks/s, output: 177.64 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:18<00:17,  2.17it/s, est. speed input: 854.44 toks/s, output: 185.61 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:19<00:20,  1.80it/s, est. speed input: 822.06 toks/s, output: 188.48 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:19<00:20,  1.76it/s, est. speed input: 823.52 toks/s, output: 193.22 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:20<00:18,  1.88it/s, est. speed input: 819.37 toks/s, output: 199.51 toks/s]INFO 01-19 00:21:13 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 566.6 tokens/s, Running: 35 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:21<00:21,  1.56it/s, est. speed input: 787.18 toks/s, output: 201.67 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:21<00:19,  1.69it/s, est. speed input: 783.88 toks/s, output: 208.02 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:21<00:16,  1.97it/s, est. speed input: 779.44 toks/s, output: 215.84 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:23<00:26,  1.18it/s, est. speed input: 728.72 toks/s, output: 211.61 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.18it/s, est. speed input: 964.28 toks/s, output: 548.96 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.26it/s, est. speed input: 964.28 toks/s, output: 548.96 toks/s]
[codecarbon INFO @ 00:21:17] Energy consumed for RAM : 0.000764 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:21:17] Delta energy consumed for CPU with cpu_load : 0.000077 kWh, power : 30.03286280454546 W
[codecarbon INFO @ 00:21:17] Energy consumed for All CPU : 0.000328 kWh
[codecarbon INFO @ 00:21:17] Energy consumed for all GPUs : 0.001667 kWh. Total GPU Power : 204.3830593937302 W
[codecarbon INFO @ 00:21:17] 0.002759 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:21:17', project_name='codecarbon', run_id='66aea397-dbaf-4b25-a800-219de473c2ec', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.87636873219162, emissions=0.000655479706966048, emissions_rate=1.603566381496686e-05, cpu_power=30.03286280454546, gpu_power=204.3830593937302, ram_power=70.0, cpu_energy=0.00032771654210882634, gpu_energy=0.0016673568894400859, ram_energy=0.0007638072698558163, energy_consumed=0.0027588807014047286, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 15.99s
  Throughput: 540.99 tokens/sec
  Energy per token: 0.77 J/token
  Requests/sec: 4.20
  Total time: 23.83s
  Emissions: 0.000655 kg CO2
  Total energy consumed: 0.002759 kWh

Results saved to: results/run_20260119_002118/results.json
Run 3 completed successfully
--- Run 4 of 5 ---
Using GPUs: 1,2,3
INFO 01-19 00:21:25 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:21:32] offline tracker init
[codecarbon WARNING @ 00:21:32] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:21:32] [setup] RAM Tracking...
[codecarbon INFO @ 00:21:32] [setup] CPU Tracking...
[codecarbon WARNING @ 00:21:33] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:21:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:21:33] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:21:33] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:21:33] [setup] GPU Tracking...
[codecarbon INFO @ 00:21:33] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:21:33] You have 8 GPUs but we will monitor only 3 ([1, 2, 3]) of them. Check your configuration.
[codecarbon INFO @ 00:21:33] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:21:33] >>> Tracker's metadata:
[codecarbon INFO @ 00:21:33]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:21:33]   Python version: 3.12.3
[codecarbon INFO @ 00:21:33]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:21:33]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:21:33]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:21:33]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:21:33]   GPU count: 3
[codecarbon INFO @ 00:21:33]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3']
[codecarbon INFO @ 00:21:33] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 3
  Total GPUs: 3
  Block Size: 80
  Batch Size: 154
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:21:34 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9273, 'max_num_seqs': 154, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:21:34 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:21:34 [model.py:1510] Using max model len 32768
INFO 01-19 00:21:37 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9273.
INFO 01-19 00:21:37 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:37 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:37 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2222566)[0;0m WARNING 01-19 00:21:37 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_054da253'), local_subscribe_addr='ipc:///tmp/4dfec8bb-26ee-4ac3-a471-6a28114d747a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7c9d114b'), local_subscribe_addr='ipc:///tmp/cdc4b54e-607d-4fb1-859f-f76f61c436d7', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c1e2f7cc'), local_subscribe_addr='ipc:///tmp/42bd40ef-129e-424e-b40b-c43890f84cb1', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5f6e748e'), local_subscribe_addr='ipc:///tmp/c13c2e71-c94e-48c7-98be-ef83a6530db8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:42 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:42 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:42 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:43 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:43 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:43 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2222566)[0;0m WARNING 01-19 00:21:43 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m WARNING 01-19 00:21:43 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m WARNING 01-19 00:21:43 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m INFO 01-19 00:21:43 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m INFO 01-19 00:21:43 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m INFO 01-19 00:21:43 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m INFO 01-19 00:21:44 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m INFO 01-19 00:21:44 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m INFO 01-19 00:21:44 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m INFO 01-19 00:21:44 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m INFO 01-19 00:21:44 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m INFO 01-19 00:21:44 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m INFO 01-19 00:21:44 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m INFO 01-19 00:21:44 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m INFO 01-19 00:21:44 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m INFO 01-19 00:21:44 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m INFO 01-19 00:21:44 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m INFO 01-19 00:21:44 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.23it/s]
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.50it/s]
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.16it/s]
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m 
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m INFO 01-19 00:21:45 [default_loader.py:267] Loading weights took 0.93 seconds
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m INFO 01-19 00:21:45 [default_loader.py:267] Loading weights took 0.84 seconds
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m INFO 01-19 00:21:46 [default_loader.py:267] Loading weights took 0.79 seconds
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m INFO 01-19 00:21:46 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.660908 seconds
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m INFO 01-19 00:21:46 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 1.867198 seconds
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m INFO 01-19 00:21:46 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 2.095780 seconds
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m INFO 01-19 00:21:48 [gpu_worker.py:298] Available KV cache memory: 14.29 GiB
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m INFO 01-19 00:21:48 [gpu_worker.py:298] Available KV cache memory: 14.45 GiB
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m INFO 01-19 00:21:48 [gpu_worker.py:298] Available KV cache memory: 14.11 GiB
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [kv_cache_utils.py:1087] GPU KV cache size: 336,240 tokens
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.87x
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [kv_cache_utils.py:1087] GPU KV cache size: 340,400 tokens
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.18x
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [kv_cache_utils.py:1087] GPU KV cache size: 378,640 tokens
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.01x
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP2 pid=2222576)[0;0m WARNING 01-19 00:21:49 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[codecarbon INFO @ 00:21:49] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.32 seconds
[codecarbon INFO @ 00:21:49] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.033441570000004 W
[codecarbon INFO @ 00:21:49] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:21:49] Energy consumed for all GPUs : 0.000315 kWh. Total GPU Power : 70.83422887986835 W
[codecarbon INFO @ 00:21:49] 0.000747 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=2222566)[0;0m INFO 01-19 00:21:49 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:21:50 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 4203
INFO 01-19 00:21:50 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m WARNING 01-19 00:21:50 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP0 pid=2222572)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m WARNING 01-19 00:21:50 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1374.16it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2222566)[0;0m [1;36m(Worker_PP1 pid=2222574)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
Processed prompts:   1%|          | 1/100 [00:02<03:57,  2.40s/it, est. speed input: 65.03 toks/s, output: 0.42 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:03<00:57,  1.65it/s, est. speed input: 361.20 toks/s, output: 1.68 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:37,  2.46it/s, est. speed input: 523.54 toks/s, output: 2.95 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:03<00:25,  3.56it/s, est. speed input: 551.68 toks/s, output: 4.97 toks/s]Processed prompts:  12%|â–ˆâ–        | 12/100 [00:03<00:15,  5.55it/s, est. speed input: 746.83 toks/s, output: 8.84 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:04<00:10,  7.96it/s, est. speed input: 988.98 toks/s, output: 15.25 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:04<00:11,  7.36it/s, est. speed input: 940.29 toks/s, output: 19.42 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:04<00:09,  8.27it/s, est. speed input: 974.99 toks/s, output: 25.17 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:04<00:07, 10.36it/s, est. speed input: 1090.43 toks/s, output: 34.48 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:09,  7.98it/s, est. speed input: 1106.91 toks/s, output: 40.26 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:05<00:07,  9.69it/s, est. speed input: 1185.49 toks/s, output: 52.28 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:05<00:07,  8.85it/s, est. speed input: 1154.33 toks/s, output: 59.27 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:10,  6.45it/s, est. speed input: 1090.25 toks/s, output: 64.96 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:10,  6.65it/s, est. speed input: 1076.89 toks/s, output: 69.45 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:18,  3.59it/s, est. speed input: 961.73 toks/s, output: 68.00 toks/s] Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:16,  4.05it/s, est. speed input: 948.46 toks/s, output: 73.58 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:16,  3.81it/s, est. speed input: 914.63 toks/s, output: 77.62 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:11,  5.56it/s, est. speed input: 942.18 toks/s, output: 90.64 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:11,  5.40it/s, est. speed input: 903.63 toks/s, output: 100.48 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:08<00:10,  5.60it/s, est. speed input: 999.91 toks/s, output: 111.26 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:09,  5.95it/s, est. speed input: 974.31 toks/s, output: 122.64 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:09,  6.04it/s, est. speed input: 1051.93 toks/s, output: 128.33 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:09<00:11,  4.87it/s, est. speed input: 1016.93 toks/s, output: 131.51 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:09<00:09,  5.31it/s, est. speed input: 1048.21 toks/s, output: 137.72 toks/s]INFO 01-19 00:22:00 [loggers.py:127] Engine 000: Avg prompt throughput: 2136.5 tokens/s, Avg generation throughput: 546.6 tokens/s, Running: 52 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:10<00:17,  2.86it/s, est. speed input: 965.24 toks/s, output: 138.27 toks/s] Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:10<00:14,  3.37it/s, est. speed input: 999.74 toks/s, output: 145.52 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:11<00:18,  2.60it/s, est. speed input: 994.22 toks/s, output: 145.97 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:13<00:33,  1.42it/s, est. speed input: 892.77 toks/s, output: 137.13 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:13<00:30,  1.54it/s, est. speed input: 894.67 toks/s, output: 141.58 toks/s][codecarbon INFO @ 00:22:04] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:22:04] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.033457050000003 W
[codecarbon INFO @ 00:22:04] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:22:04] Energy consumed for all GPUs : 0.001115 kWh. Total GPU Power : 192.08453108993834 W
[codecarbon INFO @ 00:22:04] 0.001948 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:14<00:30,  1.53it/s, est. speed input: 907.71 toks/s, output: 144.65 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:14<00:20,  2.10it/s, est. speed input: 901.79 toks/s, output: 158.98 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:15<00:19,  2.18it/s, est. speed input: 900.72 toks/s, output: 164.54 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:16<00:25,  1.65it/s, est. speed input: 863.99 toks/s, output: 164.07 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:17<00:33,  1.22it/s, est. speed input: 852.48 toks/s, output: 161.26 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:17<00:24,  1.60it/s, est. speed input: 869.88 toks/s, output: 170.55 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:18<00:21,  1.85it/s, est. speed input: 857.91 toks/s, output: 177.77 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:18<00:17,  2.18it/s, est. speed input: 855.00 toks/s, output: 185.73 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:19<00:20,  1.81it/s, est. speed input: 822.72 toks/s, output: 188.63 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:19<00:20,  1.77it/s, est. speed input: 824.28 toks/s, output: 193.40 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:20<00:18,  1.88it/s, est. speed input: 820.03 toks/s, output: 199.67 toks/s]INFO 01-19 00:22:10 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 567.5 tokens/s, Running: 35 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:20<00:21,  1.56it/s, est. speed input: 787.79 toks/s, output: 201.83 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:21<00:19,  1.70it/s, est. speed input: 784.53 toks/s, output: 208.19 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:21<00:16,  1.98it/s, est. speed input: 780.11 toks/s, output: 216.02 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:23<00:26,  1.18it/s, est. speed input: 729.15 toks/s, output: 211.73 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.18it/s, est. speed input: 964.97 toks/s, output: 549.35 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.26it/s, est. speed input: 964.97 toks/s, output: 549.35 toks/s]
[codecarbon INFO @ 00:22:14] Energy consumed for RAM : 0.000766 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:22:14] Delta energy consumed for CPU with cpu_load : 0.000078 kWh, power : 30.03559105636364 W
[codecarbon INFO @ 00:22:14] Energy consumed for All CPU : 0.000328 kWh
[codecarbon INFO @ 00:22:14] Energy consumed for all GPUs : 0.001672 kWh. Total GPU Power : 203.13812883453963 W
[codecarbon INFO @ 00:22:14] 0.002766 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:22:14] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:22:14', project_name='codecarbon', run_id='12e2111c-a6d6-466c-b21a-1d1217cd2301', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.95941026508808, emissions=0.0006570890684697299, emissions_rate=1.604244456199611e-05, cpu_power=30.03559105636364, gpu_power=203.13812883453963, ram_power=70.0, cpu_energy=0.0003284842958666864, gpu_energy=0.001671625781737518, ram_energy=0.0007655443444007284, energy_consumed=0.0027656544220049325, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 16.09s
  Throughput: 541.34 tokens/sec
  Energy per token: 0.77 J/token
  Requests/sec: 4.20
  Total time: 23.82s
  Emissions: 0.000657 kg CO2
  Total energy consumed: 0.002766 kWh

Results saved to: results/run_20260119_002215/results.json
Run 4 completed successfully
--- Run 5 of 5 ---
Using GPUs: 1,2,3
INFO 01-19 00:22:23 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:22:29] offline tracker init
[codecarbon WARNING @ 00:22:29] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:22:29] [setup] RAM Tracking...
[codecarbon INFO @ 00:22:29] [setup] CPU Tracking...
[codecarbon WARNING @ 00:22:31] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:22:31] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:22:31] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:22:31] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:22:31] [setup] GPU Tracking...
[codecarbon INFO @ 00:22:31] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:22:31] You have 8 GPUs but we will monitor only 3 ([1, 2, 3]) of them. Check your configuration.
[codecarbon INFO @ 00:22:31] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:22:31] >>> Tracker's metadata:
[codecarbon INFO @ 00:22:31]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:22:31]   Python version: 3.12.3
[codecarbon INFO @ 00:22:31]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:22:31]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:22:31]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:22:31]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:22:31]   GPU count: 3
[codecarbon INFO @ 00:22:31]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3']
[codecarbon INFO @ 00:22:31] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 3
  Total GPUs: 3
  Block Size: 80
  Batch Size: 154
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:22:31 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 3, 'block_size': 80, 'enable_prefix_caching': False, 'max_num_batched_tokens': 9273, 'max_num_seqs': 154, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:22:32 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:22:32 [model.py:1510] Using max model len 32768
INFO 01-19 00:22:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=9273.
INFO 01-19 00:22:34 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:34 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:34 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=3, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2223886)[0;0m WARNING 01-19 00:22:34 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2], buffer_handle=(3, 16777216, 10, 'psm_675bbfbe'), local_subscribe_addr='ipc:///tmp/dc9871b9-3833-4146-95fd-49a028cf37e1', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5af0aafc'), local_subscribe_addr='ipc:///tmp/f9741d45-20b1-4b03-9049-72c8b452b30d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5672b34e'), local_subscribe_addr='ipc:///tmp/cd8a46b5-39ed-4dc3-b877-1b621b97e623', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_dba96a0f'), local_subscribe_addr='ipc:///tmp/f73cfaa1-a08a-454d-8b89-9affccc42ca8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 2 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[Gloo] Rank 1 is connected to 2 peer ranks. Expected number of connected peer ranks is : 2
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [parallel_state.py:1208] rank 0 in world size 3 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [parallel_state.py:1208] rank 2 in world size 3 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:39 [parallel_state.py:1208] rank 1 in world size 3 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2223886)[0;0m WARNING 01-19 00:22:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m WARNING 01-19 00:22:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m WARNING 01-19 00:22:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m INFO 01-19 00:22:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m INFO 01-19 00:22:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m INFO 01-19 00:22:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m INFO 01-19 00:22:40 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m INFO 01-19 00:22:40 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m INFO 01-19 00:22:40 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m INFO 01-19 00:22:40 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m INFO 01-19 00:22:40 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m INFO 01-19 00:22:40 [utils.py:125] Hidden layers were unevenly partitioned: [11,11,10]. This can be manually overridden using the VLLM_PP_LAYER_PARTITION environment variable
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m INFO 01-19 00:22:40 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m INFO 01-19 00:22:40 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m INFO 01-19 00:22:40 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m INFO 01-19 00:22:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m INFO 01-19 00:22:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m INFO 01-19 00:22:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.32it/s]
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.35it/s]
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m 
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m INFO 01-19 00:22:41 [default_loader.py:267] Loading weights took 0.85 seconds
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m INFO 01-19 00:22:41 [default_loader.py:267] Loading weights took 0.79 seconds
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m INFO 01-19 00:22:42 [gpu_model_runner.py:2653] Model loading took 4.7209 GiB and 1.550567 seconds
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m INFO 01-19 00:22:42 [default_loader.py:267] Loading weights took 0.84 seconds
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m INFO 01-19 00:22:42 [gpu_model_runner.py:2653] Model loading took 4.3146 GiB and 1.685081 seconds
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m INFO 01-19 00:22:42 [gpu_model_runner.py:2653] Model loading took 4.4768 GiB and 2.053204 seconds
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m INFO 01-19 00:22:44 [gpu_worker.py:298] Available KV cache memory: 14.29 GiB
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m INFO 01-19 00:22:44 [gpu_worker.py:298] Available KV cache memory: 14.11 GiB
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m INFO 01-19 00:22:44 [gpu_worker.py:298] Available KV cache memory: 14.45 GiB
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [kv_cache_utils.py:1087] GPU KV cache size: 336,240 tokens
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.87x
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [kv_cache_utils.py:1087] GPU KV cache size: 340,400 tokens
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 25.18x
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [kv_cache_utils.py:1087] GPU KV cache size: 378,640 tokens
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 28.01x
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP2 pid=2223897)[0;0m WARNING 01-19 00:22:45 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.25 seconds
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [core.py:149] Batch queue is enabled with size 3
[1;36m(EngineCore_DP0 pid=2223886)[0;0m INFO 01-19 00:22:45 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:22:45 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 4203
INFO 01-19 00:22:46 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m WARNING 01-19 00:22:46 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP0 pid=2223893)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m WARNING 01-19 00:22:46 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1359.22it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning: The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2223886)[0;0m [1;36m(Worker_PP1 pid=2223895)[0;0m   object_tensor = torch.frombuffer(pickle.dumps(obj), dtype=torch.uint8)
[codecarbon INFO @ 00:22:46] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:22:47] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.034225776000007 W
[codecarbon INFO @ 00:22:47] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:22:47] Energy consumed for all GPUs : 0.000342 kWh. Total GPU Power : 76.84251425483875 W
[codecarbon INFO @ 00:22:47] 0.000773 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:   1%|          | 1/100 [00:02<03:58,  2.41s/it, est. speed input: 64.74 toks/s, output: 0.42 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:03<00:57,  1.65it/s, est. speed input: 360.67 toks/s, output: 1.68 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:37,  2.47it/s, est. speed input: 524.22 toks/s, output: 2.96 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:03<00:25,  3.57it/s, est. speed input: 552.33 toks/s, output: 4.98 toks/s]Processed prompts:  12%|â–ˆâ–        | 12/100 [00:03<00:15,  5.57it/s, est. speed input: 747.70 toks/s, output: 8.85 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:04<00:10,  8.00it/s, est. speed input: 990.99 toks/s, output: 15.28 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:04<00:11,  7.39it/s, est. speed input: 941.99 toks/s, output: 19.45 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:04<00:09,  8.28it/s, est. speed input: 976.25 toks/s, output: 25.21 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:04<00:07, 10.36it/s, est. speed input: 1091.63 toks/s, output: 34.51 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:09,  7.96it/s, est. speed input: 1107.70 toks/s, output: 40.29 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:05<00:07,  9.69it/s, est. speed input: 1186.44 toks/s, output: 52.32 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:05<00:07,  8.86it/s, est. speed input: 1155.53 toks/s, output: 59.33 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:10,  6.45it/s, est. speed input: 1091.24 toks/s, output: 65.02 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:10,  6.63it/s, est. speed input: 1077.30 toks/s, output: 69.48 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:07<00:18,  3.58it/s, est. speed input: 961.92 toks/s, output: 68.01 toks/s] Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:07<00:16,  4.04it/s, est. speed input: 948.72 toks/s, output: 73.60 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:16,  3.79it/s, est. speed input: 914.27 toks/s, output: 77.59 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:11,  5.54it/s, est. speed input: 941.89 toks/s, output: 90.61 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:08<00:11,  5.34it/s, est. speed input: 902.41 toks/s, output: 100.35 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:08<00:10,  5.55it/s, est. speed input: 998.39 toks/s, output: 111.09 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:09,  5.93it/s, est. speed input: 973.31 toks/s, output: 122.51 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:09,  6.01it/s, est. speed input: 1050.68 toks/s, output: 128.17 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:09<00:11,  4.84it/s, est. speed input: 1015.54 toks/s, output: 131.33 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:09<00:10,  5.30it/s, est. speed input: 1046.94 toks/s, output: 137.55 toks/s]INFO 01-19 00:22:56 [loggers.py:127] Engine 000: Avg prompt throughput: 2170.1 tokens/s, Avg generation throughput: 555.2 tokens/s, Running: 52 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 0.0%
Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:10<00:17,  2.85it/s, est. speed input: 963.70 toks/s, output: 138.05 toks/s] Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:10<00:14,  3.36it/s, est. speed input: 998.09 toks/s, output: 145.28 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:11<00:18,  2.59it/s, est. speed input: 992.65 toks/s, output: 145.74 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:13<00:33,  1.42it/s, est. speed input: 892.00 toks/s, output: 137.01 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:13<00:30,  1.54it/s, est. speed input: 893.98 toks/s, output: 141.47 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:14<00:30,  1.53it/s, est. speed input: 907.09 toks/s, output: 144.55 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:14<00:20,  2.10it/s, est. speed input: 901.04 toks/s, output: 158.84 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:15<00:19,  2.18it/s, est. speed input: 900.02 toks/s, output: 164.41 toks/s][codecarbon INFO @ 00:23:01] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:23:02] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.03159118125 W
[codecarbon INFO @ 00:23:02] Energy consumed for All CPU : 0.000250 kWh
Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:16<00:25,  1.65it/s, est. speed input: 863.25 toks/s, output: 163.93 toks/s][codecarbon INFO @ 00:23:02] Energy consumed for all GPUs : 0.001199 kWh. Total GPU Power : 205.95844978489865 W
[codecarbon INFO @ 00:23:02] 0.002033 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:17<00:33,  1.22it/s, est. speed input: 851.57 toks/s, output: 161.09 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:17<00:25,  1.60it/s, est. speed input: 868.97 toks/s, output: 170.37 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:18<00:21,  1.84it/s, est. speed input: 856.85 toks/s, output: 177.55 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:18<00:17,  2.17it/s, est. speed input: 853.92 toks/s, output: 185.50 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:19<00:20,  1.80it/s, est. speed input: 821.56 toks/s, output: 188.37 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:19<00:20,  1.76it/s, est. speed input: 823.08 toks/s, output: 193.12 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:20<00:18,  1.87it/s, est. speed input: 818.89 toks/s, output: 199.39 toks/s]INFO 01-19 00:23:06 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 566.9 tokens/s, Running: 35 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.4%, Prefix cache hit rate: 0.0%
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:21<00:21,  1.55it/s, est. speed input: 786.61 toks/s, output: 201.53 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:21<00:19,  1.69it/s, est. speed input: 783.26 toks/s, output: 207.85 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:21<00:16,  1.96it/s, est. speed input: 778.74 toks/s, output: 215.64 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:23<00:26,  1.17it/s, est. speed input: 727.74 toks/s, output: 211.32 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.17it/s, est. speed input: 963.09 toks/s, output: 548.28 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.25it/s, est. speed input: 963.09 toks/s, output: 548.28 toks/s]
[codecarbon INFO @ 00:23:09] Energy consumed for RAM : 0.000726 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:23:10] Delta energy consumed for CPU with cpu_load : 0.000061 kWh, power : 30.034619790000004 W
[codecarbon INFO @ 00:23:10] Energy consumed for All CPU : 0.000311 kWh
[codecarbon INFO @ 00:23:10] Energy consumed for all GPUs : 0.001634 kWh. Total GPU Power : 200.08859834582185 W
[codecarbon INFO @ 00:23:10] 0.002671 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:23:10', project_name='codecarbon', run_id='8d314706-e4ac-4a0a-9fee-d69982630923', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=38.91412064805627, emissions=0.0006345169700486499, emissions_rate=1.630557133199266e-05, cpu_power=30.034619790000004, gpu_power=200.08859834582185, ram_power=70.0, cpu_energy=0.000311355940989734, gpu_energy=0.0016336496402544753, ram_energy=0.0007256440281596346, energy_consumed=0.002670649609403844, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=3, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 14.27s
  Throughput: 546.51 tokens/sec
  Energy per token: 0.75 J/token
  Requests/sec: 4.24
  Total time: 23.59s
  Emissions: 0.000635 kg CO2
  Total energy consumed: 0.002671 kWh

Results saved to: results/run_20260119_002311/results.json
Run 5 completed successfully
========================================
Command 3: python vllm_evaluation.py --num_prompts 100 --tensor_parallel_size 4 --enable_chunked_prefill
Required GPUs: 4
========================================
--- Run 1 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:23:17 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:23:23] offline tracker init
[codecarbon WARNING @ 00:23:23] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:23:24] [setup] RAM Tracking...
[codecarbon INFO @ 00:23:24] [setup] CPU Tracking...
[codecarbon WARNING @ 00:23:25] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:23:25] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:23:25] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:23:25] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:23:25] [setup] GPU Tracking...
[codecarbon INFO @ 00:23:25] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:23:25] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:23:25] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:23:25] >>> Tracker's metadata:
[codecarbon INFO @ 00:23:25]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:23:25]   Python version: 3.12.3
[codecarbon INFO @ 00:23:25]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:23:25]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:23:25]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:23:25]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:23:25]   GPU count: 4
[codecarbon INFO @ 00:23:25]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:23:25] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 16
  Batch Size: 1024
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:23:25 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 16, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:23:26 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:23:26 [model.py:1510] Using max model len 32768
INFO 01-19 00:23:29 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 01-19 00:23:29 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:29 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:29 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:29 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_64ef051a'), local_subscribe_addr='ipc:///tmp/015820b9-a3fc-4830-b6b7-4bf9c2f86953', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c817f24f'), local_subscribe_addr='ipc:///tmp/b74aaab4-a1d7-4456-8c21-c3b9fb6eeed8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_95411297'), local_subscribe_addr='ipc:///tmp/b8f0e0ae-7740-47c8-96c0-2b375fc385a7', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_75da0df9'), local_subscribe_addr='ipc:///tmp/a77406d9-2988-4392-bfad-e141da8af283', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_befc5d8b'), local_subscribe_addr='ipc:///tmp/f8e30f72-4ce6-4441-8ad5-5832c12f06e1', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f02686ff'), local_subscribe_addr='ipc:///tmp/18c27731-2877-4fcb-b9d0-4527732a3265', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:34 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m WARNING 01-19 00:23:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP2 pid=2225182)[0;0m INFO 01-19 00:23:34 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP3 pid=2225184)[0;0m INFO 01-19 00:23:34 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP1 pid=2225179)[0;0m INFO 01-19 00:23:34 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m INFO 01-19 00:23:34 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP2 pid=2225182)[0;0m INFO 01-19 00:23:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m INFO 01-19 00:23:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP3 pid=2225184)[0;0m INFO 01-19 00:23:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP1 pid=2225179)[0;0m INFO 01-19 00:23:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP2 pid=2225182)[0;0m INFO 01-19 00:23:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m INFO 01-19 00:23:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP3 pid=2225184)[0;0m INFO 01-19 00:23:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP1 pid=2225179)[0;0m INFO 01-19 00:23:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP2 pid=2225182)[0;0m INFO 01-19 00:23:35 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m INFO 01-19 00:23:35 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP3 pid=2225184)[0;0m INFO 01-19 00:23:35 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP1 pid=2225179)[0;0m INFO 01-19 00:23:35 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP2 pid=2225182)[0;0m INFO 01-19 00:23:36 [default_loader.py:267] Loading weights took 0.92 seconds
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP3 pid=2225184)[0;0m INFO 01-19 00:23:37 [default_loader.py:267] Loading weights took 1.02 seconds
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.58it/s]
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP2 pid=2225182)[0;0m INFO 01-19 00:23:37 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.628157 seconds
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP1 pid=2225179)[0;0m INFO 01-19 00:23:37 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.10it/s]
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.00it/s]
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m 
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m INFO 01-19 00:23:37 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP3 pid=2225184)[0;0m INFO 01-19 00:23:37 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.918906 seconds
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP1 pid=2225179)[0;0m INFO 01-19 00:23:38 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.195126 seconds
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m INFO 01-19 00:23:38 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.510579 seconds
[codecarbon INFO @ 00:23:40] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:23:41] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.057380436000003 W
[codecarbon INFO @ 00:23:41] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:23:41] Energy consumed for all GPUs : 0.000404 kWh. Total GPU Power : 90.74734618101755 W
[codecarbon INFO @ 00:23:41] 0.000835 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP2 pid=2225182)[0;0m INFO 01-19 00:23:42 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP3 pid=2225184)[0;0m INFO 01-19 00:23:42 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m INFO 01-19 00:23:42 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP1 pid=2225179)[0;0m INFO 01-19 00:23:42 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP2 pid=2225182)[0;0m WARNING 01-19 00:23:43 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP3 pid=2225184)[0;0m WARNING 01-19 00:23:43 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP0 pid=2225174)[0;0m WARNING 01-19 00:23:43 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m [1;36m(Worker_TP1 pid=2225179)[0;0m WARNING 01-19 00:23:43 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:43 [core.py:210] init engine (profile, create kv cache, warmup model) took 5.12 seconds
[1;36m(EngineCore_DP0 pid=2225168)[0;0m INFO 01-19 00:23:44 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:23:44 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 31024
INFO 01-19 00:23:44 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1374.68it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<01:24,  1.17it/s, est. speed input: 182.73 toks/s, output: 1.17 toks/s]Processed prompts:   2%|â–         | 2/100 [00:01<01:22,  1.18it/s, est. speed input: 126.32 toks/s, output: 1.77 toks/s]Processed prompts:   4%|â–         | 4/100 [00:02<00:50,  1.91it/s, est. speed input: 437.11 toks/s, output: 2.97 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:09,  8.81it/s, est. speed input: 1529.11 toks/s, output: 14.62 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:02<00:04, 19.00it/s, est. speed input: 2881.89 toks/s, output: 40.97 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 26.94it/s, est. speed input: 3337.50 toks/s, output: 72.80 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:02<00:01, 32.17it/s, est. speed input: 3732.38 toks/s, output: 106.89 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:02<00:01, 35.70it/s, est. speed input: 3922.39 toks/s, output: 145.10 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:03<00:01, 29.80it/s, est. speed input: 4154.22 toks/s, output: 185.35 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:03<00:01, 24.59it/s, est. speed input: 3869.77 toks/s, output: 222.11 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:03<00:01, 19.50it/s, est. speed input: 3617.95 toks/s, output: 253.18 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:02, 14.53it/s, est. speed input: 3564.51 toks/s, output: 273.84 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 14.84it/s, est. speed input: 3465.36 toks/s, output: 312.85 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:04<00:02, 12.29it/s, est. speed input: 3430.30 toks/s, output: 342.66 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:05<00:03,  8.92it/s, est. speed input: 3188.22 toks/s, output: 351.25 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02,  9.58it/s, est. speed input: 3183.46 toks/s, output: 384.94 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:05<00:01, 11.81it/s, est. speed input: 3156.83 toks/s, output: 441.61 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:05<00:01, 12.67it/s, est. speed input: 3282.64 toks/s, output: 477.36 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:06<00:01, 10.35it/s, est. speed input: 3175.70 toks/s, output: 499.20 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 11.98it/s, est. speed input: 3102.59 toks/s, output: 555.61 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:00, 13.19it/s, est. speed input: 3185.58 toks/s, output: 594.21 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  5.83it/s, est. speed input: 2830.91 toks/s, output: 572.53 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:07<00:01,  4.57it/s, est. speed input: 2604.62 toks/s, output: 578.38 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:02,  3.85it/s, est. speed input: 2462.27 toks/s, output: 576.50 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  3.85it/s, est. speed input: 2671.67 toks/s, output: 815.57 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.80it/s, est. speed input: 2671.67 toks/s, output: 815.57 toks/s]
[codecarbon INFO @ 00:23:53] Energy consumed for RAM : 0.000532 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:23:53] Delta energy consumed for CPU with cpu_load : 0.000099 kWh, power : 30.17784998357143 W
[codecarbon INFO @ 00:23:53] Energy consumed for All CPU : 0.000229 kWh
[codecarbon INFO @ 00:23:53] Energy consumed for all GPUs : 0.001214 kWh. Total GPU Power : 236.246811889768 W
[codecarbon INFO @ 00:23:53] 0.001975 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:23:53] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:23:53', project_name='codecarbon', run_id='78288a54-aa77-4979-8be6-8570216e47e3', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=28.416683577932417, emissions=0.0004692035819010437, emissions_rate=1.651155317313009e-05, cpu_power=30.17784998357143, gpu_power=236.246811889768, ram_power=70.0, cpu_energy=0.00022887814106144682, gpu_energy=0.0012139076377941649, ram_energy=0.0005320681996578869, energy_consumed=0.001974853978513499, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 18.81s
  Throughput: 808.33 tokens/sec
  Energy per token: 1.03 J/token
  Requests/sec: 11.69
  Total time: 8.55s
  Emissions: 0.000469 kg CO2
  Total energy consumed: 0.001975 kWh

Results saved to: results/run_20260119_002354/results.json
Run 1 completed successfully
--- Run 2 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:24:01 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:24:08] offline tracker init
[codecarbon WARNING @ 00:24:08] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:24:08] [setup] RAM Tracking...
[codecarbon INFO @ 00:24:08] [setup] CPU Tracking...
[codecarbon WARNING @ 00:24:09] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:24:09] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:24:09] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:24:09] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:24:09] [setup] GPU Tracking...
[codecarbon INFO @ 00:24:09] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:24:09] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:24:09] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:24:09] >>> Tracker's metadata:
[codecarbon INFO @ 00:24:09]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:24:09]   Python version: 3.12.3
[codecarbon INFO @ 00:24:09]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:24:09]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:24:09]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:24:09]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:24:09]   GPU count: 4
[codecarbon INFO @ 00:24:09]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:24:09] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 16
  Batch Size: 1024
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:24:09 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 16, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:24:10 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:24:10 [model.py:1510] Using max model len 32768
INFO 01-19 00:24:12 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 01-19 00:24:12 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:12 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:12 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:12 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_23d012c5'), local_subscribe_addr='ipc:///tmp/aa16686f-b473-4170-81c8-c28d18d48d37', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1371595f'), local_subscribe_addr='ipc:///tmp/5181ff60-84d4-4cb9-b62a-636df43a078d', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bc6887d9'), local_subscribe_addr='ipc:///tmp/9b957814-e0d7-4ced-8b06-99d811da64cb', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_718396e1'), local_subscribe_addr='ipc:///tmp/a2ed47b8-7bb6-4138-ab67-43f5271ac308', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_020d997c'), local_subscribe_addr='ipc:///tmp/a51c2edd-06a7-4876-8eed-18a6cfdbc8c0', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:17 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:17 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:17 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:17 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ac810dda'), local_subscribe_addr='ipc:///tmp/b9d3fb80-4210-4ce4-9f96-eafc91e15eba', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:17 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:18 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:18 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:18 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m WARNING 01-19 00:24:18 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP3 pid=2226398)[0;0m INFO 01-19 00:24:18 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP2 pid=2226395)[0;0m INFO 01-19 00:24:18 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m INFO 01-19 00:24:18 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP1 pid=2226389)[0;0m INFO 01-19 00:24:18 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP1 pid=2226389)[0;0m INFO 01-19 00:24:18 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m INFO 01-19 00:24:18 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP3 pid=2226398)[0;0m INFO 01-19 00:24:18 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP2 pid=2226395)[0;0m INFO 01-19 00:24:18 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP1 pid=2226389)[0;0m INFO 01-19 00:24:18 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP3 pid=2226398)[0;0m INFO 01-19 00:24:18 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m INFO 01-19 00:24:18 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP2 pid=2226395)[0;0m INFO 01-19 00:24:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP1 pid=2226389)[0;0m INFO 01-19 00:24:19 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m INFO 01-19 00:24:19 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP3 pid=2226398)[0;0m INFO 01-19 00:24:19 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP2 pid=2226395)[0;0m INFO 01-19 00:24:19 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.50it/s]
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP1 pid=2226389)[0;0m INFO 01-19 00:24:20 [default_loader.py:267] Loading weights took 1.14 seconds
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.05it/s]
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.94it/s]
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m 
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m INFO 01-19 00:24:20 [default_loader.py:267] Loading weights took 1.06 seconds
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP3 pid=2226398)[0;0m INFO 01-19 00:24:21 [default_loader.py:267] Loading weights took 1.05 seconds
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP1 pid=2226389)[0;0m INFO 01-19 00:24:21 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.874206 seconds
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP2 pid=2226395)[0;0m INFO 01-19 00:24:21 [default_loader.py:267] Loading weights took 1.08 seconds
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m INFO 01-19 00:24:21 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.065064 seconds
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP3 pid=2226398)[0;0m INFO 01-19 00:24:21 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.357151 seconds
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP2 pid=2226395)[0;0m INFO 01-19 00:24:22 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.682685 seconds
[codecarbon INFO @ 00:24:24] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:24:25] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.056615922000002 W
[codecarbon INFO @ 00:24:25] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:24:25] Energy consumed for all GPUs : 0.000432 kWh. Total GPU Power : 97.14329889434939 W
[codecarbon INFO @ 00:24:25] 0.000864 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m INFO 01-19 00:24:26 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP1 pid=2226389)[0;0m INFO 01-19 00:24:26 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP3 pid=2226398)[0;0m INFO 01-19 00:24:26 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP2 pid=2226395)[0;0m INFO 01-19 00:24:26 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:26 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:26 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:26 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:26 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:26 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:26 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:26 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:26 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP0 pid=2226387)[0;0m WARNING 01-19 00:24:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP2 pid=2226395)[0;0m WARNING 01-19 00:24:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP3 pid=2226398)[0;0m WARNING 01-19 00:24:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m [1;36m(Worker_TP1 pid=2226389)[0;0m WARNING 01-19 00:24:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:27 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.79 seconds
[1;36m(EngineCore_DP0 pid=2226380)[0;0m INFO 01-19 00:24:27 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:24:27 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 31024
INFO 01-19 00:24:27 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1374.49it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<01:24,  1.17it/s, est. speed input: 182.81 toks/s, output: 1.17 toks/s]Processed prompts:   2%|â–         | 2/100 [00:01<01:23,  1.18it/s, est. speed input: 126.14 toks/s, output: 1.77 toks/s]Processed prompts:   4%|â–         | 4/100 [00:02<00:50,  1.92it/s, est. speed input: 437.65 toks/s, output: 2.98 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:09,  8.83it/s, est. speed input: 1530.96 toks/s, output: 14.64 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:02<00:03, 19.03it/s, est. speed input: 2885.04 toks/s, output: 41.01 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 26.96it/s, est. speed input: 3340.41 toks/s, output: 72.87 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:02<00:01, 32.15it/s, est. speed input: 3734.33 toks/s, output: 106.95 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:02<00:01, 35.61it/s, est. speed input: 3922.61 toks/s, output: 145.11 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:03<00:01, 29.71it/s, est. speed input: 4152.97 toks/s, output: 185.29 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:03<00:01, 24.50it/s, est. speed input: 3867.17 toks/s, output: 221.96 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:03<00:01, 19.41it/s, est. speed input: 3613.39 toks/s, output: 252.86 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:02, 14.46it/s, est. speed input: 3558.88 toks/s, output: 273.40 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 14.80it/s, est. speed input: 3460.64 toks/s, output: 312.42 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:04<00:02, 12.25it/s, est. speed input: 3424.93 toks/s, output: 342.13 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:05<00:03,  8.92it/s, est. speed input: 3184.65 toks/s, output: 350.86 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02,  9.58it/s, est. speed input: 3180.24 toks/s, output: 384.55 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:05<00:01, 11.80it/s, est. speed input: 3153.40 toks/s, output: 441.13 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:05<00:01, 12.66it/s, est. speed input: 3279.29 toks/s, output: 476.87 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:06<00:01, 10.36it/s, est. speed input: 3173.22 toks/s, output: 498.81 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 11.95it/s, est. speed input: 3098.97 toks/s, output: 554.97 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:00, 13.15it/s, est. speed input: 3181.56 toks/s, output: 593.46 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  5.83it/s, est. speed input: 2828.25 toks/s, output: 571.99 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:07<00:01,  4.55it/s, est. speed input: 2600.78 toks/s, output: 577.53 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:02,  3.85it/s, est. speed input: 2459.53 toks/s, output: 575.86 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  3.85it/s, est. speed input: 2667.86 toks/s, output: 814.41 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.78it/s, est. speed input: 2667.86 toks/s, output: 814.41 toks/s]
[codecarbon INFO @ 00:24:36] Energy consumed for RAM : 0.000518 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:24:37] Delta energy consumed for CPU with cpu_load : 0.000093 kWh, power : 30.172775263846155 W
[codecarbon INFO @ 00:24:37] Energy consumed for All CPU : 0.000223 kWh
[codecarbon INFO @ 00:24:37] Energy consumed for all GPUs : 0.001202 kWh. Total GPU Power : 238.77859233842236 W
[codecarbon INFO @ 00:24:37] 0.001942 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:24:37', project_name='codecarbon', run_id='cac713a0-1a7a-4cb7-a01b-2ef2e49e1da4', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=27.688854658044875, emissions=0.0004614969029351369, emissions_rate=1.6667244226407576e-05, cpu_power=30.172775263846155, gpu_power=238.77859233842236, ram_power=70.0, cpu_energy=0.0002226671531427099, gpu_energy=0.001202043739409575, ram_energy=0.0005177060667140823, energy_consumed=0.0019424169592663673, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.79s
  Throughput: 782.30 tokens/sec
  Energy per token: 1.01 J/token
  Requests/sec: 11.31
  Total time: 8.84s
  Emissions: 0.000461 kg CO2
  Total energy consumed: 0.001942 kWh

Results saved to: results/run_20260119_002438/results.json
Run 2 completed successfully
--- Run 3 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:24:44 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:24:50] offline tracker init
[codecarbon WARNING @ 00:24:50] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:24:50] [setup] RAM Tracking...
[codecarbon INFO @ 00:24:50] [setup] CPU Tracking...
[codecarbon WARNING @ 00:24:51] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:24:51] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:24:51] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:24:51] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:24:51] [setup] GPU Tracking...
[codecarbon INFO @ 00:24:51] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:24:51] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:24:51] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:24:51] >>> Tracker's metadata:
[codecarbon INFO @ 00:24:51]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:24:51]   Python version: 3.12.3
[codecarbon INFO @ 00:24:51]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:24:51]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:24:51]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:24:51]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:24:51]   GPU count: 4
[codecarbon INFO @ 00:24:51]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:24:51] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 16
  Batch Size: 1024
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:24:52 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 16, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:24:53 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:24:53 [model.py:1510] Using max model len 32768
INFO 01-19 00:24:54 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 01-19 00:24:54 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:55 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:55 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:55 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_6240d5c2'), local_subscribe_addr='ipc:///tmp/1e19c01b-1841-4a5d-85f6-816ff799f43a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9752c77b'), local_subscribe_addr='ipc:///tmp/4af6a71c-9ef2-4c3d-af76-9055d2383975', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9a93ff21'), local_subscribe_addr='ipc:///tmp/9dfb5a83-8a10-4f01-91e5-7451166572fd', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d7793f21'), local_subscribe_addr='ipc:///tmp/eb2b2a1d-050f-4127-9051-5537ff504254', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c023b68f'), local_subscribe_addr='ipc:///tmp/1b86bb34-39e2-4602-a6de-0dde4f997e35', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:59 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:59 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:59 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:59 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_71414cb1'), local_subscribe_addr='ipc:///tmp/caadbcd1-01ee-41e8-85d6-08b1ca0a488c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:24:59 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:59 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:59 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:59 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m WARNING 01-19 00:24:59 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m INFO 01-19 00:25:00 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP3 pid=2227566)[0;0m INFO 01-19 00:25:00 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP2 pid=2227563)[0;0m INFO 01-19 00:25:00 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP1 pid=2227561)[0;0m INFO 01-19 00:25:00 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m INFO 01-19 00:25:00 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP3 pid=2227566)[0;0m INFO 01-19 00:25:00 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m INFO 01-19 00:25:00 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP2 pid=2227563)[0;0m INFO 01-19 00:25:00 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP1 pid=2227561)[0;0m INFO 01-19 00:25:00 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP3 pid=2227566)[0;0m INFO 01-19 00:25:00 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP2 pid=2227563)[0;0m INFO 01-19 00:25:00 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP1 pid=2227561)[0;0m INFO 01-19 00:25:00 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m INFO 01-19 00:25:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP3 pid=2227566)[0;0m INFO 01-19 00:25:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP2 pid=2227563)[0;0m INFO 01-19 00:25:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP1 pid=2227561)[0;0m INFO 01-19 00:25:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.74it/s]
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.35it/s]
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.23it/s]
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m 
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m INFO 01-19 00:25:01 [default_loader.py:267] Loading weights took 0.92 seconds
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP1 pid=2227561)[0;0m INFO 01-19 00:25:02 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m INFO 01-19 00:25:02 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.633651 seconds
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP2 pid=2227563)[0;0m INFO 01-19 00:25:02 [default_loader.py:267] Loading weights took 1.01 seconds
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP3 pid=2227566)[0;0m INFO 01-19 00:25:02 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP1 pid=2227561)[0;0m INFO 01-19 00:25:02 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.926879 seconds
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP2 pid=2227563)[0;0m INFO 01-19 00:25:03 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.192446 seconds
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP3 pid=2227566)[0;0m INFO 01-19 00:25:03 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.422170 seconds
[codecarbon INFO @ 00:25:07] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP3 pid=2227566)[0;0m INFO 01-19 00:25:07 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP1 pid=2227561)[0;0m INFO 01-19 00:25:07 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP2 pid=2227563)[0;0m INFO 01-19 00:25:07 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m INFO 01-19 00:25:07 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[codecarbon INFO @ 00:25:07] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.086767236000004 W
[codecarbon INFO @ 00:25:07] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:25:07] Energy consumed for all GPUs : 0.000438 kWh. Total GPU Power : 98.3104266242501 W
[codecarbon INFO @ 00:25:07] 0.000869 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP3 pid=2227566)[0;0m WARNING 01-19 00:25:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP0 pid=2227559)[0;0m WARNING 01-19 00:25:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP2 pid=2227563)[0;0m WARNING 01-19 00:25:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m [1;36m(Worker_TP1 pid=2227561)[0;0m WARNING 01-19 00:25:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.92 seconds
[1;36m(EngineCore_DP0 pid=2227549)[0;0m INFO 01-19 00:25:08 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:25:08 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 31024
INFO 01-19 00:25:08 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1742.40it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<01:24,  1.17it/s, est. speed input: 67.88 toks/s, output: 2.34 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:01<00:51,  1.88it/s, est. speed input: 287.73 toks/s, output: 2.36 toks/s]Processed prompts:   4%|â–         | 4/100 [00:02<00:55,  1.74it/s, est. speed input: 432.93 toks/s, output: 2.98 toks/s]Processed prompts:  10%|â–ˆ         | 10/100 [00:02<00:14,  6.30it/s, est. speed input: 1261.46 toks/s, output: 10.57 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:02<00:03, 19.75it/s, est. speed input: 2860.84 toks/s, output: 43.33 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:02<00:02, 28.80it/s, est. speed input: 3753.49 toks/s, output: 79.54 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:02<00:01, 32.36it/s, est. speed input: 3743.15 toks/s, output: 112.19 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:03<00:02, 22.65it/s, est. speed input: 3646.25 toks/s, output: 147.12 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:03<00:01, 24.64it/s, est. speed input: 3672.74 toks/s, output: 195.41 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:03<00:01, 23.89it/s, est. speed input: 3643.99 toks/s, output: 243.86 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:03<00:01, 24.85it/s, est. speed input: 3772.73 toks/s, output: 288.48 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:04<00:03, 11.06it/s, est. speed input: 3263.18 toks/s, output: 288.73 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:05<00:03, 10.31it/s, est. speed input: 3106.50 toks/s, output: 325.65 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 11.30it/s, est. speed input: 3022.54 toks/s, output: 374.17 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:05<00:02, 11.93it/s, est. speed input: 3119.80 toks/s, output: 407.08 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:05<00:01, 12.62it/s, est. speed input: 3217.65 toks/s, output: 440.44 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:06<00:02,  9.48it/s, est. speed input: 3040.32 toks/s, output: 467.94 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:02,  9.49it/s, est. speed input: 2983.50 toks/s, output: 498.63 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:06<00:01, 10.95it/s, est. speed input: 2943.81 toks/s, output: 555.64 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:07<00:01,  7.24it/s, est. speed input: 2753.35 toks/s, output: 559.43 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  8.58it/s, est. speed input: 2862.78 toks/s, output: 603.70 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.98it/s, est. speed input: 2460.49 toks/s, output: 567.49 toks/s]Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:08<00:00,  7.89it/s, est. speed input: 2590.00 toks/s, output: 739.30 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  7.89it/s, est. speed input: 2627.95 toks/s, output: 856.53 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.60it/s, est. speed input: 2627.95 toks/s, output: 856.53 toks/s]
[codecarbon INFO @ 00:25:17] Energy consumed for RAM : 0.000499 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:25:18] Delta energy consumed for CPU with cpu_load : 0.000085 kWh, power : 30.237934237500003 W
[codecarbon INFO @ 00:25:18] Energy consumed for All CPU : 0.000215 kWh
[codecarbon INFO @ 00:25:18] Energy consumed for all GPUs : 0.001182 kWh. Total GPU Power : 251.56023021170853 W
[codecarbon INFO @ 00:25:18] 0.001896 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:25:18] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:25:18', project_name='codecarbon', run_id='9d8eaad6-be4f-47f7-abdc-e5cc1d175f47', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=26.715413867961615, emissions=0.00045039105297561724, emissions_rate=1.6858846177777072e-05, cpu_power=30.237934237500003, gpu_power=251.56023021170853, ram_power=70.0, cpu_energy=0.00021496208639281398, gpu_energy=0.0011816084452878073, ram_energy=0.0004991024707547493, energy_consumed=0.0018956730024353705, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 16.73s
  Throughput: 825.96 tokens/sec
  Energy per token: 0.92 J/token
  Requests/sec: 11.19
  Total time: 8.94s
  Emissions: 0.000450 kg CO2
  Total energy consumed: 0.001896 kWh

Results saved to: results/run_20260119_002519/results.json
Run 3 completed successfully
--- Run 4 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:25:26 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:25:31] offline tracker init
[codecarbon WARNING @ 00:25:31] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:25:31] [setup] RAM Tracking...
[codecarbon INFO @ 00:25:31] [setup] CPU Tracking...
[codecarbon WARNING @ 00:25:32] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:25:32] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:25:32] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:25:32] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:25:32] [setup] GPU Tracking...
[codecarbon INFO @ 00:25:32] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:25:32] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:25:32] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:25:32] >>> Tracker's metadata:
[codecarbon INFO @ 00:25:32]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:25:32]   Python version: 3.12.3
[codecarbon INFO @ 00:25:32]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:25:32]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:25:32]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:25:32]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:25:32]   GPU count: 4
[codecarbon INFO @ 00:25:32]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:25:32] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 16
  Batch Size: 1024
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:25:33 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 16, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:25:34 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:25:34 [model.py:1510] Using max model len 32768
INFO 01-19 00:25:35 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 01-19 00:25:35 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:36 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:36 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:36 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d6cd9c9a'), local_subscribe_addr='ipc:///tmp/3aaed0b8-9bc2-4b0e-877d-06aa2bb47a1a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8dd3b6be'), local_subscribe_addr='ipc:///tmp/e7e01d31-201d-4cbb-be0a-5321ea2780a2', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_10228e6e'), local_subscribe_addr='ipc:///tmp/fe4c4030-fc3e-40b5-a4a2-3baf59a63911', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_872a1dd9'), local_subscribe_addr='ipc:///tmp/49fb19f6-b0ef-43cb-bc04-54b2ccda06dd', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c27e5adf'), local_subscribe_addr='ipc:///tmp/519bdf1c-539a-4696-b367-0b5c5952f50f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:40 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:40 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:40 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:40 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_7b41c8b4'), local_subscribe_addr='ipc:///tmp/17e1d642-7838-4c10-a355-2fb540aec611', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:40 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m WARNING 01-19 00:25:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP3 pid=2230225)[0;0m INFO 01-19 00:25:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP1 pid=2230221)[0;0m INFO 01-19 00:25:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP2 pid=2230223)[0;0m INFO 01-19 00:25:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m INFO 01-19 00:25:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP2 pid=2230223)[0;0m INFO 01-19 00:25:41 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP1 pid=2230221)[0;0m INFO 01-19 00:25:41 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP3 pid=2230225)[0;0m INFO 01-19 00:25:41 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m INFO 01-19 00:25:41 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP2 pid=2230223)[0;0m INFO 01-19 00:25:41 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP1 pid=2230221)[0;0m INFO 01-19 00:25:41 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP3 pid=2230225)[0;0m INFO 01-19 00:25:41 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m INFO 01-19 00:25:41 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP1 pid=2230221)[0;0m INFO 01-19 00:25:42 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP2 pid=2230223)[0;0m INFO 01-19 00:25:42 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP3 pid=2230225)[0;0m INFO 01-19 00:25:42 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m INFO 01-19 00:25:42 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.56it/s]
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP1 pid=2230221)[0;0m INFO 01-19 00:25:43 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.09it/s]
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.99it/s]
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m 
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m INFO 01-19 00:25:43 [default_loader.py:267] Loading weights took 1.04 seconds
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP2 pid=2230223)[0;0m INFO 01-19 00:25:43 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP1 pid=2230221)[0;0m INFO 01-19 00:25:43 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.603256 seconds
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP3 pid=2230225)[0;0m INFO 01-19 00:25:44 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m INFO 01-19 00:25:44 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.919565 seconds
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP2 pid=2230223)[0;0m INFO 01-19 00:25:44 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.171968 seconds
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP3 pid=2230225)[0;0m INFO 01-19 00:25:44 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.460093 seconds
[codecarbon INFO @ 00:25:48] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP3 pid=2230225)[0;0m INFO 01-19 00:25:48 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP2 pid=2230223)[0;0m INFO 01-19 00:25:48 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m INFO 01-19 00:25:48 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP1 pid=2230221)[0;0m INFO 01-19 00:25:48 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[codecarbon INFO @ 00:25:48] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.063040374000003 W
[codecarbon INFO @ 00:25:48] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:25:48] Energy consumed for all GPUs : 0.000439 kWh. Total GPU Power : 98.60275667678144 W
[codecarbon INFO @ 00:25:48] 0.000871 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:48 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:48 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:48 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:48 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:48 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:48 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:48 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:48 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP3 pid=2230225)[0;0m WARNING 01-19 00:25:48 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP1 pid=2230221)[0;0m WARNING 01-19 00:25:48 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP2 pid=2230223)[0;0m WARNING 01-19 00:25:48 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m [1;36m(Worker_TP0 pid=2230219)[0;0m WARNING 01-19 00:25:48 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:49 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.43 seconds
[1;36m(EngineCore_DP0 pid=2230213)[0;0m INFO 01-19 00:25:49 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:25:49 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 31024
INFO 01-19 00:25:50 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1211.57it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<01:23,  1.18it/s, est. speed input: 184.69 toks/s, output: 1.18 toks/s]Processed prompts:   2%|â–         | 2/100 [00:01<01:22,  1.19it/s, est. speed input: 126.91 toks/s, output: 1.78 toks/s]Processed prompts:   4%|â–         | 4/100 [00:02<00:49,  1.93it/s, est. speed input: 439.80 toks/s, output: 2.99 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:09,  8.86it/s, est. speed input: 1537.92 toks/s, output: 14.71 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:02<00:03, 19.09it/s, est. speed input: 2898.01 toks/s, output: 41.20 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 27.06it/s, est. speed input: 3355.82 toks/s, output: 73.20 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:02<00:01, 32.30it/s, est. speed input: 3752.13 toks/s, output: 107.46 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:02<00:01, 35.83it/s, est. speed input: 3942.61 toks/s, output: 145.85 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:03<00:01, 29.84it/s, est. speed input: 4173.07 toks/s, output: 186.19 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:03<00:01, 24.63it/s, est. speed input: 3886.58 toks/s, output: 223.07 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:03<00:01, 19.51it/s, est. speed input: 3631.47 toks/s, output: 254.12 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:02, 14.54it/s, est. speed input: 3577.12 toks/s, output: 274.81 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 14.87it/s, est. speed input: 3477.84 toks/s, output: 313.97 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:04<00:02, 12.28it/s, est. speed input: 3440.10 toks/s, output: 343.64 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:05<00:03,  8.94it/s, est. speed input: 3198.23 toks/s, output: 352.35 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02,  9.61it/s, est. speed input: 3194.01 toks/s, output: 386.21 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:05<00:01, 11.83it/s, est. speed input: 3166.82 toks/s, output: 443.01 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:05<00:01, 12.68it/s, est. speed input: 3292.45 toks/s, output: 478.78 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:06<00:01, 10.39it/s, est. speed input: 3186.11 toks/s, output: 500.84 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 11.98it/s, est. speed input: 3111.34 toks/s, output: 557.19 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:00, 13.17it/s, est. speed input: 3194.10 toks/s, output: 595.80 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  5.83it/s, est. speed input: 2837.48 toks/s, output: 573.86 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:07<00:01,  4.56it/s, est. speed input: 2609.45 toks/s, output: 579.46 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:02,  3.85it/s, est. speed input: 2467.38 toks/s, output: 577.69 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  3.85it/s, est. speed input: 2677.17 toks/s, output: 817.25 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.82it/s, est. speed input: 2677.17 toks/s, output: 817.25 toks/s]
[codecarbon INFO @ 00:25:58] Energy consumed for RAM : 0.000493 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:25:59] Delta energy consumed for CPU with cpu_load : 0.000083 kWh, power : 30.170567752500002 W
[codecarbon INFO @ 00:25:59] Energy consumed for All CPU : 0.000212 kWh
[codecarbon INFO @ 00:25:59] Energy consumed for all GPUs : 0.001163 kWh. Total GPU Power : 252.03021777736186 W
[codecarbon INFO @ 00:25:59] 0.001869 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:25:59', project_name='codecarbon', run_id='e4f93f3a-8785-48b5-9c6d-239560e7c72c', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=26.418949004262686, emissions=0.0004440493903265193, emissions_rate=1.6807988472776572e-05, cpu_power=30.170567752500002, gpu_power=252.03021777736186, ram_power=70.0, cpu_energy=0.00021219718116708357, gpu_energy=0.0011634206529613778, ram_energy=0.0004933634332135424, energy_consumed=0.0018689812673420036, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 16.82s
  Throughput: 809.14 tokens/sec
  Energy per token: 0.97 J/token
  Requests/sec: 11.70
  Total time: 8.54s
  Emissions: 0.000444 kg CO2
  Total energy consumed: 0.001869 kWh

Results saved to: results/run_20260119_002559/results.json
Run 4 completed successfully
--- Run 5 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:26:04 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:26:09] offline tracker init
[codecarbon WARNING @ 00:26:09] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:26:09] [setup] RAM Tracking...
[codecarbon INFO @ 00:26:09] [setup] CPU Tracking...
[codecarbon WARNING @ 00:26:11] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:26:11] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:26:11] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:26:11] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:26:11] [setup] GPU Tracking...
[codecarbon INFO @ 00:26:11] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:26:11] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:26:11] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:26:11] >>> Tracker's metadata:
[codecarbon INFO @ 00:26:11]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:26:11]   Python version: 3.12.3
[codecarbon INFO @ 00:26:11]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:26:11]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:26:11]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:26:11]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:26:11]   GPU count: 4
[codecarbon INFO @ 00:26:11]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:26:11] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 16
  Batch Size: 1024
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: False
======================================================================

Initializing vLLM...
INFO 01-19 00:26:11 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 16, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8192, 'max_num_seqs': 1024, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:26:12 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:26:12 [model.py:1510] Using max model len 32768
INFO 01-19 00:26:14 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 01-19 00:26:14 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:14 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:14 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:14 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_fd09f1f6'), local_subscribe_addr='ipc:///tmp/46fce20f-9e60-44d0-84ac-caae9d790f1f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9fd3dc97'), local_subscribe_addr='ipc:///tmp/969fd68c-6275-4471-912f-4993f8817102', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b85b24d6'), local_subscribe_addr='ipc:///tmp/a47ef7c4-1f45-414d-96bb-74531ffab45b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_33c4db65'), local_subscribe_addr='ipc:///tmp/3dde7cec-a515-4ffc-bf71-12c6cf5a104a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_da359916'), local_subscribe_addr='ipc:///tmp/8bbf0615-8c4b-416b-9c00-20f72b6a1e20', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:19 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:19 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:19 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:19 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_92cd76ec'), local_subscribe_addr='ipc:///tmp/001478b4-7240-4b01-a049-ebc461a308ba', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:19 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m WARNING 01-19 00:26:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP2 pid=2231343)[0;0m INFO 01-19 00:26:20 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP3 pid=2231345)[0;0m INFO 01-19 00:26:20 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m INFO 01-19 00:26:20 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP1 pid=2231341)[0;0m INFO 01-19 00:26:20 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP3 pid=2231345)[0;0m INFO 01-19 00:26:20 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP2 pid=2231343)[0;0m INFO 01-19 00:26:20 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m INFO 01-19 00:26:20 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP1 pid=2231341)[0;0m INFO 01-19 00:26:20 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP3 pid=2231345)[0;0m INFO 01-19 00:26:20 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP2 pid=2231343)[0;0m INFO 01-19 00:26:20 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m INFO 01-19 00:26:20 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP1 pid=2231341)[0;0m INFO 01-19 00:26:20 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP2 pid=2231343)[0;0m INFO 01-19 00:26:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP3 pid=2231345)[0;0m INFO 01-19 00:26:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP1 pid=2231341)[0;0m INFO 01-19 00:26:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m INFO 01-19 00:26:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.59it/s]
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP2 pid=2231343)[0;0m INFO 01-19 00:26:22 [default_loader.py:267] Loading weights took 1.02 seconds
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.11it/s]
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m 
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m INFO 01-19 00:26:22 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP1 pid=2231341)[0;0m INFO 01-19 00:26:22 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP2 pid=2231343)[0;0m INFO 01-19 00:26:22 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.756192 seconds
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP3 pid=2231345)[0;0m INFO 01-19 00:26:22 [default_loader.py:267] Loading weights took 1.04 seconds
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m INFO 01-19 00:26:23 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.011544 seconds
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP1 pid=2231341)[0;0m INFO 01-19 00:26:23 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.277539 seconds
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP3 pid=2231345)[0;0m INFO 01-19 00:26:23 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.657673 seconds
[codecarbon INFO @ 00:26:26] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:26:27] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.060492726000003 W
[codecarbon INFO @ 00:26:27] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:26:27] Energy consumed for all GPUs : 0.000438 kWh. Total GPU Power : 98.41557935940345 W
[codecarbon INFO @ 00:26:27] 0.000870 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP3 pid=2231345)[0;0m INFO 01-19 00:26:27 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP2 pid=2231343)[0;0m INFO 01-19 00:26:27 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP1 pid=2231341)[0;0m INFO 01-19 00:26:27 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m INFO 01-19 00:26:27 [gpu_worker.py:298] Available KV cache memory: 15.15 GiB
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [kv_cache_utils.py:1087] GPU KV cache size: 496,384 tokens
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 40.34x
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP3 pid=2231345)[0;0m WARNING 01-19 00:26:28 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP0 pid=2231339)[0;0m WARNING 01-19 00:26:28 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP2 pid=2231343)[0;0m WARNING 01-19 00:26:28 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m [1;36m(Worker_TP1 pid=2231341)[0;0m WARNING 01-19 00:26:28 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.60 seconds
[1;36m(EngineCore_DP0 pid=2231329)[0;0m INFO 01-19 00:26:28 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:26:29 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 31024
INFO 01-19 00:26:29 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1244.19it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<01:24,  1.17it/s, est. speed input: 182.03 toks/s, output: 1.17 toks/s]Processed prompts:   2%|â–         | 2/100 [00:01<01:23,  1.18it/s, est. speed input: 125.91 toks/s, output: 1.77 toks/s]Processed prompts:   4%|â–         | 4/100 [00:02<00:50,  1.91it/s, est. speed input: 436.12 toks/s, output: 2.97 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:09,  8.79it/s, est. speed input: 1525.38 toks/s, output: 14.58 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:02<00:04, 18.96it/s, est. speed input: 2875.01 toks/s, output: 40.87 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 26.89it/s, est. speed input: 3329.81 toks/s, output: 72.64 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:02<00:01, 32.01it/s, est. speed input: 3720.92 toks/s, output: 106.56 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:02<00:01, 35.47it/s, est. speed input: 3908.87 toks/s, output: 144.60 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:03<00:01, 29.55it/s, est. speed input: 4137.07 toks/s, output: 184.58 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:03<00:01, 24.38it/s, est. speed input: 3852.01 toks/s, output: 221.09 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:03<00:01, 19.31it/s, est. speed input: 3599.02 toks/s, output: 251.85 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:02, 14.40it/s, est. speed input: 3544.91 toks/s, output: 272.33 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 14.72it/s, est. speed input: 3446.51 toks/s, output: 311.15 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:04<00:02, 12.15it/s, est. speed input: 3408.43 toks/s, output: 340.48 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:05<00:03,  8.82it/s, est. speed input: 3166.61 toks/s, output: 348.87 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02,  9.47it/s, est. speed input: 3161.33 toks/s, output: 382.26 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:05<00:01, 11.67it/s, est. speed input: 3134.34 toks/s, output: 438.46 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:05<00:01, 12.50it/s, est. speed input: 3258.45 toks/s, output: 473.84 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:06<00:01, 10.25it/s, est. speed input: 3152.68 toks/s, output: 495.58 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 11.82it/s, est. speed input: 3078.74 toks/s, output: 551.35 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01, 13.00it/s, est. speed input: 3160.38 toks/s, output: 589.51 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  5.75it/s, est. speed input: 2806.66 toks/s, output: 567.62 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:08<00:01,  4.52it/s, est. speed input: 2583.26 toks/s, output: 573.64 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:02,  3.83it/s, est. speed input: 2443.95 toks/s, output: 572.21 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  3.83it/s, est. speed input: 2652.03 toks/s, output: 809.58 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.71it/s, est. speed input: 2652.03 toks/s, output: 809.58 toks/s]
[codecarbon INFO @ 00:26:37] Energy consumed for RAM : 0.000508 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:26:38] Delta energy consumed for CPU with cpu_load : 0.000089 kWh, power : 30.171315893076926 W
[codecarbon INFO @ 00:26:38] Energy consumed for All CPU : 0.000219 kWh
[codecarbon INFO @ 00:26:38] Energy consumed for all GPUs : 0.001190 kWh. Total GPU Power : 243.80370755902808 W
[codecarbon INFO @ 00:26:38] 0.001917 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:26:38] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:26:38', project_name='codecarbon', run_id='4f23feaf-9134-4e50-9d55-a71241656f08', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=27.18175094202161, emissions=0.0004554877144650719, emissions_rate=1.675711455956691e-05, cpu_power=30.171315893076926, gpu_power=243.80370755902808, ram_power=70.0, cpu_energy=0.00021853826712696345, gpu_energy=0.001190483730168168, ram_energy=0.0005081025937636798, energy_consumed=0.0019171245910588111, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.51s
  Throughput: 801.82 tokens/sec
  Energy per token: 1.00 J/token
  Requests/sec: 11.60
  Total time: 8.62s
  Emissions: 0.000455 kg CO2
  Total energy consumed: 0.001917 kWh

Results saved to: results/run_20260119_002639/results.json
Run 5 completed successfully
========================================
Command 4: python vllm_evaluation.py --num_prompts 100 --block_size 32 --max_num_seqs 256 --max_num_batched_tokens 12288 --tensor_parallel_size 4 --enable_chunked_prefill --enable_prefix_caching
Required GPUs: 4
========================================
--- Run 1 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:26:45 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:26:51] offline tracker init
[codecarbon WARNING @ 00:26:51] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:26:51] [setup] RAM Tracking...
[codecarbon INFO @ 00:26:51] [setup] CPU Tracking...
[codecarbon WARNING @ 00:26:52] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:26:52] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:26:52] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:26:52] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:26:52] [setup] GPU Tracking...
[codecarbon INFO @ 00:26:52] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:26:52] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:26:52] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:26:52] >>> Tracker's metadata:
[codecarbon INFO @ 00:26:52]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:26:52]   Python version: 3.12.3
[codecarbon INFO @ 00:26:52]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:26:52]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:26:52]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:26:52]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:26:52]   GPU count: 4
[codecarbon INFO @ 00:26:52]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:26:52] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 32
  Batch Size: 256
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:26:52 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:26:53 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:26:53 [model.py:1510] Using max model len 32768
INFO 01-19 00:26:55 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:26:55 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:26:56 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:26:56 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:26:56 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:26:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_66da838d'), local_subscribe_addr='ipc:///tmp/87efd35b-fb41-4cab-9a5e-e302198611f8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:26:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8bc292c5'), local_subscribe_addr='ipc:///tmp/7d84e965-c8b5-43ed-a71e-9f17201c0a48', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:26:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_df0feb4f'), local_subscribe_addr='ipc:///tmp/35973860-9583-4e80-860d-e9ca65955666', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:26:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a59ddc37'), local_subscribe_addr='ipc:///tmp/4baa71ee-3e39-417a-9fab-10ad27e878fa', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:26:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6f361df7'), local_subscribe_addr='ipc:///tmp/c0de7ea7-cc48-4d50-89ea-bb39198cc62e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:27:00 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:27:00 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:27:00 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:27:00 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_47ef1eae'), local_subscribe_addr='ipc:///tmp/c355522e-99f7-48f7-9a7c-1cabc301397a', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:00 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:27:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:27:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:27:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m WARNING 01-19 00:27:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP3 pid=2232516)[0;0m INFO 01-19 00:27:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP2 pid=2232513)[0;0m INFO 01-19 00:27:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m INFO 01-19 00:27:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP1 pid=2232511)[0;0m INFO 01-19 00:27:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP3 pid=2232516)[0;0m INFO 01-19 00:27:01 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP2 pid=2232513)[0;0m INFO 01-19 00:27:01 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP1 pid=2232511)[0;0m INFO 01-19 00:27:01 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m INFO 01-19 00:27:01 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP3 pid=2232516)[0;0m INFO 01-19 00:27:01 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP2 pid=2232513)[0;0m INFO 01-19 00:27:01 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP1 pid=2232511)[0;0m INFO 01-19 00:27:01 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m INFO 01-19 00:27:01 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP3 pid=2232516)[0;0m INFO 01-19 00:27:01 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP1 pid=2232511)[0;0m INFO 01-19 00:27:01 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP2 pid=2232513)[0;0m INFO 01-19 00:27:01 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m INFO 01-19 00:27:01 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP3 pid=2232516)[0;0m INFO 01-19 00:27:03 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.59it/s]
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.13it/s]
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.02it/s]
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m 
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m INFO 01-19 00:27:03 [default_loader.py:267] Loading weights took 1.02 seconds
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP3 pid=2232516)[0;0m INFO 01-19 00:27:03 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.815086 seconds
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP1 pid=2232511)[0;0m INFO 01-19 00:27:03 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP2 pid=2232513)[0;0m INFO 01-19 00:27:04 [default_loader.py:267] Loading weights took 1.04 seconds
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m INFO 01-19 00:27:04 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.169129 seconds
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP1 pid=2232511)[0;0m INFO 01-19 00:27:04 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.486372 seconds
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP2 pid=2232513)[0;0m INFO 01-19 00:27:04 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.751975 seconds
[codecarbon INFO @ 00:27:07] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:27:08] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.056497518000004 W
[codecarbon INFO @ 00:27:08] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:27:08] Energy consumed for all GPUs : 0.000448 kWh. Total GPU Power : 100.57759488565526 W
[codecarbon INFO @ 00:27:08] 0.000879 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP3 pid=2232516)[0;0m INFO 01-19 00:27:09 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP1 pid=2232511)[0;0m INFO 01-19 00:27:09 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP2 pid=2232513)[0;0m INFO 01-19 00:27:09 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m INFO 01-19 00:27:09 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP3 pid=2232516)[0;0m WARNING 01-19 00:27:09 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP2 pid=2232513)[0;0m WARNING 01-19 00:27:09 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP0 pid=2232509)[0;0m WARNING 01-19 00:27:09 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m [1;36m(Worker_TP1 pid=2232511)[0;0m WARNING 01-19 00:27:09 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:09 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.95 seconds
[1;36m(EngineCore_DP0 pid=2232503)[0;0m INFO 01-19 00:27:10 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:27:10 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 16218
INFO 01-19 00:27:10 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1361.04it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:08,  1.30s/it, est. speed input: 120.20 toks/s, output: 0.77 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:54,  1.17s/it, est. speed input: 99.87 toks/s, output: 0.84 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:16,  5.43it/s, est. speed input: 1087.76 toks/s, output: 10.02 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.69it/s, est. speed input: 2047.25 toks/s, output: 28.84 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 20.03it/s, est. speed input: 2818.43 toks/s, output: 52.55 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.78it/s, est. speed input: 3330.34 toks/s, output: 82.19 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 29.51it/s, est. speed input: 3723.39 toks/s, output: 122.20 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.45it/s, est. speed input: 3725.16 toks/s, output: 157.49 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.46it/s, est. speed input: 3593.51 toks/s, output: 199.55 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.85it/s, est. speed input: 3603.82 toks/s, output: 240.10 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 18.18it/s, est. speed input: 3353.60 toks/s, output: 273.39 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.50it/s, est. speed input: 3502.20 toks/s, output: 311.33 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.24it/s, est. speed input: 3464.16 toks/s, output: 346.73 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.34it/s, est. speed input: 3310.62 toks/s, output: 373.69 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:04<00:02, 14.84it/s, est. speed input: 3296.36 toks/s, output: 396.89 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.11it/s, est. speed input: 3233.71 toks/s, output: 403.09 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.64it/s, est. speed input: 3214.41 toks/s, output: 450.05 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.94it/s, est. speed input: 3109.88 toks/s, output: 475.24 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.97it/s, est. speed input: 3073.90 toks/s, output: 508.57 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.83it/s, est. speed input: 2983.93 toks/s, output: 533.24 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.22it/s, est. speed input: 3079.04 toks/s, output: 564.76 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.57it/s, est. speed input: 3013.37 toks/s, output: 592.36 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.00it/s, est. speed input: 2869.24 toks/s, output: 606.95 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.56it/s, est. speed input: 2810.91 toks/s, output: 618.12 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.47it/s, est. speed input: 2724.35 toks/s, output: 623.77 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:02,  3.86it/s, est. speed input: 2509.85 toks/s, output: 602.08 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.23it/s, est. speed input: 2481.82 toks/s, output: 646.71 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.01it/s, est. speed input: 2350.93 toks/s, output: 641.75 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.01it/s, est. speed input: 2635.63 toks/s, output: 848.91 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.64it/s, est. speed input: 2635.63 toks/s, output: 848.91 toks/s]
[codecarbon INFO @ 00:27:19] Energy consumed for RAM : 0.000515 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:27:19] Delta energy consumed for CPU with cpu_load : 0.000092 kWh, power : 30.201648461538465 W
[codecarbon INFO @ 00:27:19] Energy consumed for All CPU : 0.000222 kWh
[codecarbon INFO @ 00:27:19] Energy consumed for all GPUs : 0.001209 kWh. Total GPU Power : 239.25086666137616 W
[codecarbon INFO @ 00:27:19] 0.001945 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:27:19', project_name='codecarbon', run_id='7a36bfb5-dca3-4076-9241-ef8342a3cabd', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=27.529690914787352, emissions=0.0004622266285313252, emissions_rate=1.6790113262152312e-05, cpu_power=30.201648461538465, gpu_power=239.25086666137616, ram_power=70.0, cpu_energy=0.00022152231456454977, gpu_energy=0.0012091201339643476, ram_energy=0.0005148458886050826, energy_consumed=0.00194548833713398, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.81s
  Throughput: 841.45 tokens/sec
  Energy per token: 0.96 J/token
  Requests/sec: 11.53
  Total time: 8.67s
  Emissions: 0.000462 kg CO2
  Total energy consumed: 0.001945 kWh

Results saved to: results/run_20260119_002720/results.json
Run 1 completed successfully
--- Run 2 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:27:27 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:27:32] offline tracker init
[codecarbon WARNING @ 00:27:32] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:27:32] [setup] RAM Tracking...
[codecarbon INFO @ 00:27:32] [setup] CPU Tracking...
[codecarbon WARNING @ 00:27:33] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:27:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:27:33] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:27:33] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:27:33] [setup] GPU Tracking...
[codecarbon INFO @ 00:27:33] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:27:33] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:27:33] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:27:33] >>> Tracker's metadata:
[codecarbon INFO @ 00:27:33]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:27:33]   Python version: 3.12.3
[codecarbon INFO @ 00:27:33]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:27:33]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:27:33]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:27:33]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:27:33]   GPU count: 4
[codecarbon INFO @ 00:27:33]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:27:33] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 32
  Batch Size: 256
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:27:34 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:27:35 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:27:35 [model.py:1510] Using max model len 32768
INFO 01-19 00:27:36 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:27:36 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:37 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:37 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:37 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d55eab9c'), local_subscribe_addr='ipc:///tmp/34c69b12-e912-42d6-8f83-9b58ec92e25c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5410aa7f'), local_subscribe_addr='ipc:///tmp/cc89d5fb-a7e9-4bc8-8e50-8b2a64a44e98', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_06518001'), local_subscribe_addr='ipc:///tmp/a01481b8-7f8e-401c-a021-d19a6e45f01c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_037b47fa'), local_subscribe_addr='ipc:///tmp/40f3c214-4b08-4cb3-88e7-3e7675e42857', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:40 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b3b053ae'), local_subscribe_addr='ipc:///tmp/82f9077c-ed8c-4004-a12e-7e70fc3ee7b8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:41 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:41 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:41 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:41 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:41 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:41 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:41 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:41 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:42 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:42 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:42 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:42 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_96446619'), local_subscribe_addr='ipc:///tmp/a5581623-7e1f-4cf3-a041-b9e436a29d2c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:42 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m WARNING 01-19 00:27:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP3 pid=2233699)[0;0m INFO 01-19 00:27:42 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP1 pid=2233690)[0;0m INFO 01-19 00:27:42 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP2 pid=2233692)[0;0m INFO 01-19 00:27:42 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m INFO 01-19 00:27:42 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP2 pid=2233692)[0;0m INFO 01-19 00:27:42 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP1 pid=2233690)[0;0m INFO 01-19 00:27:42 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP3 pid=2233699)[0;0m INFO 01-19 00:27:42 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m INFO 01-19 00:27:43 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP3 pid=2233699)[0;0m INFO 01-19 00:27:43 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP2 pid=2233692)[0;0m INFO 01-19 00:27:43 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP1 pid=2233690)[0;0m INFO 01-19 00:27:43 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m INFO 01-19 00:27:43 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP2 pid=2233692)[0;0m INFO 01-19 00:27:43 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP3 pid=2233699)[0;0m INFO 01-19 00:27:43 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP1 pid=2233690)[0;0m INFO 01-19 00:27:43 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m INFO 01-19 00:27:43 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP2 pid=2233692)[0;0m INFO 01-19 00:27:44 [default_loader.py:267] Loading weights took 0.92 seconds
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP1 pid=2233690)[0;0m INFO 01-19 00:27:45 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP3 pid=2233699)[0;0m INFO 01-19 00:27:45 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.55it/s]
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP2 pid=2233692)[0;0m INFO 01-19 00:27:45 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.669038 seconds
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.07it/s]
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m 
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m INFO 01-19 00:27:45 [default_loader.py:267] Loading weights took 1.05 seconds
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP1 pid=2233690)[0;0m INFO 01-19 00:27:45 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.047612 seconds
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP3 pid=2233699)[0;0m INFO 01-19 00:27:45 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.205382 seconds
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m INFO 01-19 00:27:46 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.633718 seconds
[codecarbon INFO @ 00:27:49] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:27:49] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.062863074000003 W
[codecarbon INFO @ 00:27:49] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:27:49] Energy consumed for all GPUs : 0.000451 kWh. Total GPU Power : 101.40587869194201 W
[codecarbon INFO @ 00:27:49] 0.000883 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP3 pid=2233699)[0;0m INFO 01-19 00:27:50 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m INFO 01-19 00:27:50 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP2 pid=2233692)[0;0m INFO 01-19 00:27:50 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP1 pid=2233690)[0;0m INFO 01-19 00:27:50 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:50 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:50 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:50 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:50 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:50 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:50 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:50 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:50 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP1 pid=2233690)[0;0m WARNING 01-19 00:27:50 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP2 pid=2233692)[0;0m WARNING 01-19 00:27:50 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP0 pid=2233688)[0;0m WARNING 01-19 00:27:50 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m [1;36m(Worker_TP3 pid=2233699)[0;0m WARNING 01-19 00:27:50 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:51 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.79 seconds
[1;36m(EngineCore_DP0 pid=2233682)[0;0m INFO 01-19 00:27:51 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:27:51 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 16218
INFO 01-19 00:27:52 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1368.49it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:08,  1.29s/it, est. speed input: 120.56 toks/s, output: 0.77 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:54,  1.17s/it, est. speed input: 100.23 toks/s, output: 0.84 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:02<00:16,  5.44it/s, est. speed input: 1090.71 toks/s, output: 10.05 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.73it/s, est. speed input: 2053.56 toks/s, output: 28.93 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 20.12it/s, est. speed input: 2828.47 toks/s, output: 52.74 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.87it/s, est. speed input: 3341.82 toks/s, output: 82.47 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 29.59it/s, est. speed input: 3735.17 toks/s, output: 122.59 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.41it/s, est. speed input: 3732.51 toks/s, output: 157.80 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.36it/s, est. speed input: 3597.05 toks/s, output: 199.74 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.73it/s, est. speed input: 3605.60 toks/s, output: 240.22 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 17.98it/s, est. speed input: 3349.81 toks/s, output: 273.08 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.32it/s, est. speed input: 3498.60 toks/s, output: 311.01 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.10it/s, est. speed input: 3460.39 toks/s, output: 346.35 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.20it/s, est. speed input: 3304.71 toks/s, output: 373.02 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:04<00:02, 14.72it/s, est. speed input: 3290.24 toks/s, output: 396.16 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.01it/s, est. speed input: 3224.82 toks/s, output: 401.99 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.53it/s, est. speed input: 3205.28 toks/s, output: 448.78 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.84it/s, est. speed input: 3100.40 toks/s, output: 473.79 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.87it/s, est. speed input: 3064.56 toks/s, output: 507.03 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.78it/s, est. speed input: 2975.30 toks/s, output: 531.70 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.17it/s, est. speed input: 3070.37 toks/s, output: 563.17 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.55it/s, est. speed input: 3005.32 toks/s, output: 590.78 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  7.99it/s, est. speed input: 2862.15 toks/s, output: 605.44 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.56it/s, est. speed input: 2804.29 toks/s, output: 616.66 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.46it/s, est. speed input: 2717.81 toks/s, output: 622.27 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:02,  3.85it/s, est. speed input: 2503.73 toks/s, output: 600.62 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.22it/s, est. speed input: 2475.62 toks/s, output: 645.09 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.01it/s, est. speed input: 2345.48 toks/s, output: 640.26 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.01it/s, est. speed input: 2629.44 toks/s, output: 846.91 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.61it/s, est. speed input: 2629.44 toks/s, output: 846.91 toks/s]
[codecarbon INFO @ 00:28:00] Energy consumed for RAM : 0.000512 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:28:01] Delta energy consumed for CPU with cpu_load : 0.000090 kWh, power : 30.170359179230772 W
[codecarbon INFO @ 00:28:01] Energy consumed for All CPU : 0.000220 kWh
[codecarbon INFO @ 00:28:01] Energy consumed for all GPUs : 0.001209 kWh. Total GPU Power : 241.4976233458263 W
[codecarbon INFO @ 00:28:01] 0.001940 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:28:01] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:28:01', project_name='codecarbon', run_id='c9db78f1-04f3-4401-b676-c951c20e7710', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=27.359856681898236, emissions=0.00046101040504477824, emissions_rate=1.6849883769668678e-05, cpu_power=30.170359179230772, gpu_power=241.4976233458263, ram_power=70.0, cpu_energy=0.00022005701270111017, gpu_energy=0.0012087068002948342, ram_energy=0.0005116055014284535, energy_consumed=0.001940369314424398, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.62s
  Throughput: 839.50 tokens/sec
  Energy per token: 0.96 J/token
  Requests/sec: 11.51
  Total time: 8.69s
  Emissions: 0.000461 kg CO2
  Total energy consumed: 0.001940 kWh

Results saved to: results/run_20260119_002802/results.json
Run 2 completed successfully
--- Run 3 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:28:09 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:28:16] offline tracker init
[codecarbon WARNING @ 00:28:16] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:28:16] [setup] RAM Tracking...
[codecarbon INFO @ 00:28:16] [setup] CPU Tracking...
[codecarbon WARNING @ 00:28:17] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:28:17] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:28:17] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:28:17] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:28:17] [setup] GPU Tracking...
[codecarbon INFO @ 00:28:17] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:28:17] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:28:17] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:28:17] >>> Tracker's metadata:
[codecarbon INFO @ 00:28:17]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:28:17]   Python version: 3.12.3
[codecarbon INFO @ 00:28:17]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:28:17]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:28:17]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:28:17]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:28:17]   GPU count: 4
[codecarbon INFO @ 00:28:17]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:28:17] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 32
  Batch Size: 256
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:28:18 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:28:18 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:28:18 [model.py:1510] Using max model len 32768
INFO 01-19 00:28:20 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:28:20 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:21 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:21 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:21 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_8ff62c10'), local_subscribe_addr='ipc:///tmp/f1e0a87a-f40e-427a-8f0f-3b6d454ec69e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_39bd6db4'), local_subscribe_addr='ipc:///tmp/b8aeeb8b-56ec-43b9-b732-658549d1329e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cffe64ab'), local_subscribe_addr='ipc:///tmp/d55cd0eb-c93a-44b8-88b6-67ee806c7440', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_692e4b68'), local_subscribe_addr='ipc:///tmp/52d18117-dce5-4868-b1bd-b4494c69e2cc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1847d860'), local_subscribe_addr='ipc:///tmp/ea86d4c5-96b4-47f8-a346-cee889505b83', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:25 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:26 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:26 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:26 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:26 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0591bdb3'), local_subscribe_addr='ipc:///tmp/8d5e260d-8674-4fe7-a178-61ebee55cceb', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:26 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:26 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:26 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:26 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m WARNING 01-19 00:28:26 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP3 pid=2234887)[0;0m INFO 01-19 00:28:26 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP2 pid=2234885)[0;0m INFO 01-19 00:28:26 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP1 pid=2234883)[0;0m INFO 01-19 00:28:26 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m INFO 01-19 00:28:26 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP3 pid=2234887)[0;0m INFO 01-19 00:28:26 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP2 pid=2234885)[0;0m INFO 01-19 00:28:27 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m INFO 01-19 00:28:27 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP1 pid=2234883)[0;0m INFO 01-19 00:28:27 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP3 pid=2234887)[0;0m INFO 01-19 00:28:27 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m INFO 01-19 00:28:27 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP1 pid=2234883)[0;0m INFO 01-19 00:28:27 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP2 pid=2234885)[0;0m INFO 01-19 00:28:27 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP3 pid=2234887)[0;0m INFO 01-19 00:28:27 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP1 pid=2234883)[0;0m INFO 01-19 00:28:27 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP2 pid=2234885)[0;0m INFO 01-19 00:28:27 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m INFO 01-19 00:28:27 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m INFO 01-19 00:28:28 [weight_utils.py:413] Time spent downloading weights for mistralai/Mistral-7B-Instruct-v0.1: 0.547775 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP3 pid=2234887)[0;0m INFO 01-19 00:28:28 [default_loader.py:267] Loading weights took 1.18 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.46it/s]
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.01it/s]
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.91it/s]
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m 
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m INFO 01-19 00:28:29 [default_loader.py:267] Loading weights took 1.08 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP3 pid=2234887)[0;0m INFO 01-19 00:28:29 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.920574 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP1 pid=2234883)[0;0m INFO 01-19 00:28:29 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP2 pid=2234885)[0;0m INFO 01-19 00:28:30 [default_loader.py:267] Loading weights took 1.03 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m INFO 01-19 00:28:30 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.434507 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP1 pid=2234883)[0;0m INFO 01-19 00:28:30 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.654774 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP2 pid=2234885)[0;0m INFO 01-19 00:28:30 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.964638 seconds
[codecarbon INFO @ 00:28:33] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:28:33] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.050312826000003 W
[codecarbon INFO @ 00:28:33] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:28:33] Energy consumed for all GPUs : 0.000408 kWh. Total GPU Power : 91.6402614444444 W
[codecarbon INFO @ 00:28:33] 0.000839 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP1 pid=2234883)[0;0m INFO 01-19 00:28:35 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP3 pid=2234887)[0;0m INFO 01-19 00:28:35 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m INFO 01-19 00:28:35 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP2 pid=2234885)[0;0m INFO 01-19 00:28:35 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP1 pid=2234883)[0;0m WARNING 01-19 00:28:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP0 pid=2234881)[0;0m WARNING 01-19 00:28:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP2 pid=2234885)[0;0m WARNING 01-19 00:28:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m [1;36m(Worker_TP3 pid=2234887)[0;0m WARNING 01-19 00:28:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:35 [core.py:210] init engine (profile, create kv cache, warmup model) took 5.05 seconds
[1;36m(EngineCore_DP0 pid=2234874)[0;0m INFO 01-19 00:28:36 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:28:36 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 16218
INFO 01-19 00:28:36 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1771.39it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:10,  1.32s/it, est. speed input: 118.36 toks/s, output: 0.76 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:55,  1.18s/it, est. speed input: 99.06 toks/s, output: 0.83 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:16,  5.39it/s, est. speed input: 1078.61 toks/s, output: 9.94 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.61it/s, est. speed input: 2030.72 toks/s, output: 28.61 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 19.93it/s, est. speed input: 2797.52 toks/s, output: 52.16 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.65it/s, est. speed input: 3305.93 toks/s, output: 81.58 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 29.35it/s, est. speed input: 3696.20 toks/s, output: 121.31 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.24it/s, est. speed input: 3695.06 toks/s, output: 156.22 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.22it/s, est. speed input: 3562.24 toks/s, output: 197.81 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.56it/s, est. speed input: 3570.58 toks/s, output: 237.88 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 17.93it/s, est. speed input: 3321.44 toks/s, output: 270.77 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.26it/s, est. speed input: 3469.19 toks/s, output: 308.40 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.02it/s, est. speed input: 3431.37 toks/s, output: 343.44 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.18it/s, est. speed input: 3279.39 toks/s, output: 370.16 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:04<00:02, 14.75it/s, est. speed input: 3267.06 toks/s, output: 393.37 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.10it/s, est. speed input: 3208.41 toks/s, output: 399.94 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.64it/s, est. speed input: 3190.60 toks/s, output: 446.72 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.96it/s, est. speed input: 3088.55 toks/s, output: 471.98 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 12.00it/s, est. speed input: 3053.67 toks/s, output: 505.23 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.86it/s, est. speed input: 2965.30 toks/s, output: 529.92 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.25it/s, est. speed input: 3060.50 toks/s, output: 561.36 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.58it/s, est. speed input: 2995.58 toks/s, output: 588.86 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.02it/s, est. speed input: 2853.86 toks/s, output: 603.69 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.57it/s, est. speed input: 2796.38 toks/s, output: 614.93 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.48it/s, est. speed input: 2710.90 toks/s, output: 620.69 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:02,  3.86it/s, est. speed input: 2498.76 toks/s, output: 599.42 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.24it/s, est. speed input: 2471.20 toks/s, output: 643.94 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.03it/s, est. speed input: 2342.31 toks/s, output: 639.40 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.03it/s, est. speed input: 2625.96 toks/s, output: 845.79 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.59it/s, est. speed input: 2625.96 toks/s, output: 845.79 toks/s]
[codecarbon INFO @ 00:28:45] Energy consumed for RAM : 0.000532 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:28:45] Delta energy consumed for CPU with cpu_load : 0.000099 kWh, power : 30.171377620714285 W
[codecarbon INFO @ 00:28:45] Energy consumed for All CPU : 0.000229 kWh
[codecarbon INFO @ 00:28:45] Energy consumed for all GPUs : 0.001232 kWh. Total GPU Power : 240.38171980863328 W
[codecarbon INFO @ 00:28:45] 0.001993 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:28:45', project_name='codecarbon', run_id='18b34b3c-75d5-4b5e-94c2-10a6e4d29a62', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=28.414379736874253, emissions=0.00047352481380946455, emissions_rate=1.6664970982806855e-05, cpu_power=30.171377620714285, gpu_power=240.38171980863328, ram_power=70.0, cpu_energy=0.000228834741185328, gpu_energy=0.0012321173745810654, ram_energy=0.0005320897077585364, energy_consumed=0.0019930418235249298, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 18.69s
  Throughput: 840.01 tokens/sec
  Energy per token: 0.98 J/token
  Requests/sec: 11.51
  Total time: 8.68s
  Emissions: 0.000474 kg CO2
  Total energy consumed: 0.001993 kWh

Results saved to: results/run_20260119_002847/results.json
Run 3 completed successfully
--- Run 4 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:28:53 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:28:59] offline tracker init
[codecarbon WARNING @ 00:28:59] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:28:59] [setup] RAM Tracking...
[codecarbon INFO @ 00:28:59] [setup] CPU Tracking...
[codecarbon WARNING @ 00:29:00] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:29:00] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:29:00] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:29:00] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:29:00] [setup] GPU Tracking...
[codecarbon INFO @ 00:29:00] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:29:00] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:29:00] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:29:00] >>> Tracker's metadata:
[codecarbon INFO @ 00:29:00]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:29:00]   Python version: 3.12.3
[codecarbon INFO @ 00:29:00]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:29:00]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:29:00]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:29:00]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:29:00]   GPU count: 4
[codecarbon INFO @ 00:29:00]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:29:00] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 32
  Batch Size: 256
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:29:00 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:29:01 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:29:01 [model.py:1510] Using max model len 32768
INFO 01-19 00:29:03 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:29:03 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:03 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:03 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:03 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_6d03a3c5'), local_subscribe_addr='ipc:///tmp/cc59fe33-3196-42b4-9e40-ea3eb826047b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2e0c4366'), local_subscribe_addr='ipc:///tmp/a7525b01-b4fc-4b1b-866e-9422a53760eb', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9b9fc718'), local_subscribe_addr='ipc:///tmp/ccacba78-6eaa-4cc8-bf70-9fa730c881f0', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_26973990'), local_subscribe_addr='ipc:///tmp/f361f92c-9d6e-4fe5-a36a-5351ace32a66', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fceb5562'), local_subscribe_addr='ipc:///tmp/7c7f7e85-5457-413e-92bb-9f3d6037bb40', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_8451b700'), local_subscribe_addr='ipc:///tmp/d7fee145-4d04-45ce-a18c-4590cbe57ecc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:07 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m WARNING 01-19 00:29:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP1 pid=2236088)[0;0m INFO 01-19 00:29:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP3 pid=2236092)[0;0m INFO 01-19 00:29:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP2 pid=2236090)[0;0m INFO 01-19 00:29:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m INFO 01-19 00:29:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP1 pid=2236088)[0;0m INFO 01-19 00:29:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP3 pid=2236092)[0;0m INFO 01-19 00:29:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m INFO 01-19 00:29:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP2 pid=2236090)[0;0m INFO 01-19 00:29:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP1 pid=2236088)[0;0m INFO 01-19 00:29:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP3 pid=2236092)[0;0m INFO 01-19 00:29:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP2 pid=2236090)[0;0m INFO 01-19 00:29:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m INFO 01-19 00:29:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP1 pid=2236088)[0;0m INFO 01-19 00:29:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP3 pid=2236092)[0;0m INFO 01-19 00:29:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m INFO 01-19 00:29:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP2 pid=2236090)[0;0m INFO 01-19 00:29:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP1 pid=2236088)[0;0m INFO 01-19 00:29:10 [default_loader.py:267] Loading weights took 0.91 seconds
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.53it/s]
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.08it/s]
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m 
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m INFO 01-19 00:29:10 [default_loader.py:267] Loading weights took 1.05 seconds
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP3 pid=2236092)[0;0m INFO 01-19 00:29:10 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP1 pid=2236088)[0;0m INFO 01-19 00:29:10 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.608127 seconds
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP2 pid=2236090)[0;0m INFO 01-19 00:29:11 [default_loader.py:267] Loading weights took 1.04 seconds
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m INFO 01-19 00:29:11 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.993373 seconds
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP3 pid=2236092)[0;0m INFO 01-19 00:29:11 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.150076 seconds
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP2 pid=2236090)[0;0m INFO 01-19 00:29:12 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.597876 seconds
[codecarbon INFO @ 00:29:15] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP2 pid=2236090)[0;0m INFO 01-19 00:29:16 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[codecarbon INFO @ 00:29:16] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.06645222 W
[codecarbon INFO @ 00:29:16] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:29:16] Energy consumed for all GPUs : 0.000455 kWh. Total GPU Power : 102.10911750917413 W
[codecarbon INFO @ 00:29:16] 0.000886 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m INFO 01-19 00:29:16 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP3 pid=2236092)[0;0m INFO 01-19 00:29:16 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP1 pid=2236088)[0;0m INFO 01-19 00:29:16 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP2 pid=2236090)[0;0m WARNING 01-19 00:29:16 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP3 pid=2236092)[0;0m WARNING 01-19 00:29:16 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP1 pid=2236088)[0;0m WARNING 01-19 00:29:16 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m [1;36m(Worker_TP0 pid=2236086)[0;0m WARNING 01-19 00:29:16 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:16 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.95 seconds
[1;36m(EngineCore_DP0 pid=2236080)[0;0m INFO 01-19 00:29:17 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:29:17 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 16218
INFO 01-19 00:29:17 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1260.91it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:07,  1.29s/it, est. speed input: 120.78 toks/s, output: 0.77 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:54,  1.17s/it, est. speed input: 100.23 toks/s, output: 0.84 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:02<00:16,  5.43it/s, est. speed input: 1088.79 toks/s, output: 10.03 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.71it/s, est. speed input: 2051.02 toks/s, output: 28.89 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 20.08it/s, est. speed input: 2824.82 toks/s, output: 52.67 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.86it/s, est. speed input: 3338.76 toks/s, output: 82.39 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 29.65it/s, est. speed input: 3734.07 toks/s, output: 122.55 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.52it/s, est. speed input: 3734.92 toks/s, output: 157.90 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.48it/s, est. speed input: 3601.23 toks/s, output: 199.98 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.84it/s, est. speed input: 3610.23 toks/s, output: 240.52 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 18.09it/s, est. speed input: 3355.98 toks/s, output: 273.58 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.39it/s, est. speed input: 3503.71 toks/s, output: 311.47 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.19it/s, est. speed input: 3466.38 toks/s, output: 346.95 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.33it/s, est. speed input: 3313.15 toks/s, output: 373.98 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:04<00:02, 14.84it/s, est. speed input: 3298.87 toks/s, output: 397.20 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.12it/s, est. speed input: 3236.65 toks/s, output: 403.46 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.65it/s, est. speed input: 3217.44 toks/s, output: 450.48 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.96it/s, est. speed input: 3112.95 toks/s, output: 475.71 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.99it/s, est. speed input: 3077.16 toks/s, output: 509.11 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.84it/s, est. speed input: 2986.70 toks/s, output: 533.74 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.22it/s, est. speed input: 3081.86 toks/s, output: 565.27 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.56it/s, est. speed input: 3015.47 toks/s, output: 592.77 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  7.96it/s, est. speed input: 2869.56 toks/s, output: 607.02 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.50it/s, est. speed input: 2810.01 toks/s, output: 617.92 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.39it/s, est. speed input: 2721.45 toks/s, output: 623.11 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:02,  3.81it/s, est. speed input: 2504.96 toks/s, output: 600.91 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.16it/s, est. speed input: 2476.17 toks/s, output: 645.23 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  3.98it/s, est. speed input: 2345.85 toks/s, output: 640.37 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  3.98it/s, est. speed input: 2630.01 toks/s, output: 847.10 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.61it/s, est. speed input: 2630.01 toks/s, output: 847.10 toks/s]
[codecarbon INFO @ 00:29:26] Energy consumed for RAM : 0.000500 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:29:27] Delta energy consumed for CPU with cpu_load : 0.000086 kWh, power : 30.195406200000004 W
[codecarbon INFO @ 00:29:27] Energy consumed for All CPU : 0.000215 kWh
[codecarbon INFO @ 00:29:27] Energy consumed for all GPUs : 0.001197 kWh. Total GPU Power : 249.38007357223836 W
[codecarbon INFO @ 00:29:27] 0.001913 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:29:27] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:29:27', project_name='codecarbon', run_id='8ee4aa74-3e79-4fd7-b8be-d4ab1dabc0d2', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=26.794738815166056, emissions=0.00045447299642207276, emissions_rate=1.696127734467175e-05, cpu_power=30.195406200000004, gpu_power=249.38007357223836, ram_power=70.0, cpu_energy=0.00021535168901410612, gpu_energy=0.0011970187353895767, ram_energy=0.0005004832700942644, energy_consumed=0.0019128536944979472, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.05s
  Throughput: 839.16 tokens/sec
  Energy per token: 0.94 J/token
  Requests/sec: 11.50
  Total time: 8.69s
  Emissions: 0.000454 kg CO2
  Total energy consumed: 0.001913 kWh

Results saved to: results/run_20260119_002928/results.json
Run 4 completed successfully
--- Run 5 of 5 ---
Using GPUs: 1,2,3,4
INFO 01-19 00:29:33 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:29:39] offline tracker init
[codecarbon WARNING @ 00:29:39] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:29:39] [setup] RAM Tracking...
[codecarbon INFO @ 00:29:39] [setup] CPU Tracking...
[codecarbon WARNING @ 00:29:40] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:29:40] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:29:40] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:29:40] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:29:40] [setup] GPU Tracking...
[codecarbon INFO @ 00:29:40] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:29:40] You have 8 GPUs but we will monitor only 4 ([1, 2, 3, 4]) of them. Check your configuration.
[codecarbon INFO @ 00:29:40] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:29:40] >>> Tracker's metadata:
[codecarbon INFO @ 00:29:40]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:29:40]   Python version: 3.12.3
[codecarbon INFO @ 00:29:40]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:29:40]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:29:40]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:29:40]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:29:40]   GPU count: 4
[codecarbon INFO @ 00:29:40]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1', '2', '3', '4']
[codecarbon INFO @ 00:29:40] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1, 2, 3, 4]
  Tensor Parallel Size: 4
  Pipeline Parallel Size: 1
  Total GPUs: 4
  Block Size: 32
  Batch Size: 256
  Num Prompts: 100
  Chunked Prefill: True
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:29:40 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:29:41 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:29:41 [model.py:1510] Using max model len 32768
INFO 01-19 00:29:43 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:29:43 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:44 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:44 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:44 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_cf6a1fe6'), local_subscribe_addr='ipc:///tmp/19407ffb-e5f2-4c33-b801-ba3fca3cc6d6', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_381d64d2'), local_subscribe_addr='ipc:///tmp/6c913c38-50c6-437f-9a66-239b1ae814ec', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9a495898'), local_subscribe_addr='ipc:///tmp/c595107d-9c94-498c-b03d-1465640c2a8f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4658eeab'), local_subscribe_addr='ipc:///tmp/5ec1bc08-c41f-4e7b-85f0-b0ead5efcd89', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_49759ca6'), local_subscribe_addr='ipc:///tmp/75289414-47e3-4ca2-b504-fff7bbe4f980', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:48 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:48 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:48 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:48 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_0397e4e3'), local_subscribe_addr='ipc:///tmp/65ff25ec-cf66-4868-8bbb-664746406208', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [__init__.py:1384] Found nccl from library libnccl.so.2
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:48 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m WARNING 01-19 00:29:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP3 pid=2237271)[0;0m INFO 01-19 00:29:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP2 pid=2237269)[0;0m INFO 01-19 00:29:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m INFO 01-19 00:29:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP1 pid=2237267)[0;0m INFO 01-19 00:29:48 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP3 pid=2237271)[0;0m INFO 01-19 00:29:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP3 pid=2237271)[0;0m INFO 01-19 00:29:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m INFO 01-19 00:29:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP2 pid=2237269)[0;0m INFO 01-19 00:29:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP1 pid=2237267)[0;0m INFO 01-19 00:29:49 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP2 pid=2237269)[0;0m INFO 01-19 00:29:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP1 pid=2237267)[0;0m INFO 01-19 00:29:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m INFO 01-19 00:29:49 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP3 pid=2237271)[0;0m INFO 01-19 00:29:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP2 pid=2237269)[0;0m INFO 01-19 00:29:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m INFO 01-19 00:29:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP1 pid=2237267)[0;0m INFO 01-19 00:29:49 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP1 pid=2237267)[0;0m INFO 01-19 00:29:50 [default_loader.py:267] Loading weights took 1.06 seconds
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.57it/s]
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP2 pid=2237269)[0;0m INFO 01-19 00:29:51 [default_loader.py:267] Loading weights took 1.04 seconds
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.12it/s]
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.02it/s]
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m 
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m INFO 01-19 00:29:51 [default_loader.py:267] Loading weights took 1.02 seconds
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP3 pid=2237271)[0;0m INFO 01-19 00:29:51 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP1 pid=2237267)[0;0m INFO 01-19 00:29:51 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.806311 seconds
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP2 pid=2237269)[0;0m INFO 01-19 00:29:51 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.063990 seconds
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP3 pid=2237271)[0;0m INFO 01-19 00:29:52 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.470414 seconds
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m INFO 01-19 00:29:52 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.319222 seconds
[codecarbon INFO @ 00:29:55] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:29:56] Delta energy consumed for CPU with cpu_load : 0.000130 kWh, power : 30.064491912 W
[codecarbon INFO @ 00:29:56] Energy consumed for All CPU : 0.000130 kWh
[codecarbon INFO @ 00:29:56] Energy consumed for all GPUs : 0.000449 kWh. Total GPU Power : 100.74430774796215 W
[codecarbon INFO @ 00:29:56] 0.000880 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP3 pid=2237271)[0;0m INFO 01-19 00:29:56 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m INFO 01-19 00:29:56 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP1 pid=2237267)[0;0m INFO 01-19 00:29:56 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP2 pid=2237269)[0;0m INFO 01-19 00:29:56 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:56 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:56 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:56 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:56 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:56 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:56 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:56 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:56 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP3 pid=2237271)[0;0m WARNING 01-19 00:29:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP2 pid=2237269)[0;0m WARNING 01-19 00:29:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP0 pid=2237265)[0;0m WARNING 01-19 00:29:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m [1;36m(Worker_TP1 pid=2237267)[0;0m WARNING 01-19 00:29:56 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:57 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.84 seconds
[1;36m(EngineCore_DP0 pid=2237259)[0;0m INFO 01-19 00:29:57 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:29:57 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 16218
INFO 01-19 00:29:58 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1785.21it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:08,  1.29s/it, est. speed input: 44.85 toks/s, output: 1.55 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:02<01:10,  1.37it/s, est. speed input: 212.06 toks/s, output: 1.70 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:02<00:13,  6.64it/s, est. speed input: 1112.69 toks/s, output: 11.34 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.13it/s, est. speed input: 1738.07 toks/s, output: 27.97 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:02<00:04, 16.27it/s, est. speed input: 2409.30 toks/s, output: 43.34 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 19.98it/s, est. speed input: 2798.56 toks/s, output: 62.65 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:03<00:03, 21.51it/s, est. speed input: 2900.94 toks/s, output: 87.56 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:03<00:02, 26.73it/s, est. speed input: 2943.53 toks/s, output: 127.88 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:03<00:02, 25.80it/s, est. speed input: 3191.09 toks/s, output: 160.89 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:03<00:01, 26.08it/s, est. speed input: 3409.16 toks/s, output: 194.77 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:03<00:01, 27.79it/s, est. speed input: 3488.53 toks/s, output: 230.85 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:03<00:02, 20.93it/s, est. speed input: 3340.66 toks/s, output: 261.25 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 17.85it/s, est. speed input: 3168.37 toks/s, output: 285.48 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:02, 16.48it/s, est. speed input: 3233.60 toks/s, output: 316.81 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:02, 16.31it/s, est. speed input: 3452.01 toks/s, output: 351.28 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:01, 17.66it/s, est. speed input: 3407.94 toks/s, output: 390.62 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:04<00:01, 18.03it/s, est. speed input: 3452.81 toks/s, output: 416.81 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:04<00:01, 15.91it/s, est. speed input: 3557.49 toks/s, output: 437.33 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:01, 13.38it/s, est. speed input: 3569.79 toks/s, output: 468.44 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:01, 12.97it/s, est. speed input: 3526.47 toks/s, output: 494.18 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 10.69it/s, est. speed input: 3358.37 toks/s, output: 512.02 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:02,  8.91it/s, est. speed input: 3353.74 toks/s, output: 528.97 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 10.32it/s, est. speed input: 3311.76 toks/s, output: 564.78 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:02,  6.04it/s, est. speed input: 3044.20 toks/s, output: 555.58 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:02,  5.37it/s, est. speed input: 2810.08 toks/s, output: 580.47 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:08<00:02,  3.99it/s, est. speed input: 2607.99 toks/s, output: 565.97 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.59it/s, est. speed input: 2491.14 toks/s, output: 569.02 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  3.59it/s, est. speed input: 2644.59 toks/s, output: 866.74 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.68it/s, est. speed input: 2644.59 toks/s, output: 866.74 toks/s]
[codecarbon INFO @ 00:30:06] Energy consumed for RAM : 0.000501 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:30:07] Delta energy consumed for CPU with cpu_load : 0.000086 kWh, power : 30.2081349675 W
[codecarbon INFO @ 00:30:07] Energy consumed for All CPU : 0.000216 kWh
[codecarbon INFO @ 00:30:07] Energy consumed for all GPUs : 0.001185 kWh. Total GPU Power : 246.80675222193582 W
[codecarbon INFO @ 00:30:07] 0.001901 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:30:07', project_name='codecarbon', run_id='24f20f2f-6282-4cc4-8170-64735fe16db5', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=26.808721255976707, emissions=0.0004516529929993087, emissions_rate=1.684724119016373e-05, cpu_power=30.2081349675, gpu_power=246.80675222193582, ram_power=70.0, cpu_energy=0.00021554466191780612, gpu_energy=0.0011845615032015644, ram_energy=0.0005008782786019664, energy_consumed=0.001900984443721337, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.14s
  Throughput: 860.81 tokens/sec
  Energy per token: 0.92 J/token
  Requests/sec: 11.60
  Total time: 8.62s
  Emissions: 0.000452 kg CO2
  Total energy consumed: 0.001901 kWh

Results saved to: results/run_20260119_003008/results.json
Run 5 completed successfully
========================================
Command 5: python vllm_evaluation.py --num_prompts 100 --block_size 32 --max_num_seqs 128 --max_num_batched_tokens 12288 --tensor_parallel_size 1 --enable_prefix_caching
Required GPUs: 1
========================================
--- Run 1 of 5 ---
Using GPUs: 1
INFO 01-19 00:30:14 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:30:19] offline tracker init
[codecarbon WARNING @ 00:30:19] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:30:19] [setup] RAM Tracking...
[codecarbon INFO @ 00:30:19] [setup] CPU Tracking...
[codecarbon WARNING @ 00:30:21] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:30:21] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:30:21] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:30:21] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:30:21] [setup] GPU Tracking...
[codecarbon INFO @ 00:30:21] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:30:21] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:30:21] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:30:21] >>> Tracker's metadata:
[codecarbon INFO @ 00:30:21]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:30:21]   Python version: 3.12.3
[codecarbon INFO @ 00:30:21]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:30:21]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:30:21]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:30:21]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:30:21]   GPU count: 1
[codecarbon INFO @ 00:30:21]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:30:21] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 32
  Batch Size: 128
  Num Prompts: 100
  Chunked Prefill: False
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:30:21 [utils.py:233] non-default args: {'seed': 42, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:30:22 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:30:22 [model.py:1510] Using max model len 32768
INFO 01-19 00:30:24 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:30:24 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:24 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:24 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:27 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2239677)[0;0m WARNING 01-19 00:30:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:27 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:28 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:28 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:28 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2239677)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2239677)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.40s/it]
[1;36m(EngineCore_DP0 pid=2239677)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.03s/it]
[1;36m(EngineCore_DP0 pid=2239677)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]
[1;36m(EngineCore_DP0 pid=2239677)[0;0m 
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:31 [default_loader.py:267] Loading weights took 2.22 seconds
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:31 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.919583 seconds
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:35 [gpu_worker.py:298] Available KV cache memory: 5.03 GiB
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:35 [kv_cache_utils.py:1087] GPU KV cache size: 41,152 tokens
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 2.51x
[1;36m(EngineCore_DP0 pid=2239677)[0;0m WARNING 01-19 00:30:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:35 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.12 seconds
[1;36m(EngineCore_DP0 pid=2239677)[0;0m INFO 01-19 00:30:36 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:30:36 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1286
INFO 01-19 00:30:36 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1390.36it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][codecarbon INFO @ 00:30:36] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:30:37] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.008833428000003 W
[codecarbon INFO @ 00:30:37] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:30:37] Energy consumed for all GPUs : 0.000150 kWh. Total GPU Power : 33.745343956923065 W
[codecarbon INFO @ 00:30:37] 0.000582 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:   1%|          | 1/100 [00:03<05:03,  3.06s/it, est. speed input: 50.91 toks/s, output: 0.33 toks/s]Processed prompts:   2%|â–         | 2/100 [00:05<04:32,  2.78s/it, est. speed input: 42.19 toks/s, output: 0.35 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:05<00:52,  1.77it/s, est. speed input: 417.99 toks/s, output: 2.91 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:06<00:22,  3.92it/s, est. speed input: 599.04 toks/s, output: 7.14 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:06<00:13,  6.03it/s, est. speed input: 858.08 toks/s, output: 12.09 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:06<00:07,  9.75it/s, est. speed input: 1118.93 toks/s, output: 20.53 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:06<00:06, 10.75it/s, est. speed input: 1260.24 toks/s, output: 25.01 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:06<00:06, 10.92it/s, est. speed input: 1343.56 toks/s, output: 30.22 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:06<00:05, 12.08it/s, est. speed input: 1390.34 toks/s, output: 36.21 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:07<00:04, 13.25it/s, est. speed input: 1467.85 toks/s, output: 42.78 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:07<00:03, 15.66it/s, est. speed input: 1546.45 toks/s, output: 52.49 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:06,  9.09it/s, est. speed input: 1570.34 toks/s, output: 56.53 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:08<00:06,  8.95it/s, est. speed input: 1530.22 toks/s, output: 62.29 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:05,  9.58it/s, est. speed input: 1548.81 toks/s, output: 69.31 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:05,  8.53it/s, est. speed input: 1504.11 toks/s, output: 75.51 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:09<00:06,  7.85it/s, est. speed input: 1471.34 toks/s, output: 82.28 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:09<00:06,  7.28it/s, est. speed input: 1416.37 toks/s, output: 92.32 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:09<00:06,  7.14it/s, est. speed input: 1397.14 toks/s, output: 96.00 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:09<00:06,  6.41it/s, est. speed input: 1365.53 toks/s, output: 99.20 toks/s]INFO 01-19 00:30:46 [loggers.py:127] Engine 000: Avg prompt throughput: 2202.4 tokens/s, Avg generation throughput: 327.7 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.9%, Prefix cache hit rate: 0.0%
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:10<00:06,  5.95it/s, est. speed input: 1344.10 toks/s, output: 106.46 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:10<00:07,  5.32it/s, est. speed input: 1297.28 toks/s, output: 113.33 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:11<00:10,  3.74it/s, est. speed input: 1234.58 toks/s, output: 113.61 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:11<00:09,  3.85it/s, est. speed input: 1234.72 toks/s, output: 117.82 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:11<00:07,  4.56it/s, est. speed input: 1279.29 toks/s, output: 127.99 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:12<00:07,  4.52it/s, est. speed input: 1263.38 toks/s, output: 132.44 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:12<00:07,  4.17it/s, est. speed input: 1265.05 toks/s, output: 136.14 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:12<00:03,  7.40it/s, est. speed input: 1269.90 toks/s, output: 161.37 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:13<00:04,  6.71it/s, est. speed input: 1303.46 toks/s, output: 170.88 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:03,  7.88it/s, est. speed input: 1303.91 toks/s, output: 183.63 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:03,  6.44it/s, est. speed input: 1311.07 toks/s, output: 187.08 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:13<00:03,  6.60it/s, est. speed input: 1313.84 toks/s, output: 198.08 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:14<00:02,  6.73it/s, est. speed input: 1312.78 toks/s, output: 209.30 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:14<00:03,  5.93it/s, est. speed input: 1348.86 toks/s, output: 218.63 toks/s][codecarbon INFO @ 00:30:51] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:15<00:04,  3.88it/s, est. speed input: 1273.94 toks/s, output: 221.43 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:15<00:04,  3.55it/s, est. speed input: 1246.82 toks/s, output: 224.38 toks/s][codecarbon INFO @ 00:30:52] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.003315127500002 W
[codecarbon INFO @ 00:30:52] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:30:52] Energy consumed for all GPUs : 0.000449 kWh. Total GPU Power : 71.89009617661372 W
[codecarbon INFO @ 00:30:52] 0.001283 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:17<00:05,  2.39it/s, est. speed input: 1186.03 toks/s, output: 226.12 toks/s]INFO 01-19 00:30:56 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 301.0 tokens/s, Running: 11 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.0%, Prefix cache hit rate: 0.0%
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:20<00:08,  1.28it/s, est. speed input: 1042.49 toks/s, output: 207.51 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:21<00:08,  1.15it/s, est. speed input: 986.33 toks/s, output: 206.15 toks/s] Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:23<00:10,  1.16s/it, est. speed input: 896.04 toks/s, output: 197.63 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.16s/it, est. speed input: 961.20 toks/s, output: 294.91 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.24it/s, est. speed input: 961.20 toks/s, output: 294.91 toks/s]
[codecarbon INFO @ 00:30:59] Energy consumed for RAM : 0.000733 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:31:00] Delta energy consumed for CPU with cpu_load : 0.000064 kWh, power : 30.003262734000003 W
[codecarbon INFO @ 00:31:00] Energy consumed for All CPU : 0.000314 kWh
[codecarbon INFO @ 00:31:00] Energy consumed for all GPUs : 0.000608 kWh. Total GPU Power : 69.8314772620518 W
[codecarbon INFO @ 00:31:00] 0.001654 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:31:00] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:31:00', project_name='codecarbon', run_id='e4bb5f0a-0ba8-467d-a446-e12f76cba74a', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=39.252602509688586, emissions=0.0003930874239514903, emissions_rate=1.0014302207209469e-05, cpu_power=30.003262734000003, gpu_power=69.8314772620518, ram_power=70.0, cpu_energy=0.00031402456961643354, gpu_energy=0.0006079185418883526, ram_energy=0.0007325418349846992, energy_consumed=0.0016544849464894852, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 14.56s
  Throughput: 293.98 tokens/sec
  Energy per token: 0.86 J/token
  Requests/sec: 4.23
  Total time: 23.64s
  Emissions: 0.000393 kg CO2
  Total energy consumed: 0.001654 kWh

Results saved to: results/run_20260119_003100/results.json
Run 1 completed successfully
--- Run 2 of 5 ---
Using GPUs: 1
INFO 01-19 00:31:08 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:31:14] offline tracker init
[codecarbon WARNING @ 00:31:14] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:31:14] [setup] RAM Tracking...
[codecarbon INFO @ 00:31:14] [setup] CPU Tracking...
[codecarbon WARNING @ 00:31:16] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:31:16] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:31:16] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:31:16] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:31:16] [setup] GPU Tracking...
[codecarbon INFO @ 00:31:16] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:31:16] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:31:16] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:31:16] >>> Tracker's metadata:
[codecarbon INFO @ 00:31:16]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:31:16]   Python version: 3.12.3
[codecarbon INFO @ 00:31:16]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:31:16]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:31:16]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:31:16]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:31:16]   GPU count: 1
[codecarbon INFO @ 00:31:16]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:31:16] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 32
  Batch Size: 128
  Num Prompts: 100
  Chunked Prefill: False
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:31:16 [utils.py:233] non-default args: {'seed': 42, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:31:17 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:31:17 [model.py:1510] Using max model len 32768
INFO 01-19 00:31:20 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:31:20 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:20 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:20 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:23 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2241127)[0;0m WARNING 01-19 00:31:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:23 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:24 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:24 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:24 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2241127)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2241127)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.39s/it]
[1;36m(EngineCore_DP0 pid=2241127)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.02s/it]
[1;36m(EngineCore_DP0 pid=2241127)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.07s/it]
[1;36m(EngineCore_DP0 pid=2241127)[0;0m 
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:26 [default_loader.py:267] Loading weights took 2.20 seconds
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:27 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.961203 seconds
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:31 [gpu_worker.py:298] Available KV cache memory: 5.03 GiB
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:31 [kv_cache_utils.py:1087] GPU KV cache size: 41,152 tokens
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:31 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 2.51x
[1;36m(EngineCore_DP0 pid=2241127)[0;0m WARNING 01-19 00:31:31 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:31 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.15 seconds
[codecarbon INFO @ 00:31:31] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[1;36m(EngineCore_DP0 pid=2241127)[0;0m INFO 01-19 00:31:32 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:31:32 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1286
[codecarbon INFO @ 00:31:32] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.003231324 W
[codecarbon INFO @ 00:31:32] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:31:32] Energy consumed for all GPUs : 0.000137 kWh. Total GPU Power : 30.714210142680766 W
[codecarbon INFO @ 00:31:32] 0.000568 kWh of electricity and 0.000000 L of water were used since the beginning.
INFO 01-19 00:31:32 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1877.84it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:03<05:06,  3.10s/it, est. speed input: 50.36 toks/s, output: 0.32 toks/s]Processed prompts:   2%|â–         | 2/100 [00:05<04:32,  2.78s/it, est. speed input: 42.04 toks/s, output: 0.35 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:05<02:31,  1.56s/it, est. speed input: 48.95 toks/s, output: 0.87 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:05<00:31,  2.92it/s, est. speed input: 456.11 toks/s, output: 4.20 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:06<00:13,  6.17it/s, est. speed input: 771.49 toks/s, output: 9.78 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:06<00:08,  9.28it/s, est. speed input: 1020.96 toks/s, output: 16.63 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:06<00:06, 11.14it/s, est. speed input: 1173.32 toks/s, output: 21.88 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:05, 12.09it/s, est. speed input: 1325.01 toks/s, output: 26.39 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:05, 11.96it/s, est. speed input: 1332.19 toks/s, output: 31.90 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:07<00:04, 14.07it/s, est. speed input: 1462.84 toks/s, output: 42.63 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:07<00:03, 16.13it/s, est. speed input: 1541.39 toks/s, output: 52.32 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:05,  9.53it/s, est. speed input: 1565.22 toks/s, output: 56.35 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:08<00:05,  9.31it/s, est. speed input: 1525.41 toks/s, output: 62.09 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:05,  9.86it/s, est. speed input: 1544.02 toks/s, output: 69.10 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:05,  8.74it/s, est. speed input: 1499.53 toks/s, output: 75.28 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:09<00:06,  8.00it/s, est. speed input: 1466.93 toks/s, output: 82.03 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:09<00:06,  7.37it/s, est. speed input: 1412.25 toks/s, output: 92.05 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:09<00:06,  7.22it/s, est. speed input: 1393.29 toks/s, output: 95.74 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:09<00:06,  6.47it/s, est. speed input: 1361.69 toks/s, output: 98.92 toks/s]INFO 01-19 00:31:42 [loggers.py:127] Engine 000: Avg prompt throughput: 2199.3 tokens/s, Avg generation throughput: 327.2 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.9%, Prefix cache hit rate: 0.0%
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:10<00:06,  5.99it/s, est. speed input: 1340.51 toks/s, output: 106.17 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:10<00:07,  5.34it/s, est. speed input: 1293.96 toks/s, output: 113.04 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:11<00:10,  3.76it/s, est. speed input: 1231.61 toks/s, output: 113.34 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:11<00:09,  3.86it/s, est. speed input: 1231.75 toks/s, output: 117.53 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:11<00:07,  4.56it/s, est. speed input: 1276.29 toks/s, output: 127.69 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:12<00:07,  4.53it/s, est. speed input: 1260.47 toks/s, output: 132.13 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:12<00:07,  4.18it/s, est. speed input: 1262.25 toks/s, output: 135.84 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:12<00:03,  7.41it/s, est. speed input: 1267.14 toks/s, output: 161.02 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:13<00:04,  6.70it/s, est. speed input: 1300.57 toks/s, output: 170.50 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:03,  7.87it/s, est. speed input: 1301.03 toks/s, output: 183.22 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:03,  6.42it/s, est. speed input: 1308.01 toks/s, output: 186.65 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:13<00:03,  6.56it/s, est. speed input: 1310.41 toks/s, output: 197.56 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:14<00:02,  6.67it/s, est. speed input: 1309.11 toks/s, output: 208.71 toks/s][codecarbon INFO @ 00:31:46] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:14<00:03,  5.87it/s, est. speed input: 1344.74 toks/s, output: 217.97 toks/s][codecarbon INFO @ 00:31:47] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.003888826875002 W
[codecarbon INFO @ 00:31:47] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:31:47] Energy consumed for all GPUs : 0.000435 kWh. Total GPU Power : 71.68774949047295 W
[codecarbon INFO @ 00:31:47] 0.001268 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:15<00:04,  3.84it/s, est. speed input: 1269.43 toks/s, output: 220.65 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:15<00:04,  3.51it/s, est. speed input: 1242.15 toks/s, output: 223.54 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:17<00:05,  2.36it/s, est. speed input: 1180.45 toks/s, output: 225.06 toks/s]INFO 01-19 00:31:52 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 299.5 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.8%, Prefix cache hit rate: 0.0%
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:20<00:08,  1.26it/s, est. speed input: 1037.03 toks/s, output: 206.43 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:21<00:08,  1.14it/s, est. speed input: 980.95 toks/s, output: 205.03 toks/s] Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:23<00:10,  1.17s/it, est. speed input: 891.22 toks/s, output: 196.57 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.17s/it, est. speed input: 956.04 toks/s, output: 293.32 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.22it/s, est. speed input: 956.04 toks/s, output: 293.32 toks/s]
[codecarbon INFO @ 00:31:56] Energy consumed for RAM : 0.000754 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:31:56] Delta energy consumed for CPU with cpu_load : 0.000073 kWh, power : 30.003282340909095 W
[codecarbon INFO @ 00:31:56] Energy consumed for All CPU : 0.000323 kWh
[codecarbon INFO @ 00:31:56] Energy consumed for all GPUs : 0.000615 kWh. Total GPU Power : 70.06310978196608 W
[codecarbon INFO @ 00:31:56] 0.001692 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:31:56', project_name='codecarbon', run_id='f339b12e-17af-44c4-81d7-31f651372fdc', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.337361466139555, emissions=0.0004019767317930115, emissions_rate=9.965369998988242e-06, cpu_power=30.003282340909095, gpu_power=70.06310978196608, ram_power=70.0, cpu_energy=0.0003230463755361801, gpu_energy=0.0006152154921714725, ram_energy=0.0007536377227321888, energy_consumed=0.0016918995904398415, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 15.54s
  Throughput: 292.64 tokens/sec
  Energy per token: 0.88 J/token
  Requests/sec: 4.21
  Total time: 23.75s
  Emissions: 0.000402 kg CO2
  Total energy consumed: 0.001692 kWh

Results saved to: results/run_20260119_003156/results.json
Run 2 completed successfully
--- Run 3 of 5 ---
Using GPUs: 1
INFO 01-19 00:32:02 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:32:08] offline tracker init
[codecarbon WARNING @ 00:32:08] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:32:08] [setup] RAM Tracking...
[codecarbon INFO @ 00:32:08] [setup] CPU Tracking...
[codecarbon WARNING @ 00:32:09] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:32:09] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:32:09] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:32:09] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:32:09] [setup] GPU Tracking...
[codecarbon INFO @ 00:32:09] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:32:09] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:32:09] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:32:09] >>> Tracker's metadata:
[codecarbon INFO @ 00:32:09]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:32:09]   Python version: 3.12.3
[codecarbon INFO @ 00:32:09]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:32:09]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:32:09]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:32:09]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:32:09]   GPU count: 1
[codecarbon INFO @ 00:32:09]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:32:09] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 32
  Batch Size: 128
  Num Prompts: 100
  Chunked Prefill: False
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:32:10 [utils.py:233] non-default args: {'seed': 42, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:32:11 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:32:11 [model.py:1510] Using max model len 32768
INFO 01-19 00:32:12 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:32:12 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:13 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:13 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:15 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2242268)[0;0m WARNING 01-19 00:32:16 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:16 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:16 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:16 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:16 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2242268)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2242268)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.37s/it]
[1;36m(EngineCore_DP0 pid=2242268)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.00s/it]
[1;36m(EngineCore_DP0 pid=2242268)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
[1;36m(EngineCore_DP0 pid=2242268)[0;0m 
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:19 [default_loader.py:267] Loading weights took 2.16 seconds
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:19 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.869186 seconds
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:23 [gpu_worker.py:298] Available KV cache memory: 5.03 GiB
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:23 [kv_cache_utils.py:1087] GPU KV cache size: 41,152 tokens
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 2.51x
[1;36m(EngineCore_DP0 pid=2242268)[0;0m WARNING 01-19 00:32:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:24 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.14 seconds
[1;36m(EngineCore_DP0 pid=2242268)[0;0m INFO 01-19 00:32:24 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:32:24 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1286
INFO 01-19 00:32:24 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1383.54it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][codecarbon INFO @ 00:32:25] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:32:25] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.002821896000007 W
[codecarbon INFO @ 00:32:25] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:32:25] Energy consumed for all GPUs : 0.000157 kWh. Total GPU Power : 35.19330066045467 W
[codecarbon INFO @ 00:32:25] 0.000588 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:   1%|          | 1/100 [00:03<05:06,  3.09s/it, est. speed input: 18.74 toks/s, output: 0.65 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:05<02:49,  1.75s/it, est. speed input: 88.37 toks/s, output: 0.71 toks/s]Processed prompts:   8%|â–Š         | 8/100 [00:05<00:46,  1.97it/s, est. speed input: 421.27 toks/s, output: 3.08 toks/s]Processed prompts:  12%|â–ˆâ–        | 12/100 [00:06<00:26,  3.34it/s, est. speed input: 487.95 toks/s, output: 5.63 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:06<00:13,  6.00it/s, est. speed input: 719.16 toks/s, output: 11.57 toks/s]Processed prompts:  21%|â–ˆâ–ˆ        | 21/100 [00:06<00:10,  7.19it/s, est. speed input: 919.49 toks/s, output: 15.00 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:06<00:07, 10.06it/s, est. speed input: 1093.27 toks/s, output: 22.65 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:06, 10.35it/s, est. speed input: 1176.19 toks/s, output: 27.91 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:07<00:06, 10.56it/s, est. speed input: 1218.51 toks/s, output: 31.64 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:07<00:07,  8.98it/s, est. speed input: 1183.67 toks/s, output: 35.46 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:06,  9.67it/s, est. speed input: 1156.42 toks/s, output: 43.05 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:07<00:05, 11.36it/s, est. speed input: 1254.50 toks/s, output: 54.39 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:08<00:03, 13.87it/s, est. speed input: 1291.57 toks/s, output: 69.66 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:08<00:04, 12.32it/s, est. speed input: 1305.34 toks/s, output: 75.43 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:08<00:04, 11.18it/s, est. speed input: 1325.34 toks/s, output: 81.35 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:08<00:04, 10.38it/s, est. speed input: 1362.38 toks/s, output: 87.75 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:08<00:03, 12.22it/s, est. speed input: 1365.30 toks/s, output: 99.56 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:09<00:05,  8.43it/s, est. speed input: 1379.41 toks/s, output: 104.55 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:09<00:05,  7.26it/s, est. speed input: 1434.34 toks/s, output: 110.69 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:09<00:04,  9.34it/s, est. speed input: 1466.13 toks/s, output: 124.70 toks/s]INFO 01-19 00:32:34 [loggers.py:127] Engine 000: Avg prompt throughput: 2197.7 tokens/s, Avg generation throughput: 327.6 tokens/s, Running: 39 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.0%, Prefix cache hit rate: 0.0%
Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:10<00:04,  9.19it/s, est. speed input: 1443.29 toks/s, output: 132.97 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:10<00:04,  7.13it/s, est. speed input: 1438.80 toks/s, output: 139.01 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:10<00:04,  7.05it/s, est. speed input: 1426.73 toks/s, output: 143.08 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:11<00:05,  6.36it/s, est. speed input: 1492.68 toks/s, output: 146.23 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:11<00:06,  4.59it/s, est. speed input: 1441.73 toks/s, output: 146.88 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:11<00:04,  6.67it/s, est. speed input: 1498.31 toks/s, output: 163.38 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:12<00:06,  4.62it/s, est. speed input: 1449.28 toks/s, output: 163.49 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:12<00:06,  4.04it/s, est. speed input: 1439.17 toks/s, output: 165.73 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:12<00:04,  5.49it/s, est. speed input: 1432.98 toks/s, output: 177.97 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:06,  3.57it/s, est. speed input: 1380.63 toks/s, output: 176.82 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:14<00:09,  2.51it/s, est. speed input: 1305.29 toks/s, output: 174.79 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:14<00:08,  2.46it/s, est. speed input: 1268.73 toks/s, output: 177.57 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:14<00:08,  2.54it/s, est. speed input: 1294.95 toks/s, output: 181.41 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:15<00:06,  2.90it/s, est. speed input: 1295.38 toks/s, output: 187.03 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:15<00:05,  3.25it/s, est. speed input: 1282.29 toks/s, output: 192.70 toks/s][codecarbon INFO @ 00:32:40] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:15<00:05,  3.33it/s, est. speed input: 1259.72 toks/s, output: 197.58 toks/s][codecarbon INFO @ 00:32:40] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.002913958125003 W
[codecarbon INFO @ 00:32:40] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:32:40] Energy consumed for all GPUs : 0.000456 kWh. Total GPU Power : 71.95754015392792 W
[codecarbon INFO @ 00:32:40] 0.001290 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:16<00:08,  1.93it/s, est. speed input: 1181.03 toks/s, output: 193.88 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:16<00:06,  2.34it/s, est. speed input: 1169.94 toks/s, output: 200.33 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:17<00:06,  2.40it/s, est. speed input: 1149.19 toks/s, output: 204.85 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:17<00:06,  2.24it/s, est. speed input: 1119.50 toks/s, output: 208.11 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:18<00:07,  1.67it/s, est. speed input: 1063.96 toks/s, output: 206.95 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:19<00:06,  1.93it/s, est. speed input: 1049.73 toks/s, output: 213.03 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:19<00:06,  1.75it/s, est. speed input: 1030.58 toks/s, output: 215.30 toks/s]INFO 01-19 00:32:44 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 307.6 tokens/s, Running: 10 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 0.0%
Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:20<00:04,  2.09it/s, est. speed input: 1021.84 toks/s, output: 222.48 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:20<00:04,  2.06it/s, est. speed input: 1034.75 toks/s, output: 227.08 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:21<00:04,  1.61it/s, est. speed input: 992.75 toks/s, output: 227.55 toks/s] Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:22<00:05,  1.27it/s, est. speed input: 948.96 toks/s, output: 226.24 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:23<00:04,  1.20it/s, est. speed input: 914.67 toks/s, output: 228.12 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.20it/s, est. speed input: 955.04 toks/s, output: 292.30 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.22it/s, est. speed input: 955.04 toks/s, output: 292.30 toks/s]
[codecarbon INFO @ 00:32:48] Energy consumed for RAM : 0.000730 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:32:48] Delta energy consumed for CPU with cpu_load : 0.000063 kWh, power : 30.003536919000005 W
[codecarbon INFO @ 00:32:48] Energy consumed for All CPU : 0.000313 kWh
[codecarbon INFO @ 00:32:48] Energy consumed for all GPUs : 0.000611 kWh. Total GPU Power : 69.31212498566485 W
[codecarbon INFO @ 00:32:48] 0.001653 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:32:48] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:32:48', project_name='codecarbon', run_id='65ce7735-ffae-44b0-bf38-490de54a4026', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=39.10854472918436, emissions=0.00039281748126841564, emissions_rate=1.0044287865697021e-05, cpu_power=30.003536919000005, gpu_power=69.31212498566485, ram_power=70.0, cpu_energy=0.00031282455147272443, gpu_energy=0.00061072659968886, ram_energy=0.0007297976203236936, energy_consumed=0.0016533487714852778, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 14.27s
  Throughput: 291.39 tokens/sec
  Energy per token: 0.86 J/token
  Requests/sec: 4.20
  Total time: 23.79s
  Emissions: 0.000393 kg CO2
  Total energy consumed: 0.001653 kWh

Results saved to: results/run_20260119_003249/results.json
Run 3 completed successfully
--- Run 4 of 5 ---
Using GPUs: 1
INFO 01-19 00:32:57 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:33:04] offline tracker init
[codecarbon WARNING @ 00:33:04] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:33:04] [setup] RAM Tracking...
[codecarbon INFO @ 00:33:04] [setup] CPU Tracking...
[codecarbon WARNING @ 00:33:05] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:33:05] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:33:05] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:33:05] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:33:05] [setup] GPU Tracking...
[codecarbon INFO @ 00:33:05] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:33:05] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:33:05] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:33:05] >>> Tracker's metadata:
[codecarbon INFO @ 00:33:05]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:33:05]   Python version: 3.12.3
[codecarbon INFO @ 00:33:05]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:33:05]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:33:05]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:33:05]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:33:05]   GPU count: 1
[codecarbon INFO @ 00:33:05]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:33:05] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 32
  Batch Size: 128
  Num Prompts: 100
  Chunked Prefill: False
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:33:06 [utils.py:233] non-default args: {'seed': 42, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:33:07 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:33:07 [model.py:1510] Using max model len 32768
INFO 01-19 00:33:09 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:33:09 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:10 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:10 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:14 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2243464)[0;0m WARNING 01-19 00:33:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:14 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:14 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:15 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2243464)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2243464)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.46s/it]
[1;36m(EngineCore_DP0 pid=2243464)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]
[1;36m(EngineCore_DP0 pid=2243464)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.12s/it]
[1;36m(EngineCore_DP0 pid=2243464)[0;0m 
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:18 [default_loader.py:267] Loading weights took 2.31 seconds
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:18 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 3.051169 seconds
[codecarbon INFO @ 00:33:21] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:33:21] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.005608638 W
[codecarbon INFO @ 00:33:21] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:33:21] Energy consumed for all GPUs : 0.000131 kWh. Total GPU Power : 29.525141814107254 W
[codecarbon INFO @ 00:33:21] 0.000563 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:22 [gpu_worker.py:298] Available KV cache memory: 5.03 GiB
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:23 [kv_cache_utils.py:1087] GPU KV cache size: 41,152 tokens
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 2.51x
[1;36m(EngineCore_DP0 pid=2243464)[0;0m WARNING 01-19 00:33:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:23 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.51 seconds
[1;36m(EngineCore_DP0 pid=2243464)[0;0m INFO 01-19 00:33:23 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:33:23 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1286
INFO 01-19 00:33:23 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:00<00:00, 154.63it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 344.15it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<03:14,  1.96s/it, est. speed input: 79.40 toks/s, output: 0.51 toks/s]Processed prompts:   2%|â–         | 2/100 [00:05<04:16,  2.61s/it, est. speed input: 42.54 toks/s, output: 0.60 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:05<01:04,  1.45it/s, est. speed input: 348.75 toks/s, output: 1.64 toks/s]Processed prompts:  10%|â–ˆ         | 10/100 [00:05<00:31,  2.83it/s, est. speed input: 436.50 toks/s, output: 3.69 toks/s]Processed prompts:  17%|â–ˆâ–‹        | 17/100 [00:05<00:13,  5.96it/s, est. speed input: 930.71 toks/s, output: 9.36 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:06<00:07, 10.24it/s, est. speed input: 1239.20 toks/s, output: 17.68 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:06<00:06, 11.17it/s, est. speed input: 1230.40 toks/s, output: 21.68 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:04, 13.99it/s, est. speed input: 1504.99 toks/s, output: 30.01 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:06<00:04, 15.74it/s, est. speed input: 1519.49 toks/s, output: 37.77 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:07<00:05, 11.60it/s, est. speed input: 1432.71 toks/s, output: 42.08 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:07<00:05, 10.01it/s, est. speed input: 1520.63 toks/s, output: 46.44 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:07<00:05,  9.60it/s, est. speed input: 1479.36 toks/s, output: 52.05 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:07<00:05,  9.28it/s, est. speed input: 1447.82 toks/s, output: 57.83 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:05,  9.90it/s, est. speed input: 1470.06 toks/s, output: 64.65 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:04, 10.47it/s, est. speed input: 1457.51 toks/s, output: 71.82 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:08<00:05,  8.89it/s, est. speed input: 1423.21 toks/s, output: 77.96 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:08<00:05,  8.76it/s, est. speed input: 1394.18 toks/s, output: 84.77 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:08<00:05,  8.25it/s, est. speed input: 1403.25 toks/s, output: 88.22 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:09<00:03, 10.79it/s, est. speed input: 1426.33 toks/s, output: 101.60 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:09<00:04,  9.44it/s, est. speed input: 1445.36 toks/s, output: 112.81 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:09<00:04,  8.45it/s, est. speed input: 1441.32 toks/s, output: 120.44 toks/s]INFO 01-19 00:33:34 [loggers.py:127] Engine 000: Avg prompt throughput: 2161.6 tokens/s, Avg generation throughput: 317.6 tokens/s, Running: 35 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.2%, Prefix cache hit rate: 0.0%
Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:10<00:04,  7.86it/s, est. speed input: 1509.32 toks/s, output: 127.97 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:11<00:09,  3.76it/s, est. speed input: 1377.71 toks/s, output: 123.04 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:11<00:06,  4.87it/s, est. speed input: 1407.46 toks/s, output: 134.37 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:11<00:05,  5.02it/s, est. speed input: 1470.31 toks/s, output: 143.12 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:11<00:05,  4.92it/s, est. speed input: 1448.19 toks/s, output: 147.30 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:12<00:07,  3.95it/s, est. speed input: 1478.34 toks/s, output: 149.15 toks/s][codecarbon INFO @ 00:33:36] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:12<00:05,  4.42it/s, est. speed input: 1500.15 toks/s, output: 159.24 toks/s][codecarbon INFO @ 00:33:36] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.0034265025 W
[codecarbon INFO @ 00:33:36] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:33:36] Energy consumed for all GPUs : 0.000410 kWh. Total GPU Power : 67.04652144605717 W
[codecarbon INFO @ 00:33:36] 0.001244 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:09,  2.66it/s, est. speed input: 1400.93 toks/s, output: 155.95 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:08,  2.81it/s, est. speed input: 1376.11 toks/s, output: 160.66 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:14<00:08,  2.67it/s, est. speed input: 1340.97 toks/s, output: 163.93 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:14<00:06,  3.19it/s, est. speed input: 1333.63 toks/s, output: 170.51 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:14<00:06,  3.33it/s, est. speed input: 1378.72 toks/s, output: 180.38 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:15<00:05,  3.58it/s, est. speed input: 1365.98 toks/s, output: 186.33 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:15<00:05,  3.58it/s, est. speed input: 1341.95 toks/s, output: 191.52 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:15<00:03,  4.54it/s, est. speed input: 1326.95 toks/s, output: 205.27 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:15<00:03,  4.67it/s, est. speed input: 1322.30 toks/s, output: 211.52 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:16<00:04,  2.99it/s, est. speed input: 1276.29 toks/s, output: 211.70 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:16<00:03,  3.37it/s, est. speed input: 1262.34 toks/s, output: 218.42 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:17<00:05,  2.04it/s, est. speed input: 1195.42 toks/s, output: 215.57 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:18<00:06,  1.77it/s, est. speed input: 1150.01 toks/s, output: 216.61 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:18<00:04,  2.28it/s, est. speed input: 1147.46 toks/s, output: 225.00 toks/s]INFO 01-19 00:33:44 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 292.5 tokens/s, Running: 10 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.3%, Prefix cache hit rate: 0.0%
Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:20<00:07,  1.20it/s, est. speed input: 1047.89 toks/s, output: 215.47 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:21<00:07,  1.01it/s, est. speed input: 988.31 toks/s, output: 212.68 toks/s] Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:23<00:07,  1.08s/it, est. speed input: 933.68 toks/s, output: 211.76 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  3.48it/s, est. speed input: 972.59 toks/s, output: 287.62 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  3.48it/s, est. speed input: 972.59 toks/s, output: 287.62 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.29it/s, est. speed input: 972.59 toks/s, output: 287.62 toks/s]
[codecarbon INFO @ 00:33:47] Energy consumed for RAM : 0.000787 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:33:47] Delta energy consumed for CPU with cpu_load : 0.000087 kWh, power : 30.00298417500001 W
[codecarbon INFO @ 00:33:47] Energy consumed for All CPU : 0.000337 kWh
[codecarbon INFO @ 00:33:47] Energy consumed for all GPUs : 0.000624 kWh. Total GPU Power : 70.19440282428998 W
[codecarbon INFO @ 00:33:47] 0.001748 kWh of electricity and 0.000000 L of water were used since the beginning.
EmissionsData(timestamp='2026-01-19T00:33:47', project_name='codecarbon', run_id='58a9001b-4407-41be-983d-1c8db4821b89', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=42.04348384402692, emissions=0.00041540004730668804, emissions_rate=9.880248003418097e-06, cpu_power=30.00298417500001, gpu_power=70.19440282428998, ram_power=70.0, cpu_energy=0.00033728022825865413, gpu_energy=0.0006242802216505083, ram_energy=0.0007868371918448248, energy_consumed=0.0017483976417539872, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.42s
  Throughput: 284.05 tokens/sec
  Energy per token: 0.94 J/token
  Requests/sec: 4.24
  Total time: 23.58s
  Emissions: 0.000415 kg CO2
  Total energy consumed: 0.001748 kWh

Results saved to: results/run_20260119_003348/results.json
Run 4 completed successfully
--- Run 5 of 5 ---
Using GPUs: 1
INFO 01-19 00:33:55 [__init__.py:216] Automatically detected platform cuda.
[codecarbon INFO @ 00:34:02] offline tracker init
[codecarbon WARNING @ 00:34:02] Multiple instances of codecarbon are allowed to run at the same time.
[codecarbon INFO @ 00:34:02] [setup] RAM Tracking...
[codecarbon INFO @ 00:34:02] [setup] CPU Tracking...
[codecarbon WARNING @ 00:34:03] 	RAPL - Permission denied reading RAPL file /sys/class/powercap/intel-rapl/subsystem/intel-rapl/intel-rapl:1/energy_uj. You can grant read permission with: sudo chmod -R a+r /sys/class/powercap/*
[codecarbon WARNING @ 00:34:03] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. 
 Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU

[codecarbon INFO @ 00:34:03] CPU Model on constant consumption mode: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon WARNING @ 00:34:03] No CPU tracking mode found. Falling back on CPU load mode.
[codecarbon INFO @ 00:34:03] [setup] GPU Tracking...
[codecarbon INFO @ 00:34:03] Tracking Nvidia GPU via pynvml
[codecarbon WARNING @ 00:34:03] You have 8 GPUs but we will monitor only 1 ([1]) of them. Check your configuration.
[codecarbon INFO @ 00:34:03] The below tracking methods have been set up:
                RAM Tracking Method: RAM power estimation model
                CPU Tracking Method: cpu_load
                GPU Tracking Method: pynvml
            
[codecarbon INFO @ 00:34:04] >>> Tracker's metadata:
[codecarbon INFO @ 00:34:04]   Platform system: Linux-6.14.0-37-generic-x86_64-with-glibc2.39
[codecarbon INFO @ 00:34:04]   Python version: 3.12.3
[codecarbon INFO @ 00:34:04]   CodeCarbon version: 3.1.1
[codecarbon INFO @ 00:34:04]   Available RAM : 503.395 GB
[codecarbon INFO @ 00:34:04]   CPU count: 64 thread(s) in 2 physical CPU(s)
[codecarbon INFO @ 00:34:04]   CPU model: INTEL(R) XEON(R) SILVER 4514Y
[codecarbon INFO @ 00:34:04]   GPU count: 1
[codecarbon INFO @ 00:34:04]   GPU model: 8 x NVIDIA L4 BUT only tracking these GPU ids : ['1']
[codecarbon INFO @ 00:34:04] Emissions data (if any) will be saved to file /home/wyn23/r244/src/batch_inference/emissions.csv
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset

======================================================================
vLLM EVALUATION
======================================================================
Configuration:
  Model: mistralai/Mistral-7B-Instruct-v0.1
  GPUs: [1]
  Tensor Parallel Size: 1
  Pipeline Parallel Size: 1
  Total GPUs: 1
  Block Size: 32
  Batch Size: 128
  Num Prompts: 100
  Chunked Prefill: False
  Prefix Caching: True
======================================================================

Initializing vLLM...
INFO 01-19 00:34:04 [utils.py:233] non-default args: {'seed': 42, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 01-19 00:34:05 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 01-19 00:34:05 [model.py:1510] Using max model len 32768
INFO 01-19 00:34:07 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 01-19 00:34:07 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
  return get_tokenizer(
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:08 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:08 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:12 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2244688)[0;0m WARNING 01-19 00:34:12 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:12 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:13 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:13 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:13 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2244688)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2244688)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.63s/it]
[1;36m(EngineCore_DP0 pid=2244688)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]
[1;36m(EngineCore_DP0 pid=2244688)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.28s/it]
[1;36m(EngineCore_DP0 pid=2244688)[0;0m 
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:16 [default_loader.py:267] Loading weights took 2.62 seconds
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:17 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 3.411357 seconds
[codecarbon INFO @ 00:34:19] Energy consumed for RAM : 0.000302 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:34:20] Delta energy consumed for CPU with cpu_load : 0.000129 kWh, power : 30.004634154000005 W
[codecarbon INFO @ 00:34:20] Energy consumed for All CPU : 0.000129 kWh
[codecarbon INFO @ 00:34:20] Energy consumed for all GPUs : 0.000124 kWh. Total GPU Power : 27.95383920131882 W
[codecarbon INFO @ 00:34:20] 0.000556 kWh of electricity and 0.000000 L of water were used since the beginning.
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:21 [gpu_worker.py:298] Available KV cache memory: 5.03 GiB
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:21 [kv_cache_utils.py:1087] GPU KV cache size: 41,152 tokens
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:21 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 2.51x
[1;36m(EngineCore_DP0 pid=2244688)[0;0m WARNING 01-19 00:34:21 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:21 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.32 seconds
[1;36m(EngineCore_DP0 pid=2244688)[0;0m INFO 01-19 00:34:22 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 01-19 00:34:22 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 1286
INFO 01-19 00:34:22 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1372.60it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:03<05:04,  3.07s/it, est. speed input: 50.77 toks/s, output: 0.33 toks/s]Processed prompts:   2%|â–         | 2/100 [00:05<04:32,  2.78s/it, est. speed input: 42.11 toks/s, output: 0.35 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:05<01:01,  1.52it/s, est. speed input: 332.03 toks/s, output: 2.26 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:05<00:35,  2.58it/s, est. speed input: 456.93 toks/s, output: 4.21 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:06<00:14,  5.86it/s, est. speed input: 772.87 toks/s, output: 9.80 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:06<00:08,  9.01it/s, est. speed input: 1022.92 toks/s, output: 16.66 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:06<00:06, 10.90it/s, est. speed input: 1175.32 toks/s, output: 21.91 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:05, 11.92it/s, est. speed input: 1327.76 toks/s, output: 26.45 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:05, 11.86it/s, est. speed input: 1335.19 toks/s, output: 31.97 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:07<00:04, 13.97it/s, est. speed input: 1465.66 toks/s, output: 42.72 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:07<00:03, 16.02it/s, est. speed input: 1543.88 toks/s, output: 52.40 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:05,  9.42it/s, est. speed input: 1565.71 toks/s, output: 56.37 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:08<00:05,  9.19it/s, est. speed input: 1525.25 toks/s, output: 62.09 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:05,  9.73it/s, est. speed input: 1543.40 toks/s, output: 69.07 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:05,  8.63it/s, est. speed input: 1498.34 toks/s, output: 75.22 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:09<00:06,  7.90it/s, est. speed input: 1465.29 toks/s, output: 81.94 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:09<00:06,  7.28it/s, est. speed input: 1409.95 toks/s, output: 91.90 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:09<00:06,  7.14it/s, est. speed input: 1390.79 toks/s, output: 95.56 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:09<00:06,  6.40it/s, est. speed input: 1359.10 toks/s, output: 98.73 toks/s]INFO 01-19 00:34:32 [loggers.py:127] Engine 000: Avg prompt throughput: 2205.0 tokens/s, Avg generation throughput: 324.0 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 28.9%, Prefix cache hit rate: 0.0%
Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:10<00:06,  5.92it/s, est. speed input: 1337.41 toks/s, output: 105.93 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:10<00:07,  5.28it/s, est. speed input: 1290.49 toks/s, output: 112.74 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:11<00:10,  3.71it/s, est. speed input: 1227.68 toks/s, output: 112.97 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:11<00:09,  3.82it/s, est. speed input: 1227.63 toks/s, output: 117.14 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:11<00:07,  4.52it/s, est. speed input: 1271.85 toks/s, output: 127.24 toks/s][codecarbon INFO @ 00:34:34] Energy consumed for RAM : 0.000583 kWh. RAM Power : 70.0 W
Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:12<00:07,  4.48it/s, est. speed input: 1255.91 toks/s, output: 131.65 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:12<00:07,  4.14it/s, est. speed input: 1257.72 toks/s, output: 135.35 toks/s][codecarbon INFO @ 00:34:35] Delta energy consumed for CPU with cpu_load : 0.000121 kWh, power : 30.003527263125005 W
[codecarbon INFO @ 00:34:35] Energy consumed for All CPU : 0.000250 kWh
[codecarbon INFO @ 00:34:35] Energy consumed for all GPUs : 0.000405 kWh. Total GPU Power : 67.55289288260329 W
[codecarbon INFO @ 00:34:35] 0.001239 kWh of electricity and 0.000000 L of water were used since the beginning.
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:12<00:03,  7.36it/s, est. speed input: 1262.62 toks/s, output: 160.45 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:13<00:04,  6.68it/s, est. speed input: 1296.15 toks/s, output: 169.92 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:03,  7.85it/s, est. speed input: 1296.67 toks/s, output: 182.61 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:03,  6.42it/s, est. speed input: 1303.92 toks/s, output: 186.06 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:13<00:03,  6.58it/s, est. speed input: 1306.76 toks/s, output: 197.01 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:14<00:02,  6.70it/s, est. speed input: 1305.67 toks/s, output: 208.16 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:14<00:03,  5.91it/s, est. speed input: 1341.70 toks/s, output: 217.47 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:15<00:04,  3.88it/s, est. speed input: 1267.56 toks/s, output: 220.32 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:15<00:04,  3.55it/s, est. speed input: 1240.70 toks/s, output: 223.28 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:17<00:05,  2.39it/s, est. speed input: 1180.75 toks/s, output: 225.12 toks/s]INFO 01-19 00:34:42 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 302.8 tokens/s, Running: 12 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.8%, Prefix cache hit rate: 0.0%
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:20<00:08,  1.28it/s, est. speed input: 1038.60 toks/s, output: 206.74 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:21<00:08,  1.15it/s, est. speed input: 982.92 toks/s, output: 205.44 toks/s] Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:23<00:10,  1.16s/it, est. speed input: 893.31 toks/s, output: 197.03 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.16s/it, est. speed input: 958.29 toks/s, output: 294.01 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.23it/s, est. speed input: 958.29 toks/s, output: 294.01 toks/s]
[codecarbon INFO @ 00:34:46] Energy consumed for RAM : 0.000797 kWh. RAM Power : 70.0 W
[codecarbon INFO @ 00:34:46] Delta energy consumed for CPU with cpu_load : 0.000092 kWh, power : 30.003066639230774 W
[codecarbon INFO @ 00:34:46] Energy consumed for All CPU : 0.000342 kWh
[codecarbon INFO @ 00:34:46] Energy consumed for all GPUs : 0.000629 kWh. Total GPU Power : 70.08685566140822 W
[codecarbon INFO @ 00:34:46] 0.001768 kWh of electricity and 0.000000 L of water were used since the beginning.
[codecarbon WARNING @ 00:34:46] The CSV format has changed, backing up old emission file.
EmissionsData(timestamp='2026-01-19T00:34:46', project_name='codecarbon', run_id='fef3d0ac-cde7-45a7-8c1e-899ea2bbd5cb', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=42.578189046122134, emissions=0.0004201743081810824, emissions_rate=9.868299183084922e-06, cpu_power=30.003066639230774, gpu_power=70.08685566140822, ram_power=70.0, cpu_energy=0.00034174624679467376, gpu_energy=0.0006294802258075549, ram_energy=0.0007972657900070772, energy_consumed=0.001768492262609306, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)

Results:
  Model loading time: 17.82s
  Throughput: 293.08 tokens/sec
  Energy per token: 0.92 J/token
  Requests/sec: 4.22
  Total time: 23.71s
  Emissions: 0.000420 kg CO2
  Total energy consumed: 0.001768 kWh

Results saved to: results/run_20260119_003446/results.json
Run 5 completed successfully
All runs completed!
