INFO 12-28 00:33:00 [__init__.py:216] Automatically detected platform cuda.
[WARNING 12-28 00:33:04] ax.service.utils.instantiation: Objective thresholds were not set. They will be selected using a heuristic, but should be specified on the objective for best performance.
[INFO 12-28 00:33:04] ax.generation_strategy.dispatch_utils: Using Generators.BOTORCH_MODULAR since there are more ordered parameters than there are categories for the unordered categorical parameters.
[INFO 12-28 00:33:04] ax.generation_strategy.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 16 trials, BoTorch for subsequent trials]). Iterations after 16 will take longer to generate due to model-fitting.
[INFO 12-28 00:33:04] ax.service.ax_client: Generated new trial 0 with parameters {'block_size': 32, 'max_num_seqs': 128, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 2, 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model Sobol.
[codecarbon INFO @ 00:33:04] offline tracker init
[codecarbon WARNING @ 00:33:04] Multiple instances of codecarbon are allowed to run at the same time.
Loading Open-Orca/OpenOrca dataset...
Loaded 1000 prompts from dataset
======================================================================
Starting Bayesian Optimization: 10 random + 30 BO trials.
INFO 12-28 00:33:06 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:33:07 [model.py:547] Resolved architecture: MistralForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
INFO 12-28 00:33:07 [model.py:1510] Using max model len 32768
INFO 12-28 00:33:09 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:33:09 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:09 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:09 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:09 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_bbc9157c'), local_subscribe_addr='ipc:///tmp/b5b8f53a-2ea8-407d-b28a-54c68999ed4e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_22af610b'), local_subscribe_addr='ipc:///tmp/e487a5ea-769e-432f-8e27-765dcca49d6c', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c0f19a46'), local_subscribe_addr='ipc:///tmp/4df875e3-3d30-41a4-8654-ef5a3499ce40', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2b590dfa'), local_subscribe_addr='ipc:///tmp/2af5d481-3c26-4f1f-8a26-50e1de89b475', remote_subscribe_addr=None, remote_addr_ipv6=False)
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_69c86ba6'), local_subscribe_addr='ipc:///tmp/61561ee4-e2a2-4bfa-b22f-05a81e8d07d9', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:14 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:14 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_16e931ec'), local_subscribe_addr='ipc:///tmp/7739e06d-cf3c-4bfb-80a4-2beaa7cb6b5e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:14 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:14 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_b337716f'), local_subscribe_addr='ipc:///tmp/5747aeab-f158-458d-8f5a-56e6d30937cc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [__init__.py:1384] Found nccl from library libnccl.so.2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [pynccl.py:103] vLLM is using nccl==2.27.3
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:14 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m WARNING 12-28 00:33:15 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m INFO 12-28 00:33:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP1 pid=2855641)[0;0m INFO 12-28 00:33:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP0 pid=2855639)[0;0m INFO 12-28 00:33:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m INFO 12-28 00:33:15 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP0 pid=2855639)[0;0m INFO 12-28 00:33:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m INFO 12-28 00:33:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m INFO 12-28 00:33:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP1 pid=2855641)[0;0m INFO 12-28 00:33:15 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP0 pid=2855639)[0;0m INFO 12-28 00:33:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m INFO 12-28 00:33:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m INFO 12-28 00:33:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP1 pid=2855641)[0;0m INFO 12-28 00:33:15 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP0 pid=2855639)[0;0m INFO 12-28 00:33:15 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m INFO 12-28 00:33:15 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP1 pid=2855641)[0;0m INFO 12-28 00:33:15 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m INFO 12-28 00:33:15 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP0 pid=2855639)[0;0m INFO 12-28 00:33:17 [default_loader.py:267] Loading weights took 1.15 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.09s/it]
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.73it/s]
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m 
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m INFO 12-28 00:33:17 [default_loader.py:267] Loading weights took 1.16 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP1 pid=2855641)[0;0m INFO 12-28 00:33:17 [default_loader.py:267] Loading weights took 1.17 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP0 pid=2855639)[0;0m INFO 12-28 00:33:18 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 1.771015 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m INFO 12-28 00:33:18 [default_loader.py:267] Loading weights took 1.15 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m INFO 12-28 00:33:18 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.086104 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP1 pid=2855641)[0;0m INFO 12-28 00:33:18 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.336908 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m INFO 12-28 00:33:18 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.613677 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP0 pid=2855639)[0;0m INFO 12-28 00:33:21 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP1 pid=2855641)[0;0m INFO 12-28 00:33:21 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m INFO 12-28 00:33:22 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m INFO 12-28 00:33:22 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [kv_cache_utils.py:1087] GPU KV cache size: 512,320 tokens
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.21x
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [kv_cache_utils.py:1087] GPU KV cache size: 512,320 tokens
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.21x
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [kv_cache_utils.py:1087] GPU KV cache size: 508,672 tokens
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.99x
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [kv_cache_utils.py:1087] GPU KV cache size: 508,672 tokens
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.99x
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP1 pid=2855641)[0;0m WARNING 12-28 00:33:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP1_TP0 pid=2855639)[0;0m WARNING 12-28 00:33:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.47 seconds
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=2855628)[0;0m INFO 12-28 00:33:23 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:33:24 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m WARNING 12-28 00:33:24 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m WARNING 12-28 00:33:24 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1380.72it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m 
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP0 pid=2855635)[0;0m 
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m 
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(EngineCore_DP0 pid=2855628)[0;0m [1;36m(Worker_PP0_TP1 pid=2855637)[0;0m 
Processed prompts:   1%|          | 1/100 [00:02<03:27,  2.09s/it, est. speed input: 74.60 toks/s, output: 0.48 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:02<01:20,  1.20it/s, est. speed input: 209.62 toks/s, output: 1.04 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:02<00:19,  4.70it/s, est. speed input: 961.67 toks/s, output: 6.71 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 15/100 [00:03<00:09,  8.88it/s, est. speed input: 1269.43 toks/s, output: 15.15 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:03<00:05, 14.65it/s, est. speed input: 2126.28 toks/s, output: 29.48 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:03<00:03, 19.71it/s, est. speed input: 2446.42 toks/s, output: 46.07 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:03<00:03, 22.10it/s, est. speed input: 2670.77 toks/s, output: 61.62 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:03<00:03, 18.08it/s, est. speed input: 2673.08 toks/s, output: 78.48 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:04<00:02, 20.63it/s, est. speed input: 2799.86 toks/s, output: 100.92 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:04<00:03, 17.67it/s, est. speed input: 2755.42 toks/s, output: 121.30 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:04<00:03, 13.89it/s, est. speed input: 2651.79 toks/s, output: 135.37 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:04<00:03, 14.16it/s, est. speed input: 2589.68 toks/s, output: 156.40 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:05<00:03, 14.35it/s, est. speed input: 2625.41 toks/s, output: 171.37 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:05<00:02, 18.21it/s, est. speed input: 2663.88 toks/s, output: 206.81 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:05<00:03, 11.03it/s, est. speed input: 2610.01 toks/s, output: 217.75 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:06<00:04,  8.03it/s, est. speed input: 2419.20 toks/s, output: 223.32 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:06<00:04,  7.84it/s, est. speed input: 2483.09 toks/s, output: 238.80 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:06<00:04,  7.56it/s, est. speed input: 2394.10 toks/s, output: 254.41 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:07<00:03,  8.77it/s, est. speed input: 2515.29 toks/s, output: 284.95 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:07<00:03,  7.07it/s, est. speed input: 2396.34 toks/s, output: 295.22 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:07<00:03,  8.09it/s, est. speed input: 2387.50 toks/s, output: 327.67 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:07<00:02,  9.33it/s, est. speed input: 2468.26 toks/s, output: 352.42 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:07<00:01, 12.02it/s, est. speed input: 2478.89 toks/s, output: 392.17 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:08<00:01, 12.67it/s, est. speed input: 2455.92 toks/s, output: 415.78 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:08<00:01,  8.03it/s, est. speed input: 2325.81 toks/s, output: 421.53 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:08<00:01,  8.27it/s, est. speed input: 2288.78 toks/s, output: 443.06 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:08<00:01,  9.52it/s, est. speed input: 2269.00 toks/s, output: 469.20 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:10<00:02,  4.23it/s, est. speed input: 2050.41 toks/s, output: 449.30 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:11<00:03,  2.66it/s, est. speed input: 1861.74 toks/s, output: 425.14 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:11<00:03,  2.36it/s, est. speed input: 1769.60 toks/s, output: 421.69 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:12<00:04,  1.70it/s, est. speed input: 1612.47 toks/s, output: 402.90 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  1.70it/s, est. speed input: 1746.23 toks/s, output: 540.55 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  7.71it/s, est. speed input: 1746.23 toks/s, output: 540.55 toks/s]
[rank3]:[W1228 00:33:38.011615234 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=168, addr=[localhost]:33808, remote=[localhost]:46465): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:682 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x709effed9eb0 in /home/wyn23/r244/venv/lib/python3.12/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x5d694d1 (0x709ee3fef4d1 in /home/wyn23/r244/venv/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x5d6a8cd (0x709ee3ff08cd in /home/wyn23/r244/venv/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #3: <unknown function> + 0x5d6b47a (0x709ee3ff147a in /home/wyn23/r244/venv/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x31e (0x709ee3fec19e in /home/wyn23/r244/venv/lib/python3.12/site-packages/torch/lib/libtorch_cpu.so)
frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x398 (0x709ea34d1b18 in /home/wyn23/r244/venv/lib/python3.12/site-packages/torch/lib/libtorch_cuda.so)
frame #6: <unknown function> + 0xecdb4 (0x709e86e01db4 in /lib/x86_64-linux-gnu/libstdc++.so.6)
frame #7: <unknown function> + 0x9caa4 (0x709f01069aa4 in /lib/x86_64-linux-gnu/libc.so.6)
frame #8: <unknown function> + 0x129c6c (0x709f010f6c6c in /lib/x86_64-linux-gnu/libc.so.6)

[rank3]:[W1228 00:33:38.018984214 ProcessGroupNCCL.cpp:1783] [PG ID 0 PG GUID 0 Rank 3] Failed to check the "should dump" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?
[INFO 12-28 00:33:41] ax.service.ax_client: Completed trial 0 with data: {'throughput(token/s)': 537.443621, 'energy(J/token)': 1.399473}.
[INFO 12-28 00:33:41] ax.service.ax_client: Generated new trial 1 with parameters {'block_size': 128, 'max_num_seqs': 128, 'max_num_batched_tokens': 8192, 'tensor_parallel_size': 1, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:33:37', project_name='codecarbon', run_id='811ab150-5cad-4ff7-b630-fc0f352a0ea6', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=31.461902220034972, emissions=0.0006475427620372953, emissions_rate=2.058180581417418e-05, cpu_power=31.25751501, gpu_power=114.09451969256868, ram_power=70.0, cpu_energy=0.0003811564437017894, gpu_energy=0.0014714581216095368, ram_energy=0.000872859939136672, energy_consumed=0.002725474504447998, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 1/10 completed. Throughput: 537.44, Energy: (1.3994734297550697, 0.0)
INFO 12-28 00:33:43 [utils.py:233] non-default args: {'seed': 42, 'block_size': 128, 'enable_prefix_caching': True, 'max_num_batched_tokens': 8192, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:33:43 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:33:43 [model.py:1510] Using max model len 32768
INFO 12-28 00:33:43 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-28 00:33:43 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

WARNING 12-28 00:33:44 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
INFO 12-28 00:33:52 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:33:56 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:33:56 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:33:58 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2857258)[0;0m WARNING 12-28 00:33:59 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:33:59 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:33:59 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:33:59 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:33:59 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2857258)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2857258)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.30s/it]
[1;36m(EngineCore_DP0 pid=2857258)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]
[1;36m(EngineCore_DP0 pid=2857258)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.02it/s]
[1;36m(EngineCore_DP0 pid=2857258)[0;0m 
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:34:02 [default_loader.py:267] Loading weights took 2.00 seconds
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:34:02 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.655088 seconds
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:34:05 [gpu_worker.py:298] Available KV cache memory: 5.45 GiB
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:34:05 [kv_cache_utils.py:1087] GPU KV cache size: 44,544 tokens
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:34:05 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 3.59x
[1;36m(EngineCore_DP0 pid=2857258)[0;0m WARNING 12-28 00:34:05 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:34:05 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.23 seconds
[1;36m(EngineCore_DP0 pid=2857258)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2857258)[0;0m 
[1;36m(EngineCore_DP0 pid=2857258)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2857258)[0;0m 
[1;36m(EngineCore_DP0 pid=2857258)[0;0m INFO 12-28 00:34:06 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:34:06 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1030.95it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:02<03:26,  2.09s/it, est. speed input: 74.79 toks/s, output: 0.48 toks/s]Processed prompts:   2%|â–         | 2/100 [00:04<03:25,  2.10s/it, est. speed input: 51.08 toks/s, output: 0.72 toks/s]Processed prompts:   4%|â–         | 4/100 [00:05<02:04,  1.29s/it, est. speed input: 176.80 toks/s, output: 1.20 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:06<00:38,  2.35it/s, est. speed input: 468.08 toks/s, output: 3.33 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:06<00:16,  5.15it/s, est. speed input: 688.10 toks/s, output: 8.55 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:06<00:09,  8.48it/s, est. speed input: 1082.95 toks/s, output: 15.99 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:06<00:07, 10.20it/s, est. speed input: 1152.86 toks/s, output: 21.04 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:06<00:05, 12.73it/s, est. speed input: 1371.13 toks/s, output: 31.42 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:07<00:04, 13.45it/s, est. speed input: 1405.39 toks/s, output: 37.44 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:07<00:03, 17.43it/s, est. speed input: 1460.39 toks/s, output: 50.68 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:07<00:05, 10.50it/s, est. speed input: 1396.05 toks/s, output: 54.72 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:05,  9.46it/s, est. speed input: 1460.98 toks/s, output: 59.73 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:06,  8.12it/s, est. speed input: 1465.55 toks/s, output: 65.19 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:08<00:05,  8.78it/s, est. speed input: 1447.24 toks/s, output: 72.50 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:09<00:04,  9.73it/s, est. speed input: 1437.64 toks/s, output: 83.72 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:09<00:06,  7.04it/s, est. speed input: 1370.35 toks/s, output: 88.52 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:10<00:05,  7.17it/s, est. speed input: 1441.22 toks/s, output: 103.88 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:10<00:05,  6.63it/s, est. speed input: 1414.48 toks/s, output: 107.14 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:10<00:06,  6.13it/s, est. speed input: 1392.71 toks/s, output: 110.63 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:11<00:09,  3.89it/s, est. speed input: 1332.04 toks/s, output: 110.02 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:11<00:11,  3.04it/s, est. speed input: 1269.09 toks/s, output: 110.89 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:12<00:09,  3.45it/s, est. speed input: 1260.28 toks/s, output: 116.03 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:12<00:10,  3.21it/s, est. speed input: 1224.49 toks/s, output: 119.32 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:12<00:08,  3.70it/s, est. speed input: 1210.59 toks/s, output: 124.70 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:13<00:08,  3.34it/s, est. speed input: 1205.54 toks/s, output: 132.10 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:13<00:07,  3.78it/s, est. speed input: 1200.95 toks/s, output: 137.92 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:13<00:07,  3.93it/s, est. speed input: 1258.01 toks/s, output: 142.96 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:14<00:08,  3.09it/s, est. speed input: 1271.67 toks/s, output: 145.27 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:14<00:09,  2.81it/s, est. speed input: 1302.51 toks/s, output: 148.76 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:14<00:05,  4.32it/s, est. speed input: 1294.52 toks/s, output: 162.80 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:15<00:06,  3.37it/s, est. speed input: 1255.73 toks/s, output: 165.53 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:15<00:06,  3.62it/s, est. speed input: 1242.63 toks/s, output: 171.35 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:15<00:05,  4.15it/s, est. speed input: 1235.89 toks/s, output: 177.90 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:16<00:04,  3.85it/s, est. speed input: 1205.40 toks/s, output: 187.93 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:16<00:04,  3.72it/s, est. speed input: 1177.68 toks/s, output: 198.35 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:17<00:06,  2.29it/s, est. speed input: 1111.38 toks/s, output: 195.58 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:18<00:03,  3.86it/s, est. speed input: 1107.32 toks/s, output: 220.21 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:18<00:03,  3.40it/s, est. speed input: 1084.09 toks/s, output: 223.93 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:18<00:02,  3.85it/s, est. speed input: 1077.12 toks/s, output: 231.52 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:19<00:03,  2.66it/s, est. speed input: 1039.00 toks/s, output: 231.87 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:20<00:03,  2.10it/s, est. speed input: 997.31 toks/s, output: 236.41 toks/s] Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:20<00:03,  2.27it/s, est. speed input: 999.55 toks/s, output: 242.81 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:23<00:06,  1.04s/it, est. speed input: 889.41 toks/s, output: 223.73 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  2.83it/s, est. speed input: 943.92 toks/s, output: 286.65 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  2.83it/s, est. speed input: 943.92 toks/s, output: 286.65 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.17it/s, est. speed input: 943.92 toks/s, output: 286.65 toks/s]
[rank0]:[W1228 00:34:31.242392465 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[INFO 12-28 00:34:34] ax.service.ax_client: Completed trial 1 with data: {'throughput(token/s)': 285.474605, 'energy(J/token)': 1.037509}.
[INFO 12-28 00:34:34] ax.service.ax_client: Generated new trial 2 with parameters {'block_size': 64, 'max_num_seqs': 64, 'max_num_batched_tokens': 8192, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'bfloat16', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:34:31', project_name='codecarbon', run_id='25ab3247-c000-4856-8b52-c65975b53a82', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=48.44501309201587, emissions=0.00047095342213106093, emissions_rate=9.721401483296902e-06, cpu_power=30.002450520000004, gpu_power=64.33122585691349, ram_power=70.0, cpu_energy=0.0003878346769335994, gpu_energy=0.0006935899993152361, ram_energy=0.0009007942906690814, energy_consumed=0.001982218966917917, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 2/10 completed. Throughput: 285.47, Energy: (1.0375092004804451, 0.0)
INFO 12-28 00:34:36 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'tensor_parallel_size': 4, 'block_size': 64, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8192, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:34:36 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:34:36 [model.py:1510] Using max model len 32768
INFO 12-28 00:34:36 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-28 00:34:36 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:34:47 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:34:51 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:34:51 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2858560)[0;0m WARNING 12-28 00:34:51 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:34:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_eb49b91f'), local_subscribe_addr='ipc:///tmp/97e4111a-1722-49a2-b871-830d42bcdc4d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:34:59 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:35:00 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:35:00 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:35:00 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:35:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_13952ba9'), local_subscribe_addr='ipc:///tmp/12afb2e4-9311-4d63-b49d-9a7b00857228', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:35:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f0909da4'), local_subscribe_addr='ipc:///tmp/5112ac51-f2ad-436f-b0e8-fd5cbbb24e6c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:35:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b798b016'), local_subscribe_addr='ipc:///tmp/64d24db0-0883-4fa7-a3d7-7b03cd7dca15', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:35:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_720d9a75'), local_subscribe_addr='ipc:///tmp/988f6c36-269f-488b-9feb-38c4d0ad46f6', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:35:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:35:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:35:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:35:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:35:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:35:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:35:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:35:07 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:35:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:35:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:35:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:35:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:35:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_00dfb1e4'), local_subscribe_addr='ipc:///tmp/cbd881e9-2c91-4fd2-93bc-138b16f48dfb', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:35:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:35:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:35:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:35:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:35:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:35:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:35:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:35:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:35:08 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:35:08 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:35:08 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:35:08 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 00:35:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:35:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:35:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:35:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2859857)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:10 [default_loader.py:267] Loading weights took 0.96 seconds
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:10 [default_loader.py:267] Loading weights took 0.88 seconds
[1;36m(Worker_TP0 pid=2859857)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.76it/s]
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:10 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP0 pid=2859857)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.40it/s]
[1;36m(Worker_TP0 pid=2859857)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.28it/s]
[1;36m(Worker_TP0 pid=2859857)[0;0m 
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:11 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.643559 seconds
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:11 [default_loader.py:267] Loading weights took 0.91 seconds
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:11 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.757496 seconds
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:11 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.024555 seconds
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:11 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.313464 seconds
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:16 [gpu_worker.py:298] Available KV cache memory: 16.02 GiB
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:16 [gpu_worker.py:298] Available KV cache memory: 16.02 GiB
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:16 [gpu_worker.py:298] Available KV cache memory: 16.02 GiB
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:16 [gpu_worker.py:298] Available KV cache memory: 16.02 GiB
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [kv_cache_utils.py:1087] GPU KV cache size: 524,992 tokens
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 42.50x
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [kv_cache_utils.py:1087] GPU KV cache size: 524,992 tokens
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 42.50x
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [kv_cache_utils.py:1087] GPU KV cache size: 524,992 tokens
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 42.50x
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [kv_cache_utils.py:1087] GPU KV cache size: 524,992 tokens
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 42.50x
[1;36m(Worker_TP3 pid=2859860)[0;0m WARNING 12-28 00:35:16 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2859859)[0;0m WARNING 12-28 00:35:16 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2859858)[0;0m WARNING 12-28 00:35:16 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2859857)[0;0m WARNING 12-28 00:35:16 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:16 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.94 seconds
[1;36m(EngineCore_DP0 pid=2858560)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2858560)[0;0m 
[1;36m(EngineCore_DP0 pid=2858560)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2858560)[0;0m 
[1;36m(EngineCore_DP0 pid=2858560)[0;0m INFO 12-28 00:35:17 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:35:17 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1407.70it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<01:26,  1.15it/s, est. speed input: 178.67 toks/s, output: 1.15 toks/s]Processed prompts:   2%|â–         | 2/100 [00:01<01:15,  1.29it/s, est. speed input: 135.50 toks/s, output: 1.90 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:01<00:23,  3.96it/s, est. speed input: 555.09 toks/s, output: 6.41 toks/s]Processed prompts:  10%|â–ˆ         | 10/100 [00:01<00:10,  8.63it/s, est. speed input: 1091.71 toks/s, output: 18.49 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 15/100 [00:02<00:06, 12.70it/s, est. speed input: 1809.38 toks/s, output: 33.16 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:05, 14.58it/s, est. speed input: 2418.77 toks/s, output: 43.40 toks/s]WARNING 12-28 00:35:20 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:02<00:03, 20.75it/s, est. speed input: 2844.83 toks/s, output: 66.73 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:02<00:03, 22.49it/s, est. speed input: 3376.14 toks/s, output: 85.02 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:02<00:03, 21.93it/s, est. speed input: 3446.60 toks/s, output: 91.50 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:02<00:02, 25.30it/s, est. speed input: 3523.71 toks/s, output: 104.96 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:03<00:02, 20.52it/s, est. speed input: 3548.79 toks/s, output: 108.87 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:03<00:02, 27.13it/s, est. speed input: 3849.59 toks/s, output: 146.72 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:03<00:01, 28.80it/s, est. speed input: 3773.03 toks/s, output: 168.14 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:03<00:01, 32.44it/s, est. speed input: 4067.51 toks/s, output: 210.99 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:03<00:01, 28.01it/s, est. speed input: 4057.72 toks/s, output: 243.81 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:03<00:01, 25.56it/s, est. speed input: 4258.44 toks/s, output: 279.57 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:02, 15.87it/s, est. speed input: 3879.19 toks/s, output: 292.05 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 12.16it/s, est. speed input: 3570.91 toks/s, output: 314.50 toks/s]WARNING 12-28 00:35:22 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:04<00:02, 12.72it/s, est. speed input: 3562.30 toks/s, output: 342.00 toks/s]WARNING 12-28 00:35:22 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02,  9.53it/s, est. speed input: 3486.11 toks/s, output: 353.63 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:05<00:02, 10.24it/s, est. speed input: 3406.21 toks/s, output: 381.20 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:05<00:02, 10.93it/s, est. speed input: 3337.29 toks/s, output: 412.23 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:05<00:01, 12.05it/s, est. speed input: 3356.57 toks/s, output: 442.54 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:05<00:01, 13.83it/s, est. speed input: 3485.91 toks/s, output: 490.40 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:05<00:00, 17.18it/s, est. speed input: 3463.73 toks/s, output: 565.50 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:05<00:00, 18.77it/s, est. speed input: 3402.15 toks/s, output: 622.12 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:06<00:00, 20.19it/s, est. speed input: 3495.95 toks/s, output: 675.31 toks/s]WARNING 12-28 00:35:23 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 00:35:23 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:06<00:00,  9.17it/s, est. speed input: 3205.05 toks/s, output: 677.35 toks/s]WARNING 12-28 00:35:24 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
WARNING 12-28 00:35:25 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:08<00:00,  4.29it/s, est. speed input: 2698.47 toks/s, output: 629.79 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  5.48it/s, est. speed input: 2637.32 toks/s, output: 718.80 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  5.48it/s, est. speed input: 2637.32 toks/s, output: 718.80 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.64it/s, est. speed input: 2637.32 toks/s, output: 718.80 toks/s]
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:26 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2859857)[0;0m INFO 12-28 00:35:26 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:26 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:26 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2859858)[0;0m INFO 12-28 00:35:26 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2859859)[0;0m INFO 12-28 00:35:26 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:26 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2859860)[0;0m INFO 12-28 00:35:26 [multiproc_executor.py:599] WorkerProc shutting down.
[INFO 12-28 00:35:32] ax.service.ax_client: Completed trial 2 with data: {'throughput(token/s)': 712.761874, 'energy(J/token)': 1.791104}.
[INFO 12-28 00:35:32] ax.service.ax_client: Generated new trial 3 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 4096, 'tensor_parallel_size': 1, 'pipeline_parallel_size': 4, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'bfloat16', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:35:26', project_name='codecarbon', run_id='e0e98889-1b61-4e1c-9d81-3323f579ec3b', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=51.169589512981474, emissions=0.0007296943831205783, emissions_rate=1.4260313402269103e-05, cpu_power=30.239360978571437, gpu_power=264.89931968852824, ram_power=70.0, cpu_energy=0.0004107727105772281, gpu_energy=0.0017065119207630985, ram_energy=0.0009539618620603707, energy_consumed=0.003071246493400697, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 3/10 completed. Throughput: 712.76, Energy: (1.7911043862372444, 0.0)
INFO 12-28 00:35:34 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'pipeline_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 4096, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:35:34 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:35:34 [model.py:1510] Using max model len 32768
INFO 12-28 00:35:34 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=4096.
INFO 12-28 00:35:34 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:35:44 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:35:48 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:35:48 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=4, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2862435)[0;0m WARNING 12-28 00:35:48 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:35:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_44d308c9'), local_subscribe_addr='ipc:///tmp/e09e9309-adc2-47b5-a1ef-72bfe2e17ed0', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:35:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:35:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:35:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:35:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:36:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_86273e82'), local_subscribe_addr='ipc:///tmp/1e3a285f-6138-4f50-9975-923df5f57314', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:36:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8c041a54'), local_subscribe_addr='ipc:///tmp/fcdaaa0d-0496-43b2-aedf-a57f16aec293', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:36:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3e204299'), local_subscribe_addr='ipc:///tmp/698ae8fb-a944-41b9-8c8a-c4f9173b9063', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:36:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_432523a5'), local_subscribe_addr='ipc:///tmp/597f7885-6d56-4de1-8634-be27d46eab81', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:36:04 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:36:04 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:36:04 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:36:04 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:36:04 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:36:04 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:36:04 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:36:04 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 00:36:05 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 00:36:05 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 2, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 00:36:05 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 00:36:05 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 3, TP rank 0, EP rank 0
WARNING 12-28 00:36:05 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:36:05 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:36:05 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:36:05 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:05 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP3 pid=2862694)[0;0m INFO 12-28 00:36:05 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:05 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:05 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:05 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:05 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:05 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:05 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP3 pid=2862694)[0;0m INFO 12-28 00:36:05 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:05 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:05 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP3 pid=2862694)[0;0m INFO 12-28 00:36:05 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:06 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:06 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:06 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP3 pid=2862694)[0;0m INFO 12-28 00:36:06 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=2862691)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:06 [default_loader.py:267] Loading weights took 0.45 seconds
[1;36m(Worker_PP0 pid=2862691)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.02it/s]
[1;36m(Worker_PP0 pid=2862691)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.67it/s]
[1;36m(Worker_PP0 pid=2862691)[0;0m 
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:07 [default_loader.py:267] Loading weights took 0.55 seconds
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:07 [default_loader.py:267] Loading weights took 0.51 seconds
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:07 [gpu_model_runner.py:2653] Model loading took 3.2580 GiB and 1.000032 seconds
[1;36m(Worker_PP3 pid=2862694)[0;0m INFO 12-28 00:36:07 [default_loader.py:267] Loading weights took 0.54 seconds
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:07 [gpu_model_runner.py:2653] Model loading took 3.5021 GiB and 1.282099 seconds
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:07 [gpu_model_runner.py:2653] Model loading took 3.2580 GiB and 1.500602 seconds
[1;36m(Worker_PP3 pid=2862694)[0;0m INFO 12-28 00:36:08 [gpu_model_runner.py:2653] Model loading took 3.5021 GiB and 1.773328 seconds
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:09 [gpu_worker.py:298] Available KV cache memory: 16.08 GiB
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:09 [gpu_worker.py:298] Available KV cache memory: 16.08 GiB
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:09 [gpu_worker.py:298] Available KV cache memory: 15.87 GiB
[1;36m(Worker_PP3 pid=2862694)[0;0m INFO 12-28 00:36:09 [gpu_worker.py:298] Available KV cache memory: 15.83 GiB
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:09 [kv_cache_utils.py:1087] GPU KV cache size: 519,872 tokens
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 63.21x
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:09 [kv_cache_utils.py:1087] GPU KV cache size: 526,848 tokens
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.06x
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:09 [kv_cache_utils.py:1087] GPU KV cache size: 526,848 tokens
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.06x
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:09 [kv_cache_utils.py:1087] GPU KV cache size: 518,784 tokens
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:09 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 63.08x
[1;36m(Worker_PP3 pid=2862694)[0;0m WARNING 12-28 00:36:09 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:10 [core.py:210] init engine (profile, create kv cache, warmup model) took 1.79 seconds
[1;36m(EngineCore_DP0 pid=2862435)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2862435)[0;0m 
[1;36m(EngineCore_DP0 pid=2862435)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2862435)[0;0m 
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:10 [core.py:149] Batch queue is enabled with size 4
[1;36m(EngineCore_DP0 pid=2862435)[0;0m INFO 12-28 00:36:10 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:36:10 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(Worker_PP0 pid=2862691)[0;0m WARNING 12-28 00:36:10 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=2862691)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0 pid=2862691)[0;0m 
[1;36m(Worker_PP0 pid=2862691)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0 pid=2862691)[0;0m 
[1;36m(Worker_PP1 pid=2862692)[0;0m WARNING 12-28 00:36:10 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1327.02it/s]
[1;36m(Worker_PP1 pid=2862692)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP1 pid=2862692)[0;0m 
[1;36m(Worker_PP1 pid=2862692)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP1 pid=2862692)[0;0m 
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(Worker_PP2 pid=2862693)[0;0m WARNING 12-28 00:36:10 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP2 pid=2862693)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP2 pid=2862693)[0;0m 
[1;36m(Worker_PP2 pid=2862693)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP2 pid=2862693)[0;0m 
Processed prompts:   1%|          | 1/100 [00:01<01:54,  1.16s/it, est. speed input: 134.39 toks/s, output: 0.86 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:02<01:08,  1.42it/s, est. speed input: 433.95 toks/s, output: 1.34 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:02<00:36,  2.62it/s, est. speed input: 572.42 toks/s, output: 2.50 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:02<00:23,  3.94it/s, est. speed input: 893.02 toks/s, output: 4.69 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:02<00:11,  7.57it/s, est. speed input: 1109.38 toks/s, output: 9.68 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:09,  8.90it/s, est. speed input: 1599.52 toks/s, output: 13.52 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 15/100 [00:02<00:09,  9.34it/s, est. speed input: 1679.70 toks/s, output: 17.68 toks/s]Processed prompts:  17%|â–ˆâ–‹        | 17/100 [00:03<00:07, 11.01it/s, est. speed input: 1842.16 toks/s, output: 23.22 toks/s]Processed prompts:  19%|â–ˆâ–‰        | 19/100 [00:03<00:06, 11.83it/s, est. speed input: 1816.41 toks/s, output: 29.02 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:03<00:04, 16.31it/s, est. speed input: 1937.58 toks/s, output: 42.68 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:03<00:05, 14.44it/s, est. speed input: 1983.09 toks/s, output: 52.06 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:03<00:05, 12.03it/s, est. speed input: 2160.58 toks/s, output: 58.94 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:04<00:06, 10.91it/s, est. speed input: 2225.63 toks/s, output: 66.29 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:04<00:05, 11.70it/s, est. speed input: 2293.82 toks/s, output: 75.43 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:04<00:06, 10.98it/s, est. speed input: 2294.01 toks/s, output: 83.50 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:04<00:06,  9.37it/s, est. speed input: 2379.27 toks/s, output: 91.18 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:04<00:05, 11.02it/s, est. speed input: 2332.44 toks/s, output: 107.00 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:05<00:06,  8.50it/s, est. speed input: 2259.99 toks/s, output: 112.96 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:05<00:05, 10.68it/s, est. speed input: 2226.97 toks/s, output: 131.47 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:05<00:05,  9.19it/s, est. speed input: 2143.55 toks/s, output: 139.53 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:05<00:05,  9.97it/s, est. speed input: 2118.67 toks/s, output: 151.55 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:06<00:06,  7.96it/s, est. speed input: 2163.28 toks/s, output: 158.00 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:06<00:06,  7.21it/s, est. speed input: 2134.02 toks/s, output: 166.20 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:06<00:07,  6.59it/s, est. speed input: 2099.97 toks/s, output: 169.69 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:07<00:08,  5.20it/s, est. speed input: 2021.43 toks/s, output: 170.22 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:07<00:11,  3.91it/s, est. speed input: 1931.37 toks/s, output: 168.74 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:07<00:07,  5.98it/s, est. speed input: 1980.63 toks/s, output: 191.56 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:08<00:06,  6.27it/s, est. speed input: 1983.64 toks/s, output: 197.98 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:08<00:07,  5.59it/s, est. speed input: 2025.33 toks/s, output: 201.48 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:08<00:05,  6.75it/s, est. speed input: 2021.23 toks/s, output: 215.76 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:08<00:05,  7.15it/s, est. speed input: 1997.21 toks/s, output: 222.77 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:08<00:04,  7.30it/s, est. speed input: 1960.91 toks/s, output: 235.48 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:09<00:05,  5.95it/s, est. speed input: 1907.00 toks/s, output: 238.24 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:10<00:11,  2.81it/s, est. speed input: 1725.77 toks/s, output: 225.70 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:10<00:11,  2.71it/s, est. speed input: 1670.30 toks/s, output: 227.47 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:11<00:14,  2.12it/s, est. speed input: 1563.78 toks/s, output: 222.80 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:11<00:12,  2.49it/s, est. speed input: 1542.19 toks/s, output: 229.46 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:11<00:09,  3.04it/s, est. speed input: 1525.84 toks/s, output: 237.56 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:11<00:08,  3.17it/s, est. speed input: 1525.89 toks/s, output: 242.84 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:12<00:09,  2.97it/s, est. speed input: 1483.30 toks/s, output: 246.22 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:12<00:08,  3.07it/s, est. speed input: 1451.76 toks/s, output: 251.55 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:12<00:06,  3.70it/s, est. speed input: 1436.60 toks/s, output: 259.90 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:09,  2.54it/s, est. speed input: 1368.72 toks/s, output: 257.98 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:14<00:13,  1.70it/s, est. speed input: 1273.52 toks/s, output: 250.81 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:15<00:18,  1.19it/s, est. speed input: 1168.38 toks/s, output: 240.20 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:16<00:18,  1.12it/s, est. speed input: 1098.90 toks/s, output: 237.47 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:18<00:21,  1.06s/it, est. speed input: 1019.78 toks/s, output: 230.94 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:20<00:27,  1.43s/it, est. speed input: 909.43 toks/s, output: 217.64 toks/s] Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:20<00:00,  1.43s/it, est. speed input: 1088.69 toks/s, output: 450.64 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:20<00:00,  4.81it/s, est. speed input: 1088.69 toks/s, output: 450.64 toks/s]
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:32 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=2862691)[0;0m INFO 12-28 00:36:32 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:32 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1 pid=2862692)[0;0m INFO 12-28 00:36:32 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:32 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP2 pid=2862693)[0;0m INFO 12-28 00:36:32 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP3 pid=2862694)[0;0m INFO 12-28 00:36:32 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:36:37] ax.service.ax_client: Completed trial 3 with data: {'throughput(token/s)': 448.97823, 'energy(J/token)': 1.562663}.
[INFO 12-28 00:36:37] ax.service.ax_client: Generated new trial 4 with parameters {'block_size': 32, 'max_num_seqs': 64, 'max_num_batched_tokens': 8192, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 4, 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'dtype': 'float16', 'data_parallel_size': 1} using model Sobol.
[INFO 12-28 00:36:37] ax.service.ax_client: Completed trial 4 with data: {'throughput(token/s)': 0.0, 'energy(J/token)': 1000000000.0}.
[INFO 12-28 00:36:37] ax.service.ax_client: Generated new trial 5 with parameters {'block_size': 64, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'float16', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:36:32', project_name='codecarbon', run_id='2f22942c-a732-4dfe-a774-134f78755457', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=58.72747400903609, emissions=0.0009668526748962693, emissions_rate=1.6463379214088192e-05, cpu_power=30.050769372857143, gpu_power=278.08919992856994, ram_power=70.0, cpu_energy=0.0004727616518236655, gpu_energy=0.0024958858855956834, ram_energy=0.001100786127002276, energy_consumed=0.004069433664421624, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 4/10 completed. Throughput: 448.98, Energy: (1.5626625271379042, 0.0)
Skipping trial: 8 GPUs required, only 4 available.
   Initial Trial 5/10 completed. Throughput: 0.00, Energy: (1000000000.0, 0.0)
INFO 12-28 00:36:39 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'tensor_parallel_size': 4, 'block_size': 64, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:36:39 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 00:36:39 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 00:36:39 [model.py:1510] Using max model len 32768
INFO 12-28 00:36:39 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:36:39 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:36:49 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:36:53 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:36:53 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2863793)[0;0m WARNING 12-28 00:36:53 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:36:53 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_07fef03d'), local_subscribe_addr='ipc:///tmp/352a8586-81e7-420b-9835-ec6910bae958', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:37:01 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:37:01 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:37:01 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:37:01 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:37:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8098a035'), local_subscribe_addr='ipc:///tmp/9c5db5c7-1bce-447e-93b4-82add3e2b59f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:37:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_999e8958'), local_subscribe_addr='ipc:///tmp/e823426c-bf9d-470f-921c-8d64a2b95d73', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:37:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b0efb5e3'), local_subscribe_addr='ipc:///tmp/754dd6dc-6b65-4b36-b87f-b140c0c16b60', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:37:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cff1f358'), local_subscribe_addr='ipc:///tmp/a22cb50d-a529-4cd8-aa2b-2c72a47a3ccd', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:37:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:37:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:37:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:37:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:37:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:37:09 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:37:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:37:09 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:37:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:37:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:37:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:37:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:37:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e233aa5d'), local_subscribe_addr='ipc:///tmp/9b0cb225-342b-4238-8996-89b191e1f211', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:37:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:37:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:37:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:37:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:37:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:37:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:37:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:37:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:37:10 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:37:10 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:37:10 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:37:10 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 00:37:10 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:37:10 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:37:10 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:37:10 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:10 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:10 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2864129)[0;0m INFO 12-28 00:37:10 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:10 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2864129)[0;0m INFO 12-28 00:37:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2864129)[0;0m INFO 12-28 00:37:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2864129)[0;0m INFO 12-28 00:37:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2864126)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2864126)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.81s/it]
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:14 [default_loader.py:267] Loading weights took 2.72 seconds
[1;36m(Worker_TP0 pid=2864126)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.26s/it]
[1;36m(Worker_TP0 pid=2864126)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.34s/it]
[1;36m(Worker_TP0 pid=2864126)[0;0m 
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:14 [default_loader.py:267] Loading weights took 2.71 seconds
[1;36m(Worker_TP3 pid=2864129)[0;0m INFO 12-28 00:37:14 [default_loader.py:267] Loading weights took 2.51 seconds
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:14 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 3.312541 seconds
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:14 [default_loader.py:267] Loading weights took 2.70 seconds
[1;36m(Worker_TP3 pid=2864129)[0;0m INFO 12-28 00:37:14 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 3.490565 seconds
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:14 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 3.442321 seconds
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:15 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 4.046698 seconds
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:19 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2864129)[0;0m INFO 12-28 00:37:19 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:19 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:19 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(Worker_TP2 pid=2864128)[0;0m WARNING 12-28 00:37:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2864127)[0;0m WARNING 12-28 00:37:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2864129)[0;0m WARNING 12-28 00:37:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2864126)[0;0m WARNING 12-28 00:37:20 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.64 seconds
[1;36m(EngineCore_DP0 pid=2863793)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2863793)[0;0m 
[1;36m(EngineCore_DP0 pid=2863793)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2863793)[0;0m 
[1;36m(EngineCore_DP0 pid=2863793)[0;0m INFO 12-28 00:37:20 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:37:20 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1391.66it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:12,  1.34s/it, est. speed input: 116.76 toks/s, output: 0.75 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:59,  1.22s/it, est. speed input: 96.39 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.25it/s, est. speed input: 1051.00 toks/s, output: 9.68 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.31it/s, est. speed input: 1981.89 toks/s, output: 27.92 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 20.94it/s, est. speed input: 2931.57 toks/s, output: 58.18 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:02<00:02, 25.29it/s, est. speed input: 3452.74 toks/s, output: 91.32 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:03<00:02, 29.35it/s, est. speed input: 3777.26 toks/s, output: 125.41 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:03<00:02, 20.06it/s, est. speed input: 3578.41 toks/s, output: 156.14 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 22.07it/s, est. speed input: 3546.90 toks/s, output: 194.82 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 20.77it/s, est. speed input: 3496.41 toks/s, output: 231.38 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 20.01it/s, est. speed input: 3370.08 toks/s, output: 270.61 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:02, 18.25it/s, est. speed input: 3245.07 toks/s, output: 300.29 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 18.35it/s, est. speed input: 3350.53 toks/s, output: 335.01 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:04<00:01, 21.84it/s, est. speed input: 3552.40 toks/s, output: 391.60 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:04<00:01, 20.37it/s, est. speed input: 3530.18 toks/s, output: 428.15 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:04<00:01, 21.32it/s, est. speed input: 3517.11 toks/s, output: 470.46 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:05<00:01, 19.41it/s, est. speed input: 3489.65 toks/s, output: 508.52 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:05<00:01, 13.52it/s, est. speed input: 3322.16 toks/s, output: 529.38 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:05<00:01, 10.87it/s, est. speed input: 3324.34 toks/s, output: 543.63 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:06<00:01, 13.06it/s, est. speed input: 3441.82 toks/s, output: 596.86 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:06<00:00, 14.13it/s, est. speed input: 3396.58 toks/s, output: 631.63 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:01,  5.73it/s, est. speed input: 2943.89 toks/s, output: 593.08 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:07<00:01,  6.04it/s, est. speed input: 2856.64 toks/s, output: 624.78 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:08<00:01,  4.77it/s, est. speed input: 2634.49 toks/s, output: 629.72 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:08<00:01,  4.21it/s, est. speed input: 2550.54 toks/s, output: 631.57 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.21it/s, est. speed input: 2684.59 toks/s, output: 781.94 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.85it/s, est. speed input: 2684.59 toks/s, output: 781.94 toks/s]
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:29 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2864126)[0;0m INFO 12-28 00:37:29 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:29 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:29 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2864128)[0;0m INFO 12-28 00:37:29 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2864127)[0;0m INFO 12-28 00:37:29 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2864129)[0;0m INFO 12-28 00:37:29 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:37:34] ax.service.ax_client: Completed trial 5 with data: {'throughput(token/s)': 775.221844, 'energy(J/token)': 1.699433}.
[INFO 12-28 00:37:34] ax.service.ax_client: Generated new trial 6 with parameters {'block_size': 128, 'max_num_seqs': 128, 'max_num_batched_tokens': 4096, 'tensor_parallel_size': 1, 'pipeline_parallel_size': 2, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:37:29', project_name='codecarbon', run_id='e75c391c-366c-40d1-a8ad-dbdc72b7ad90', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=51.30713436496444, emissions=0.0007399021253768523, emissions_rate=1.4421037825143115e-05, cpu_power=30.160060281428574, gpu_power=269.48475874720566, ram_power=70.0, cpu_energy=0.0004111358961828252, gpu_energy=0.0017463597304194778, ram_energy=0.0009567147340492942, energy_consumed=0.003114210360651597, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 6/10 completed. Throughput: 775.22, Energy: (1.6994326661127406, 0.0)
INFO 12-28 00:37:36 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 2, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:37:37 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:37:37 [model.py:1510] Using max model len 32768
INFO 12-28 00:37:37 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=4096.
INFO 12-28 00:37:37 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:37:43 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:37:47 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:37:47 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2865175)[0;0m WARNING 12-28 00:37:47 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:37:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_046ffeb7'), local_subscribe_addr='ipc:///tmp/ac3c0dc2-27b8-49ac-97a4-530fc809e679', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:37:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:37:54 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:37:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_583a92b2'), local_subscribe_addr='ipc:///tmp/b2f655ec-0dc8-4046-b796-8cf4cdafb78a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:38:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9036079a'), local_subscribe_addr='ipc:///tmp/57d4c514-505a-4292-a412-1170230525f8', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:38:01 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:38:01 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:38:01 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:38:01 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 00:38:01 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 00:38:01 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
WARNING 12-28 00:38:02 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:38:02 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1 pid=2865379)[0;0m INFO 12-28 00:38:02 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:02 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=2865379)[0;0m INFO 12-28 00:38:02 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:02 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=2865379)[0;0m INFO 12-28 00:38:02 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:02 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:02 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=2865379)[0;0m INFO 12-28 00:38:02 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=2865378)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_PP0 pid=2865378)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.01it/s]
[1;36m(Worker_PP0 pid=2865378)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.84it/s]
[1;36m(Worker_PP0 pid=2865378)[0;0m 
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:04 [default_loader.py:267] Loading weights took 1.09 seconds
[1;36m(Worker_PP1 pid=2865379)[0;0m INFO 12-28 00:38:04 [default_loader.py:267] Loading weights took 1.07 seconds
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:04 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 1.692367 seconds
[1;36m(Worker_PP1 pid=2865379)[0;0m INFO 12-28 00:38:05 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 1.940860 seconds
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:06 [gpu_worker.py:298] Available KV cache memory: 12.62 GiB
[1;36m(Worker_PP1 pid=2865379)[0;0m INFO 12-28 00:38:06 [gpu_worker.py:298] Available KV cache memory: 12.58 GiB
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:38:07 [kv_cache_utils.py:1087] GPU KV cache size: 206,592 tokens
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:38:07 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.83x
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:38:07 [kv_cache_utils.py:1087] GPU KV cache size: 206,080 tokens
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:38:07 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 24.77x
[1;36m(Worker_PP1 pid=2865379)[0;0m WARNING 12-28 00:38:07 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:38:07 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.27 seconds
[1;36m(EngineCore_DP0 pid=2865175)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2865175)[0;0m 
[1;36m(EngineCore_DP0 pid=2865175)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2865175)[0;0m 
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:38:08 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=2865175)[0;0m INFO 12-28 00:38:08 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:38:08 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(Worker_PP0 pid=2865378)[0;0m WARNING 12-28 00:38:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=2865378)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0 pid=2865378)[0;0m 
[1;36m(Worker_PP0 pid=2865378)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0 pid=2865378)[0;0m 
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1289.50it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<01:47,  1.08s/it, est. speed input: 144.31 toks/s, output: 0.92 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:01<00:46,  2.08it/s, est. speed input: 255.67 toks/s, output: 1.85 toks/s]Processed prompts:   4%|â–         | 4/100 [00:02<01:05,  1.46it/s, est. speed input: 331.19 toks/s, output: 1.49 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:03<01:00,  1.58it/s, est. speed input: 418.75 toks/s, output: 1.55 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:03<00:50,  1.85it/s, est. speed input: 391.39 toks/s, output: 2.25 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:03<00:12,  6.84it/s, est. speed input: 1061.55 toks/s, output: 8.59 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:03<00:07, 10.77it/s, est. speed input: 1238.29 toks/s, output: 15.59 toks/s]Processed prompts:  21%|â–ˆâ–ˆ        | 21/100 [00:03<00:06, 12.72it/s, est. speed input: 1353.93 toks/s, output: 20.65 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:04<00:06, 12.63it/s, est. speed input: 1439.31 toks/s, output: 26.35 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:04<00:04, 15.27it/s, est. speed input: 1728.15 toks/s, output: 36.60 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:04<00:04, 17.08it/s, est. speed input: 1825.99 toks/s, output: 44.97 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:04<00:04, 13.26it/s, est. speed input: 1772.09 toks/s, output: 52.39 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:04<00:04, 13.90it/s, est. speed input: 1802.13 toks/s, output: 59.00 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:05<00:03, 16.21it/s, est. speed input: 1875.22 toks/s, output: 70.43 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:05<00:03, 17.08it/s, est. speed input: 2022.36 toks/s, output: 81.91 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:05<00:03, 13.81it/s, est. speed input: 2042.01 toks/s, output: 91.18 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:05<00:03, 13.54it/s, est. speed input: 2076.50 toks/s, output: 98.82 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:05<00:04, 12.60it/s, est. speed input: 2099.43 toks/s, output: 106.78 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:06<00:03, 13.49it/s, est. speed input: 2054.75 toks/s, output: 119.66 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:06<00:06,  7.34it/s, est. speed input: 2015.33 toks/s, output: 119.63 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:06<00:05,  7.66it/s, est. speed input: 1965.70 toks/s, output: 128.90 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:07<00:04,  9.04it/s, est. speed input: 2021.34 toks/s, output: 140.37 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:07<00:06,  6.50it/s, est. speed input: 1898.63 toks/s, output: 144.11 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:07<00:05,  6.80it/s, est. speed input: 1885.74 toks/s, output: 149.25 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:08<00:07,  5.23it/s, est. speed input: 1808.44 toks/s, output: 149.96 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:08<00:09,  4.04it/s, est. speed input: 1812.64 toks/s, output: 149.97 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:08<00:07,  4.59it/s, est. speed input: 1836.56 toks/s, output: 155.57 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:09<00:08,  3.91it/s, est. speed input: 1771.53 toks/s, output: 157.43 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:09<00:05,  5.58it/s, est. speed input: 1792.66 toks/s, output: 171.35 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:09<00:04,  7.50it/s, est. speed input: 1779.40 toks/s, output: 185.50 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:09<00:04,  6.42it/s, est. speed input: 1785.29 toks/s, output: 194.53 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:09<00:04,  6.21it/s, est. speed input: 1758.33 toks/s, output: 199.63 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:10<00:08,  3.32it/s, est. speed input: 1706.69 toks/s, output: 193.66 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:10<00:06,  3.95it/s, est. speed input: 1716.90 toks/s, output: 200.84 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:11<00:06,  4.01it/s, est. speed input: 1685.57 toks/s, output: 205.68 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:11<00:05,  4.23it/s, est. speed input: 1664.16 toks/s, output: 211.27 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:11<00:05,  4.24it/s, est. speed input: 1635.95 toks/s, output: 216.30 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:11<00:03,  6.40it/s, est. speed input: 1611.42 toks/s, output: 239.26 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:12<00:05,  3.65it/s, est. speed input: 1524.35 toks/s, output: 235.08 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:12<00:05,  3.57it/s, est. speed input: 1509.87 toks/s, output: 239.49 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:13<00:04,  3.62it/s, est. speed input: 1484.74 toks/s, output: 244.60 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:13<00:03,  4.04it/s, est. speed input: 1470.49 toks/s, output: 251.64 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:13<00:04,  3.37it/s, est. speed input: 1424.63 toks/s, output: 253.90 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:14<00:04,  2.92it/s, est. speed input: 1449.82 toks/s, output: 256.10 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:14<00:03,  3.33it/s, est. speed input: 1431.64 toks/s, output: 263.19 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:14<00:03,  3.98it/s, est. speed input: 1443.18 toks/s, output: 271.30 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:14<00:02,  3.72it/s, est. speed input: 1392.24 toks/s, output: 281.70 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:16<00:04,  2.06it/s, est. speed input: 1296.52 toks/s, output: 272.64 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:16<00:03,  2.10it/s, est. speed input: 1272.52 toks/s, output: 276.44 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:19<00:07,  1.11s/it, est. speed input: 1091.12 toks/s, output: 248.26 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:19<00:05,  1.19it/s, est. speed input: 1083.52 toks/s, output: 258.10 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:20<00:04,  1.16it/s, est. speed input: 1038.94 toks/s, output: 258.59 toks/s]Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:21<00:03,  1.30it/s, est. speed input: 1026.49 toks/s, output: 264.13 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:21<00:00,  2.75it/s, est. speed input: 1066.61 toks/s, output: 299.24 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:21<00:00,  2.75it/s, est. speed input: 1071.05 toks/s, output: 310.93 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:21<00:00,  4.73it/s, est. speed input: 1071.05 toks/s, output: 310.93 toks/s]
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:30 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=2865378)[0;0m INFO 12-28 00:38:30 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=2865379)[0;0m INFO 12-28 00:38:30 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:38:35] ax.service.ax_client: Completed trial 6 with data: {'throughput(token/s)': 309.769058, 'energy(J/token)': 1.492943}.
[INFO 12-28 00:38:35] ax.service.ax_client: Generated new trial 7 with parameters {'block_size': 64, 'max_num_seqs': 256, 'max_num_batched_tokens': 8192, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 2, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'bfloat16', 'data_parallel_size': 1} using model Sobol.
[INFO 12-28 00:38:35] ax.service.ax_client: Completed trial 7 with data: {'throughput(token/s)': 0.0, 'energy(J/token)': 1000000000.0}.
[INFO 12-28 00:38:35] ax.service.ax_client: Generated new trial 8 with parameters {'block_size': 64, 'max_num_seqs': 64, 'max_num_batched_tokens': 4096, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'dtype': 'float16', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:38:30', project_name='codecarbon', run_id='47a9f6c2-0a5c-4c50-a2a6-0fc71b95cb24', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=53.943791317986324, emissions=0.0006478327803254851, emissions_rate=1.2009403946167946e-05, cpu_power=30.019001610000004, gpu_power=139.37627697560723, ram_power=70.0, cpu_energy=0.00043256012833958057, gpu_energy=0.0012861257511218582, ram_energy=0.0010080092971901954, energy_consumed=0.002726695176651634, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 7/10 completed. Throughput: 309.77, Energy: (1.4929433666837844, 0.0)
Skipping trial: 8 GPUs required, only 4 available.
   Initial Trial 8/10 completed. Throughput: 0.00, Energy: (1000000000.0, 0.0)
INFO 12-28 00:38:37 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'tensor_parallel_size': 4, 'block_size': 64, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:38:38 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 00:38:38 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 00:38:38 [model.py:1510] Using max model len 32768
INFO 12-28 00:38:38 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=4096.
INFO 12-28 00:38:38 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:38:46 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:38:49 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:38:49 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2866805)[0;0m WARNING 12-28 00:38:49 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:38:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_1c4f94fb'), local_subscribe_addr='ipc:///tmp/7884f667-2574-48cc-833c-1d67c1349281', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:38:55 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:38:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:38:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:38:57 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:39:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_af7ea03b'), local_subscribe_addr='ipc:///tmp/840cc07b-8dd6-414e-aff7-8d1bd0d9d97f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:39:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ec410985'), local_subscribe_addr='ipc:///tmp/01e3bce9-8b80-4afc-99d6-7aefafe63f4d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:39:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f1c77c5b'), local_subscribe_addr='ipc:///tmp/26cd87d9-48b6-41ab-8e92-cae9965c98e4', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:39:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_163835ca'), local_subscribe_addr='ipc:///tmp/84011090-aafa-4a93-b7e0-52fccfe74c52', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:39:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:39:02 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:39:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:39:02 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:39:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:39:02 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:39:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:39:02 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:39:03 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:39:03 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:39:03 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:39:03 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:39:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_428f27c8'), local_subscribe_addr='ipc:///tmp/e2bd6530-8d97-4a65-8614-3c55108938be', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:39:03 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:39:03 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:39:03 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:39:03 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:39:03 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:39:03 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:39:03 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:39:03 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:39:03 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:39:03 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:39:03 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:39:03 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 00:39:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:39:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:39:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:39:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:03 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:04 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:04 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:04 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:04 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:04 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:04 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:04 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:04 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:04 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:04 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:04 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2867074)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2867074)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.79s/it]
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:06 [default_loader.py:267] Loading weights took 2.12 seconds
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:07 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.855353 seconds
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:07 [default_loader.py:267] Loading weights took 2.12 seconds
[1;36m(Worker_TP0 pid=2867074)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
[1;36m(Worker_TP0 pid=2867074)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.33s/it]
[1;36m(Worker_TP0 pid=2867074)[0;0m 
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:07 [default_loader.py:267] Loading weights took 2.69 seconds
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:07 [default_loader.py:267] Loading weights took 2.70 seconds
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:08 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 3.615652 seconds
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:08 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 3.613460 seconds
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:08 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 3.890764 seconds
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:11 [gpu_worker.py:298] Available KV cache memory: 16.21 GiB
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:11 [gpu_worker.py:298] Available KV cache memory: 16.21 GiB
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:11 [gpu_worker.py:298] Available KV cache memory: 16.21 GiB
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:12 [gpu_worker.py:298] Available KV cache memory: 16.21 GiB
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [kv_cache_utils.py:1087] GPU KV cache size: 531,008 tokens
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.32x
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [kv_cache_utils.py:1087] GPU KV cache size: 531,008 tokens
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.32x
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [kv_cache_utils.py:1087] GPU KV cache size: 531,008 tokens
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.32x
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [kv_cache_utils.py:1087] GPU KV cache size: 531,008 tokens
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.32x
[1;36m(Worker_TP1 pid=2867075)[0;0m WARNING 12-28 00:39:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2867077)[0;0m WARNING 12-28 00:39:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2867076)[0;0m WARNING 12-28 00:39:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2867074)[0;0m WARNING 12-28 00:39:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.93 seconds
[1;36m(EngineCore_DP0 pid=2866805)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2866805)[0;0m 
[1;36m(EngineCore_DP0 pid=2866805)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2866805)[0;0m 
[1;36m(EngineCore_DP0 pid=2866805)[0;0m INFO 12-28 00:39:12 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:39:13 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1822.85it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<00:44,  2.23it/s, est. speed input: 348.20 toks/s, output: 2.23 toks/s]Processed prompts:   2%|â–         | 2/100 [00:00<00:43,  2.25it/s, est. speed input: 240.04 toks/s, output: 3.36 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:01<00:30,  3.17it/s, est. speed input: 625.62 toks/s, output: 4.75 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:01<00:13,  6.52it/s, est. speed input: 861.21 toks/s, output: 12.62 toks/s]Processed prompts:  14%|â–ˆâ–        | 14/100 [00:02<00:08, 10.22it/s, est. speed input: 1354.21 toks/s, output: 27.92 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:05, 13.80it/s, est. speed input: 1418.73 toks/s, output: 42.67 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:02<00:04, 17.55it/s, est. speed input: 2394.03 toks/s, output: 56.38 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 22.46it/s, est. speed input: 3011.90 toks/s, output: 70.62 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:02<00:03, 20.62it/s, est. speed input: 3093.76 toks/s, output: 83.20 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:02<00:03, 20.59it/s, est. speed input: 3103.32 toks/s, output: 98.57 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:03<00:02, 26.18it/s, est. speed input: 3499.01 toks/s, output: 138.71 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:03<00:02, 22.95it/s, est. speed input: 3459.21 toks/s, output: 154.11 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:03<00:02, 22.41it/s, est. speed input: 3718.11 toks/s, output: 175.38 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:03<00:02, 21.61it/s, est. speed input: 3579.51 toks/s, output: 211.26 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:03<00:02, 21.47it/s, est. speed input: 3489.73 toks/s, output: 239.69 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:03<00:01, 27.04it/s, est. speed input: 3558.53 toks/s, output: 303.32 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:04<00:01, 24.64it/s, est. speed input: 3608.22 toks/s, output: 333.35 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:04<00:01, 25.58it/s, est. speed input: 3559.13 toks/s, output: 366.66 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:04<00:01, 20.65it/s, est. speed input: 3691.09 toks/s, output: 395.88 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:04<00:01, 17.41it/s, est. speed input: 3578.79 toks/s, output: 423.02 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:04<00:01, 19.01it/s, est. speed input: 3730.09 toks/s, output: 460.34 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:04<00:01, 18.10it/s, est. speed input: 3953.49 toks/s, output: 498.71 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:05<00:00, 17.26it/s, est. speed input: 3886.14 toks/s, output: 524.21 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:05<00:00, 15.34it/s, est. speed input: 3769.84 toks/s, output: 545.48 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:05<00:01, 12.09it/s, est. speed input: 3678.04 toks/s, output: 561.32 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:05<00:01,  9.42it/s, est. speed input: 3505.00 toks/s, output: 572.23 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:06<00:00,  9.31it/s, est. speed input: 3397.33 toks/s, output: 601.35 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:06<00:00,  7.90it/s, est. speed input: 3270.43 toks/s, output: 615.65 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:06<00:01,  5.40it/s, est. speed input: 3054.49 toks/s, output: 598.84 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:08<00:01,  2.73it/s, est. speed input: 2648.42 toks/s, output: 547.00 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:08<00:00,  4.99it/s, est. speed input: 2733.77 toks/s, output: 657.98 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.77it/s, est. speed input: 2665.74 toks/s, output: 668.17 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.77it/s, est. speed input: 2665.74 toks/s, output: 668.17 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.77it/s, est. speed input: 2665.74 toks/s, output: 668.17 toks/s]
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:22 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2867074)[0;0m INFO 12-28 00:39:22 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:22 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2867075)[0;0m INFO 12-28 00:39:22 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:22 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:22 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2867077)[0;0m INFO 12-28 00:39:22 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2867076)[0;0m INFO 12-28 00:39:22 [multiproc_executor.py:599] WorkerProc shutting down.
[INFO 12-28 00:39:27] ax.service.ax_client: Completed trial 8 with data: {'throughput(token/s)': 663.784999, 'energy(J/token)': 1.762703}.
[INFO 12-28 00:39:27] ax.service.ax_client: Generated new trial 9 with parameters {'block_size': 128, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 4, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'bfloat16', 'data_parallel_size': 1} using model Sobol.
[INFO 12-28 00:39:27] ax.service.ax_client: Completed trial 9 with data: {'throughput(token/s)': 0.0, 'energy(J/token)': 1000000000.0}.
[INFO 12-28 00:39:27] ax.service.ax_client: Generated new trial 10 with parameters {'block_size': 64, 'max_num_seqs': 128, 'max_num_batched_tokens': 8192, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 4, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'bfloat16', 'data_parallel_size': 1} using model Sobol.
[INFO 12-28 00:39:27] ax.service.ax_client: Completed trial 10 with data: {'throughput(token/s)': 0.0, 'energy(J/token)': 1000000000.0}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:39:27] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 357.66484344005585), ObjectiveThreshold(throughput(token/s) >= 263.223878162118)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
[INFO 12-28 00:39:28] ax.service.ax_client: Generated new trial 11 with parameters {'block_size': 32, 'max_num_seqs': 128, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 1, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:39:22', project_name='codecarbon', run_id='961f7ba1-a53c-4658-9fcd-39e18302ea96', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=45.22027899406385, emissions=0.000660422644031794, emissions_rate=1.4604568099159426e-05, cpu_power=30.360940083750002, gpu_power=216.45840519328442, ram_power=70.0, cpu_energy=0.000365604979248028, gpu_energy=0.001565489863501135, ram_energy=0.0008485904290091843, energy_consumed=0.0027796852717583472, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   Initial Trial 9/10 completed. Throughput: 663.78, Energy: (1.7627033606359082, 0.0)
Skipping trial: 8 GPUs required, only 4 available.
   Initial Trial 10/10 completed. Throughput: 0.00, Energy: (1000000000.0, 0.0)
Skipping trial: 8 GPUs required, only 4 available.
   BO Iteration 1/30 completed. Throughput: 0.00, Energy: 1000000000.000000. Pareto size: 4.
INFO 12-28 00:39:29 [utils.py:233] non-default args: {'seed': 42, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:39:30 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:39:30 [model.py:1510] Using max model len 32768
INFO 12-28 00:39:30 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:39:30 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:39:36 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:38 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:38 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:40 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2868433)[0;0m WARNING 12-28 00:39:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:41 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:41 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:41 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2868433)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2868433)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.09s/it]
[1;36m(EngineCore_DP0 pid=2868433)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.27it/s]
[1;36m(EngineCore_DP0 pid=2868433)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.20it/s]
[1;36m(EngineCore_DP0 pid=2868433)[0;0m 
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:43 [default_loader.py:267] Loading weights took 1.70 seconds
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:44 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.275552 seconds
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:48 [gpu_worker.py:298] Available KV cache memory: 5.03 GiB
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:48 [kv_cache_utils.py:1087] GPU KV cache size: 41,152 tokens
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:48 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 2.51x
[1;36m(EngineCore_DP0 pid=2868433)[0;0m WARNING 12-28 00:39:48 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:48 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.23 seconds
[1;36m(EngineCore_DP0 pid=2868433)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2868433)[0;0m 
[1;36m(EngineCore_DP0 pid=2868433)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2868433)[0;0m 
[1;36m(EngineCore_DP0 pid=2868433)[0;0m INFO 12-28 00:39:48 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:39:49 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1364.38it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:03<05:12,  3.16s/it, est. speed input: 49.37 toks/s, output: 0.32 toks/s]Processed prompts:   2%|â–         | 2/100 [00:05<04:41,  2.87s/it, est. speed input: 40.82 toks/s, output: 0.34 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:05<02:35,  1.61s/it, est. speed input: 47.54 toks/s, output: 0.84 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:06<00:32,  2.84it/s, est. speed input: 443.23 toks/s, output: 4.08 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:06<00:13,  6.00it/s, est. speed input: 750.17 toks/s, output: 9.51 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:06<00:08,  9.05it/s, est. speed input: 993.37 toks/s, output: 16.18 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:06<00:06, 10.88it/s, est. speed input: 1142.14 toks/s, output: 21.29 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:06<00:05, 11.86it/s, est. speed input: 1290.78 toks/s, output: 25.71 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:07<00:05, 11.78it/s, est. speed input: 1298.79 toks/s, output: 31.10 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:07<00:04, 13.88it/s, est. speed input: 1427.02 toks/s, output: 41.59 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:07<00:03, 15.94it/s, est. speed input: 1504.27 toks/s, output: 51.06 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:08<00:05,  9.43it/s, est. speed input: 1529.57 toks/s, output: 55.06 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:08<00:05,  9.21it/s, est. speed input: 1491.31 toks/s, output: 60.70 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:05,  9.76it/s, est. speed input: 1509.79 toks/s, output: 67.56 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:05,  8.66it/s, est. speed input: 1467.10 toks/s, output: 73.65 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:09<00:06,  7.94it/s, est. speed input: 1436.15 toks/s, output: 80.31 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:09<00:06,  7.32it/s, est. speed input: 1383.59 toks/s, output: 90.19 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:09<00:06,  7.17it/s, est. speed input: 1365.21 toks/s, output: 93.81 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:10<00:06,  6.43it/s, est. speed input: 1334.78 toks/s, output: 96.96 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:10<00:06,  5.95it/s, est. speed input: 1314.73 toks/s, output: 104.13 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:10<00:07,  5.31it/s, est. speed input: 1269.70 toks/s, output: 110.92 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:11<00:10,  3.73it/s, est. speed input: 1209.39 toks/s, output: 111.29 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:11<00:09,  3.84it/s, est. speed input: 1209.82 toks/s, output: 115.44 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:12<00:07,  4.54it/s, est. speed input: 1254.00 toks/s, output: 125.46 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:12<00:07,  4.50it/s, est. speed input: 1238.71 toks/s, output: 129.85 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:12<00:07,  4.15it/s, est. speed input: 1240.81 toks/s, output: 133.53 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:12<00:03,  7.36it/s, est. speed input: 1245.84 toks/s, output: 158.32 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:13<00:04,  6.66it/s, est. speed input: 1279.13 toks/s, output: 167.69 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:03,  7.83it/s, est. speed input: 1279.82 toks/s, output: 180.23 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:03,  6.40it/s, est. speed input: 1287.22 toks/s, output: 183.68 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:14<00:03,  6.57it/s, est. speed input: 1290.28 toks/s, output: 194.53 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:14<00:02,  6.69it/s, est. speed input: 1289.59 toks/s, output: 205.60 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:14<00:03,  5.89it/s, est. speed input: 1325.50 toks/s, output: 214.85 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:15<00:04,  3.86it/s, est. speed input: 1252.76 toks/s, output: 217.75 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:16<00:04,  3.53it/s, est. speed input: 1226.42 toks/s, output: 220.71 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:17<00:05,  2.38it/s, est. speed input: 1167.89 toks/s, output: 222.67 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:20<00:08,  1.27it/s, est. speed input: 1027.78 toks/s, output: 204.58 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:21<00:08,  1.15it/s, est. speed input: 972.99 toks/s, output: 203.36 toks/s] Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:23<00:10,  1.17s/it, est. speed input: 884.58 toks/s, output: 195.10 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  1.17s/it, est. speed input: 948.93 toks/s, output: 291.14 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:23<00:00,  4.19it/s, est. speed input: 948.93 toks/s, output: 291.14 toks/s]
[rank0]:[W1228 00:40:13.536188566 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[INFO 12-28 00:40:16] ax.service.ax_client: Completed trial 11 with data: {'throughput(token/s)': 290.232831, 'energy(J/token)': 0.966065}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:40:16] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 394.8079671859741), ObjectiveThreshold(throughput(token/s) >= 652.6417828654232)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
[INFO 12-28 00:40:17] ax.service.ax_client: Generated new trial 12 with parameters {'block_size': 32, 'max_num_seqs': 64, 'max_num_batched_tokens': 8192, 'tensor_parallel_size': 1, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'dtype': 'bfloat16', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:40:13', project_name='codecarbon', run_id='e21409f5-d0c8-4884-ae2d-505d68ede5ea', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=44.372046274947934, emissions=0.0004430495779616867, emissions_rate=9.98488046317189e-06, cpu_power=30.221640462000003, gpu_power=70.46508779452536, ram_power=70.0, cpu_energy=0.0003579597442668104, gpu_energy=0.0006748174842980958, ram_energy=0.0008319958794564526, energy_consumed=0.0018647731080213591, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 2/30 completed. Throughput: 290.23, Energy: 0.966065. Pareto size: 3.
INFO 12-28 00:40:18 [utils.py:233] non-default args: {'dtype': 'bfloat16', 'seed': 42, 'block_size': 32, 'enable_prefix_caching': False, 'max_num_batched_tokens': 8192, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:40:19 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:40:19 [model.py:1510] Using max model len 32768
INFO 12-28 00:40:19 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-28 00:40:19 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:40:28 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:32 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:32 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:35 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2871111)[0;0m WARNING 12-28 00:40:36 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:36 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:36 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:36 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2871111)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2871111)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.30s/it]
[1;36m(EngineCore_DP0 pid=2871111)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.05it/s]
[1;36m(EngineCore_DP0 pid=2871111)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.01s/it]
[1;36m(EngineCore_DP0 pid=2871111)[0;0m 
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:39 [default_loader.py:267] Loading weights took 2.07 seconds
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:39 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.722024 seconds
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:42 [gpu_worker.py:298] Available KV cache memory: 5.45 GiB
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:43 [kv_cache_utils.py:1087] GPU KV cache size: 44,608 tokens
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:43 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 3.62x
[1;36m(EngineCore_DP0 pid=2871111)[0;0m WARNING 12-28 00:40:43 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:43 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.37 seconds
[1;36m(EngineCore_DP0 pid=2871111)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2871111)[0;0m 
[1;36m(EngineCore_DP0 pid=2871111)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2871111)[0;0m 
[1;36m(EngineCore_DP0 pid=2871111)[0;0m INFO 12-28 00:40:43 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:40:43 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1393.27it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:02<03:29,  2.11s/it, est. speed input: 73.78 toks/s, output: 0.47 toks/s]Processed prompts:   2%|â–         | 2/100 [00:03<03:02,  1.86s/it, est. speed input: 56.28 toks/s, output: 0.79 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:04<00:57,  1.64it/s, est. speed input: 230.11 toks/s, output: 2.66 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:04<00:46,  2.04it/s, est. speed input: 289.24 toks/s, output: 2.80 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:04<00:35,  2.59it/s, est. speed input: 328.03 toks/s, output: 4.11 toks/s]Processed prompts:  10%|â–ˆ         | 10/100 [00:04<00:19,  4.54it/s, est. speed input: 449.02 toks/s, output: 7.60 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 15/100 [00:05<00:12,  6.63it/s, est. speed input: 740.58 toks/s, output: 13.57 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:05<00:11,  7.22it/s, est. speed input: 986.50 toks/s, output: 17.70 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:05<00:07,  9.94it/s, est. speed input: 1157.87 toks/s, output: 27.16 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:05<00:06, 10.99it/s, est. speed input: 1274.45 toks/s, output: 30.99 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:06<00:07,  9.88it/s, est. speed input: 1357.25 toks/s, output: 36.33 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:06<00:06, 10.97it/s, est. speed input: 1365.98 toks/s, output: 37.46 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:06<00:06,  9.90it/s, est. speed input: 1335.45 toks/s, output: 41.60 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:07<00:07,  8.75it/s, est. speed input: 1333.10 toks/s, output: 43.04 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:07<00:06,  8.83it/s, est. speed input: 1392.33 toks/s, output: 45.93 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:07<00:05, 10.70it/s, est. speed input: 1467.75 toks/s, output: 54.58 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:07<00:04, 11.02it/s, est. speed input: 1493.17 toks/s, output: 57.64 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:05,  9.33it/s, est. speed input: 1483.23 toks/s, output: 62.07 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:08<00:04, 11.22it/s, est. speed input: 1559.97 toks/s, output: 73.07 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:08<00:03, 11.91it/s, est. speed input: 1575.19 toks/s, output: 89.70 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:09<00:04,  8.52it/s, est. speed input: 1565.64 toks/s, output: 96.91 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:09<00:05,  7.54it/s, est. speed input: 1510.77 toks/s, output: 101.83 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:09<00:05,  6.90it/s, est. speed input: 1535.44 toks/s, output: 105.10 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:10<00:05,  6.05it/s, est. speed input: 1494.43 toks/s, output: 114.11 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:10<00:06,  5.11it/s, est. speed input: 1447.31 toks/s, output: 116.58 toks/s]WARNING 12-28 00:40:54 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:11<00:06,  5.32it/s, est. speed input: 1435.42 toks/s, output: 121.47 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:11<00:06,  4.75it/s, est. speed input: 1472.36 toks/s, output: 124.91 toks/s]WARNING 12-28 00:40:55 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:12<00:11,  2.65it/s, est. speed input: 1367.35 toks/s, output: 122.21 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:12<00:10,  2.93it/s, est. speed input: 1348.83 toks/s, output: 126.38 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:13<00:10,  2.72it/s, est. speed input: 1367.57 toks/s, output: 129.03 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:13<00:09,  3.04it/s, est. speed input: 1358.40 toks/s, output: 133.94 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:13<00:07,  3.59it/s, est. speed input: 1369.76 toks/s, output: 139.03 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:06,  3.96it/s, est. speed input: 1332.63 toks/s, output: 149.90 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:14<00:05,  4.08it/s, est. speed input: 1383.54 toks/s, output: 155.51 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:14<00:06,  3.41it/s, est. speed input: 1414.41 toks/s, output: 157.60 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:14<00:04,  5.01it/s, est. speed input: 1423.74 toks/s, output: 172.09 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:15<00:05,  3.52it/s, est. speed input: 1376.72 toks/s, output: 174.12 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:15<00:05,  3.53it/s, est. speed input: 1353.81 toks/s, output: 179.42 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:15<00:03,  5.14it/s, est. speed input: 1351.75 toks/s, output: 194.80 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:15<00:03,  4.68it/s, est. speed input: 1332.46 toks/s, output: 199.76 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:16<00:03,  4.46it/s, est. speed input: 1311.76 toks/s, output: 204.91 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:17<00:06,  2.15it/s, est. speed input: 1226.10 toks/s, output: 200.18 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:17<00:04,  2.67it/s, est. speed input: 1217.64 toks/s, output: 207.53 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:17<00:03,  3.27it/s, est. speed input: 1210.42 toks/s, output: 215.23 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:18<00:06,  1.68it/s, est. speed input: 1130.87 toks/s, output: 209.95 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:19<00:04,  2.09it/s, est. speed input: 1122.92 toks/s, output: 217.72 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:20<00:06,  1.29it/s, est. speed input: 1044.56 toks/s, output: 211.38 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:23<00:10,  1.34s/it, est. speed input: 931.24 toks/s, output: 197.98 toks/s] Processed prompts:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:23<00:01,  2.23it/s, est. speed input: 953.69 toks/s, output: 251.59 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:24<00:00,  2.41it/s, est. speed input: 935.39 toks/s, output: 266.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  2.41it/s, est. speed input: 923.02 toks/s, output: 271.46 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  4.08it/s, est. speed input: 923.02 toks/s, output: 271.46 toks/s]
[rank0]:[W1228 00:41:09.856739288 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[INFO 12-28 00:41:11] ax.service.ax_client: Completed trial 12 with data: {'throughput(token/s)': 270.649706, 'energy(J/token)': 1.120056}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:41:12] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 149.0722900032997), ObjectiveThreshold(throughput(token/s) >= 241.73388367900552)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
[INFO 12-28 00:41:12] ax.service.ax_client: Generated new trial 13 with parameters {'block_size': 128, 'max_num_seqs': 256, 'max_num_batched_tokens': 4096, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 4, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model Sobol.
[INFO 12-28 00:41:12] ax.service.ax_client: Completed trial 13 with data: {'throughput(token/s)': 0.0, 'energy(J/token)': 1000000000.0}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:41:13] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 149.30236411094666), ObjectiveThreshold(throughput(token/s) >= 241.73377363114048)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
[INFO 12-28 00:41:13] ax.service.ax_client: Generated new trial 14 with parameters {'block_size': 128, 'max_num_seqs': 64, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 2, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'float16', 'data_parallel_size': 1} using model Sobol.
EmissionsData(timestamp='2025-12-28T00:41:08', project_name='codecarbon', run_id='2ba16b37-3691-4a41-990f-3cc27ef1a326', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=50.64013990201056, emissions=0.0004923831524335982, emissions_rate=9.723179149709441e-06, cpu_power=30.026519715000003, gpu_power=67.02216162369844, ram_power=70.0, cpu_energy=0.0004055043483475657, gpu_energy=0.0007232883564078918, ram_energy=0.0009436229055362574, energy_consumed=0.0020724156102917145, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 3/30 completed. Throughput: 270.65, Energy: 1.120056. Pareto size: 4.
Skipping trial: 8 GPUs required, only 4 available.
   BO Iteration 4/30 completed. Throughput: 0.00, Energy: 1000000000.000000. Pareto size: 4.
INFO 12-28 00:41:15 [utils.py:233] non-default args: {'dtype': 'float16', 'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:41:15 [model.py:547] Resolved architecture: MistralForCausalLM
WARNING 12-28 00:41:15 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 12-28 00:41:15 [model.py:1510] Using max model len 32768
INFO 12-28 00:41:15 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:41:15 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:41:23 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:27 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:27 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2873679)[0;0m WARNING 12-28 00:41:27 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_40c0086e'), local_subscribe_addr='ipc:///tmp/29e10c1c-e251-4e91-9d09-73e90277ba20', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:41:35 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:41:35 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:41:35 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:41:35 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:41:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0a6b5b0b'), local_subscribe_addr='ipc:///tmp/088baaec-f9d4-4328-bb20-3e518231bf2e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:41:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c91bcf15'), local_subscribe_addr='ipc:///tmp/bb8f11ad-98a4-4fec-bd25-e9a09c0f2c0f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:41:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c11070f6'), local_subscribe_addr='ipc:///tmp/5ef0b6e8-193a-465c-92df-5f4fccac8a0e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:41:41 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e0e73963'), local_subscribe_addr='ipc:///tmp/ea01a677-ffee-4ef2-99ff-9656ee35a9ca', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:41:41 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:41 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:41 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:41 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:41:41 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:41 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:41:41 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:41:41 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:41:42 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:41:42 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:41:42 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:41:42 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:41:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_2ad89f1b'), local_subscribe_addr='ipc:///tmp/b05bf1d9-473e-4ae3-91ec-80edc9dccf11', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:41:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_edb735af'), local_subscribe_addr='ipc:///tmp/bf0f3e90-f38e-4775-bf4e-75e5e2274b72', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:41:42 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:42 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:41:42 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:41:42 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:42 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:42 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:41:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:41:42 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:42 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:41:42 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:42 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:41:42 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:42 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:41:42 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:41:42 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:41:42 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:41:42 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:41:42 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
INFO 12-28 00:41:42 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
WARNING 12-28 00:41:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:41:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:41:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:41:42 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m INFO 12-28 00:41:42 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:41:42 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:41:42 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:41:42 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m INFO 12-28 00:41:43 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:41:43 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m INFO 12-28 00:41:43 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:41:43 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:41:43 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:41:43 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:41:43 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:41:43 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m INFO 12-28 00:41:43 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:41:43 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:41:43 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:41:43 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m INFO 12-28 00:41:46 [default_loader.py:267] Loading weights took 3.07 seconds
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m INFO 12-28 00:41:47 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 3.659689 seconds
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.83s/it]
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.95s/it]
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m 
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:41:47 [default_loader.py:267] Loading weights took 3.90 seconds
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:41:48 [default_loader.py:267] Loading weights took 3.91 seconds
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:41:48 [default_loader.py:267] Loading weights took 3.89 seconds
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:41:48 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 4.643117 seconds
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:41:48 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 4.922860 seconds
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:41:48 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 5.129666 seconds
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:41:51 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m INFO 12-28 00:41:51 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:41:52 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:41:52 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [kv_cache_utils.py:1087] GPU KV cache size: 512,256 tokens
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.02x
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [kv_cache_utils.py:1087] GPU KV cache size: 512,256 tokens
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.02x
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [kv_cache_utils.py:1087] GPU KV cache size: 508,672 tokens
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.81x
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [kv_cache_utils.py:1087] GPU KV cache size: 508,672 tokens
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.81x
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m WARNING 12-28 00:41:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m WARNING 12-28 00:41:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.10 seconds
[1;36m(EngineCore_DP0 pid=2873679)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2873679)[0;0m 
[1;36m(EngineCore_DP0 pid=2873679)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2873679)[0;0m 
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=2873679)[0;0m INFO 12-28 00:41:53 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:41:53 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(Worker_PP0_TP0 pid=2874243)[0;0m WARNING 12-28 00:41:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m WARNING 12-28 00:41:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1387.22it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(Worker_PP0_TP0 pid=2874243)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m 
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m 
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m 
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m 
Processed prompts:   1%|          | 1/100 [00:02<03:25,  2.07s/it, est. speed input: 75.19 toks/s, output: 0.48 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:02<00:58,  1.66it/s, est. speed input: 352.60 toks/s, output: 1.33 toks/s]Processed prompts:   4%|â–         | 4/100 [00:02<00:41,  2.30it/s, est. speed input: 397.41 toks/s, output: 2.54 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:02<00:23,  4.07it/s, est. speed input: 685.24 toks/s, output: 5.25 toks/s]Processed prompts:   8%|â–Š         | 8/100 [00:02<00:16,  5.64it/s, est. speed input: 827.47 toks/s, output: 7.58 toks/s]Processed prompts:  14%|â–ˆâ–        | 14/100 [00:02<00:07, 11.19it/s, est. speed input: 1633.73 toks/s, output: 21.20 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:03<00:07, 11.73it/s, est. speed input: 1737.92 toks/s, output: 26.15 toks/s]Processed prompts:  21%|â–ˆâ–ˆ        | 21/100 [00:03<00:04, 17.54it/s, est. speed input: 2315.59 toks/s, output: 38.26 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:03<00:03, 19.57it/s, est. speed input: 2508.66 toks/s, output: 45.93 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:03<00:03, 18.65it/s, est. speed input: 2561.51 toks/s, output: 52.27 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:03<00:04, 14.12it/s, est. speed input: 2385.61 toks/s, output: 58.20 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:03<00:04, 14.77it/s, est. speed input: 2497.49 toks/s, output: 62.44 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:04<00:05, 13.10it/s, est. speed input: 2385.41 toks/s, output: 70.51 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:04<00:04, 13.34it/s, est. speed input: 2344.43 toks/s, output: 72.88 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:04<00:05, 11.59it/s, est. speed input: 2307.07 toks/s, output: 82.03 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:04<00:04, 13.74it/s, est. speed input: 2372.65 toks/s, output: 93.52 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:04<00:04, 12.60it/s, est. speed input: 2338.06 toks/s, output: 103.01 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:04<00:03, 17.25it/s, est. speed input: 2496.36 toks/s, output: 125.80 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:05<00:02, 18.81it/s, est. speed input: 2501.40 toks/s, output: 146.79 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:05<00:02, 17.71it/s, est. speed input: 2452.84 toks/s, output: 166.61 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:05<00:03, 11.54it/s, est. speed input: 2402.02 toks/s, output: 171.04 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:06<00:05,  8.52it/s, est. speed input: 2242.26 toks/s, output: 174.59 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:06<00:05,  8.09it/s, est. speed input: 2156.39 toks/s, output: 187.63 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:06<00:05,  7.09it/s, est. speed input: 2101.29 toks/s, output: 196.19 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:06<00:04,  8.24it/s, est. speed input: 2193.27 toks/s, output: 213.20 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:07<00:04,  7.83it/s, est. speed input: 2119.79 toks/s, output: 226.48 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:07<00:03,  9.24it/s, est. speed input: 2099.91 toks/s, output: 244.11 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:07<00:04,  7.38it/s, est. speed input: 2173.87 toks/s, output: 255.42 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:07<00:04,  7.39it/s, est. speed input: 2171.63 toks/s, output: 263.86 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:08<00:04,  6.22it/s, est. speed input: 2108.00 toks/s, output: 268.37 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:08<00:02,  9.72it/s, est. speed input: 2240.20 toks/s, output: 300.13 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:08<00:02,  9.90it/s, est. speed input: 2237.98 toks/s, output: 318.34 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:08<00:01, 11.68it/s, est. speed input: 2213.37 toks/s, output: 349.47 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:09<00:02,  6.97it/s, est. speed input: 2078.13 toks/s, output: 352.67 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:09<00:03,  5.49it/s, est. speed input: 1973.80 toks/s, output: 361.87 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:09<00:02,  6.57it/s, est. speed input: 1953.08 toks/s, output: 386.07 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:09<00:02,  6.88it/s, est. speed input: 1940.45 toks/s, output: 396.86 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:11<00:05,  2.42it/s, est. speed input: 1691.54 toks/s, output: 360.00 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:11<00:04,  2.58it/s, est. speed input: 1721.02 toks/s, output: 367.36 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:12<00:04,  2.27it/s, est. speed input: 1613.36 toks/s, output: 368.16 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:13<00:04,  1.80it/s, est. speed input: 1504.08 toks/s, output: 360.79 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:14<00:00,  4.73it/s, est. speed input: 1553.75 toks/s, output: 499.41 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:14<00:00,  4.73it/s, est. speed input: 1553.75 toks/s, output: 499.41 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:14<00:00,  6.86it/s, est. speed input: 1553.75 toks/s, output: 499.41 toks/s]
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:42:09 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP0 pid=2874243)[0;0m INFO 12-28 00:42:09 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:42:09 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:42:09 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP0 pid=2874245)[0;0m INFO 12-28 00:42:09 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=2874244)[0;0m INFO 12-28 00:42:09 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP1 pid=2874246)[0;0m INFO 12-28 00:42:09 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:42:14] ax.service.ax_client: Completed trial 14 with data: {'throughput(token/s)': 496.909304, 'energy(J/token)': 1.675361}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:42:15] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 151.99266612529755), ObjectiveThreshold(throughput(token/s) >= 241.73371432039949)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
[INFO 12-28 00:42:15] ax.service.ax_client: Generated new trial 15 with parameters {'block_size': 64, 'max_num_seqs': 256, 'max_num_batched_tokens': 4096, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 2, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'bfloat16', 'data_parallel_size': 1} using model Sobol.
[INFO 12-28 00:42:15] ax.service.ax_client: Completed trial 15 with data: {'throughput(token/s)': 0.0, 'energy(J/token)': 1000000000.0}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:42:16] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 159.0890970826149), ObjectiveThreshold(throughput(token/s) >= 241.73356403797288)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:42:20] ax.service.ax_client: Generated new trial 16 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:42:09', project_name='codecarbon', run_id='78507f86-8105-491b-b094-c7fd35ce25ec', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=54.19119125197176, emissions=0.0008049401962428014, emissions_rate=1.4853709203403301e-05, cpu_power=30.313908183000006, gpu_power=216.29862089620073, ram_power=70.0, cpu_energy=0.0004383480681215263, gpu_energy=0.0019370273829535378, ram_energy=0.0010125768373002455, energy_consumed=0.0033879522883753094, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 5/30 completed. Throughput: 496.91, Energy: 1.675361. Pareto size: 4.
Skipping trial: 8 GPUs required, only 4 available.
   BO Iteration 6/30 completed. Throughput: 0.00, Energy: 1000000000.000000. Pareto size: 4.
INFO 12-28 00:42:21 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:42:22 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:42:22 [model.py:1510] Using max model len 32768
INFO 12-28 00:42:22 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:42:22 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:42:28 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:30 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:30 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2876464)[0;0m WARNING 12-28 00:42:30 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:30 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_3a3426ea'), local_subscribe_addr='ipc:///tmp/9e143f81-b632-4e96-8f57-451e4c93e37c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:42:37 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:42:37 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:42:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e0e6532e'), local_subscribe_addr='ipc:///tmp/2034bda6-a20f-4f2c-a05b-c1350a4bb0b3', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:42:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ca2e8791'), local_subscribe_addr='ipc:///tmp/f549806d-4210-4aee-805f-982d51fa2c3f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:42:43 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:42:43 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:42:43 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:42:43 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:42:43 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:42:43 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:42:43 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_9553971d'), local_subscribe_addr='ipc:///tmp/be078008-0ae2-4495-90c4-f2a99b4e0dd9', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:42:43 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:42:43 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:42:43 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:42:43 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:42:43 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:42:43 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 00:42:43 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:42:43 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:42:43 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2876800)[0;0m INFO 12-28 00:42:43 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:42:44 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2876800)[0;0m INFO 12-28 00:42:44 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:42:44 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2876800)[0;0m INFO 12-28 00:42:44 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:42:44 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2876800)[0;0m INFO 12-28 00:42:44 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2876799)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2876799)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.19s/it]
[1;36m(Worker_TP0 pid=2876799)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
[1;36m(Worker_TP0 pid=2876799)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.11it/s]
[1;36m(Worker_TP0 pid=2876799)[0;0m 
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:42:46 [default_loader.py:267] Loading weights took 1.83 seconds
[1;36m(Worker_TP1 pid=2876800)[0;0m INFO 12-28 00:42:46 [default_loader.py:267] Loading weights took 1.84 seconds
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:42:47 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 2.409321 seconds
[1;36m(Worker_TP1 pid=2876800)[0;0m INFO 12-28 00:42:47 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 2.629645 seconds
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:42:51 [gpu_worker.py:298] Available KV cache memory: 12.24 GiB
[1;36m(Worker_TP1 pid=2876800)[0;0m INFO 12-28 00:42:51 [gpu_worker.py:298] Available KV cache memory: 12.24 GiB
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:52 [kv_cache_utils.py:1087] GPU KV cache size: 200,576 tokens
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:52 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.22x
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:52 [kv_cache_utils.py:1087] GPU KV cache size: 200,576 tokens
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:52 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.22x
[1;36m(Worker_TP1 pid=2876800)[0;0m WARNING 12-28 00:42:52 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2876799)[0;0m WARNING 12-28 00:42:52 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:52 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.80 seconds
[1;36m(EngineCore_DP0 pid=2876464)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2876464)[0;0m 
[1;36m(EngineCore_DP0 pid=2876464)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2876464)[0;0m 
[1;36m(EngineCore_DP0 pid=2876464)[0;0m INFO 12-28 00:42:52 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:42:52 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1399.45it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<03:11,  1.93s/it, est. speed input: 80.68 toks/s, output: 0.52 toks/s]Processed prompts:   2%|â–         | 2/100 [00:03<02:50,  1.74s/it, est. speed input: 67.20 toks/s, output: 0.56 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:32,  2.82it/s, est. speed input: 666.34 toks/s, output: 4.64 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:03<00:13,  6.27it/s, est. speed input: 955.94 toks/s, output: 11.40 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:03<00:08,  9.69it/s, est. speed input: 1371.43 toks/s, output: 19.32 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:03<00:04, 15.74it/s, est. speed input: 1790.70 toks/s, output: 32.85 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:04<00:04, 16.70it/s, est. speed input: 2103.06 toks/s, output: 44.97 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:04<00:03, 19.70it/s, est. speed input: 2233.87 toks/s, output: 58.18 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:04<00:02, 24.29it/s, est. speed input: 2437.45 toks/s, output: 80.77 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:04<00:02, 20.54it/s, est. speed input: 2648.78 toks/s, output: 94.22 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:05<00:03, 16.58it/s, est. speed input: 2537.60 toks/s, output: 110.80 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:05<00:03, 13.49it/s, est. speed input: 2384.50 toks/s, output: 124.84 toks/s]WARNING 12-28 00:42:58 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:05<00:03, 13.61it/s, est. speed input: 2382.63 toks/s, output: 155.13 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:06<00:03, 12.54it/s, est. speed input: 2329.43 toks/s, output: 167.18 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:06<00:03, 10.68it/s, est. speed input: 2330.20 toks/s, output: 184.18 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:06<00:03,  9.87it/s, est. speed input: 2262.69 toks/s, output: 196.97 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:06<00:02, 11.90it/s, est. speed input: 2299.15 toks/s, output: 224.59 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:07<00:02, 11.85it/s, est. speed input: 2288.89 toks/s, output: 240.37 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:07<00:02, 10.54it/s, est. speed input: 2230.29 toks/s, output: 254.11 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:07<00:02,  9.70it/s, est. speed input: 2209.63 toks/s, output: 268.18 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:08<00:03,  7.33it/s, est. speed input: 2099.87 toks/s, output: 276.52 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:08<00:01, 10.70it/s, est. speed input: 2204.93 toks/s, output: 330.09 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:08<00:01, 11.63it/s, est. speed input: 2223.37 toks/s, output: 350.94 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:08<00:02,  8.24it/s, est. speed input: 2122.40 toks/s, output: 358.43 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:09<00:01,  8.28it/s, est. speed input: 2159.37 toks/s, output: 384.93 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:09<00:01,  9.20it/s, est. speed input: 2218.06 toks/s, output: 407.51 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:09<00:01,  7.96it/s, est. speed input: 2151.46 toks/s, output: 422.32 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:09<00:00,  8.36it/s, est. speed input: 2122.73 toks/s, output: 443.39 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:10<00:01,  3.77it/s, est. speed input: 1932.24 toks/s, output: 418.55 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:13<00:03,  1.56it/s, est. speed input: 1601.39 toks/s, output: 365.16 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:13<00:02,  1.78it/s, est. speed input: 1573.90 toks/s, output: 376.85 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  1.78it/s, est. speed input: 1676.50 toks/s, output: 470.70 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  7.40it/s, est. speed input: 1676.50 toks/s, output: 470.70 toks/s]
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:43:07 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2876799)[0;0m INFO 12-28 00:43:07 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2876800)[0;0m INFO 12-28 00:43:07 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:43:12] ax.service.ax_client: Completed trial 16 with data: {'throughput(token/s)': 468.167465, 'energy(J/token)': 1.271307}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:43:13] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 149.98584359884262), ObjectiveThreshold(throughput(token/s) >= 241.73357676870341)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:43:16] ax.service.ax_client: Generated new trial 17 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 4096, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:43:07', project_name='codecarbon', run_id='46458ba1-1261-4df8-a224-3f4fe0d109fb', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=45.66473376599606, emissions=0.0005335352476814249, emissions_rate=1.168374812859893e-05, cpu_power=30.111201255, gpu_power=137.48493403836963, ram_power=70.0, cpu_energy=0.0003693442858749053, gpu_energy=0.0010189330373693295, ram_energy=0.0008573453599667927, energy_consumed=0.0022456226832110278, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 7/30 completed. Throughput: 468.17, Energy: 1.271307. Pareto size: 4.
INFO 12-28 00:43:17 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 4096, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:43:18 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:43:18 [model.py:1510] Using max model len 32768
INFO 12-28 00:43:18 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=4096.
INFO 12-28 00:43:18 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:43:24 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:26 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:26 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2878536)[0;0m WARNING 12-28 00:43:26 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_6ef1e69f'), local_subscribe_addr='ipc:///tmp/f29edb3b-c9f7-4ad8-bcec-fc1b541aff5a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:43:32 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:43:33 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:43:33 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:43:33 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:43:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_711c1895'), local_subscribe_addr='ipc:///tmp/07c2dd1a-d37d-46a6-a5e1-2eefb2d01ae7', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:43:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2124cab8'), local_subscribe_addr='ipc:///tmp/cb69dc51-ec98-4686-84ba-95d614d935c5', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:43:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d2bac551'), local_subscribe_addr='ipc:///tmp/99d623bd-5a4d-40a1-8b3c-c39985b9de64', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:43:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_82a76554'), local_subscribe_addr='ipc:///tmp/b67b8264-a116-4efb-88d6-17978b579121', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:43:38 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:43:38 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:43:38 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:43:38 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:43:38 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:43:38 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:43:38 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:43:38 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:43:38 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:43:38 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:43:38 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:43:38 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:43:38 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_09532a08'), local_subscribe_addr='ipc:///tmp/1ee0cccc-8fc0-4cbb-ae76-5a49d59f72ee', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:43:38 [__init__.py:1384] Found nccl from library libnccl.so.2
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:43:38 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:43:38 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:43:38 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:43:38 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:43:38 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:43:38 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:43:38 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:43:38 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:43:38 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:43:38 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:43:38 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
WARNING 12-28 00:43:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:43:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:43:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:43:39 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2878874)[0;0m INFO 12-28 00:43:39 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:39 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:39 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:39 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2878874)[0;0m INFO 12-28 00:43:39 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:39 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:39 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:39 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2878874)[0;0m INFO 12-28 00:43:39 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2878874)[0;0m INFO 12-28 00:43:40 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2878871)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:40 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP0 pid=2878871)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.06it/s]
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:41 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP0 pid=2878871)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.84it/s]
[1;36m(Worker_TP0 pid=2878871)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.69it/s]
[1;36m(Worker_TP0 pid=2878871)[0;0m 
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:41 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:41 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.351989 seconds
[1;36m(Worker_TP3 pid=2878874)[0;0m INFO 12-28 00:43:41 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:42 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.676496 seconds
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:42 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.821180 seconds
[1;36m(Worker_TP3 pid=2878874)[0;0m INFO 12-28 00:43:42 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.136868 seconds
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:45 [gpu_worker.py:298] Available KV cache memory: 16.06 GiB
[1;36m(Worker_TP3 pid=2878874)[0;0m INFO 12-28 00:43:45 [gpu_worker.py:298] Available KV cache memory: 16.06 GiB
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:45 [gpu_worker.py:298] Available KV cache memory: 16.06 GiB
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:45 [gpu_worker.py:298] Available KV cache memory: 16.06 GiB
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [kv_cache_utils.py:1087] GPU KV cache size: 526,336 tokens
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.00x
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [kv_cache_utils.py:1087] GPU KV cache size: 526,336 tokens
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.00x
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [kv_cache_utils.py:1087] GPU KV cache size: 526,336 tokens
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.00x
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [kv_cache_utils.py:1087] GPU KV cache size: 526,336 tokens
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.00x
[1;36m(Worker_TP0 pid=2878871)[0;0m WARNING 12-28 00:43:46 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2878873)[0;0m WARNING 12-28 00:43:46 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2878874)[0;0m WARNING 12-28 00:43:46 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2878872)[0;0m WARNING 12-28 00:43:46 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.88 seconds
[1;36m(EngineCore_DP0 pid=2878536)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2878536)[0;0m 
[1;36m(EngineCore_DP0 pid=2878536)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2878536)[0;0m 
[1;36m(EngineCore_DP0 pid=2878536)[0;0m INFO 12-28 00:43:46 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:43:47 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1808.54it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<00:45,  2.17it/s, est. speed input: 339.33 toks/s, output: 2.17 toks/s]Processed prompts:   2%|â–         | 2/100 [00:00<00:43,  2.23it/s, est. speed input: 237.75 toks/s, output: 3.33 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:01<00:32,  2.95it/s, est. speed input: 590.51 toks/s, output: 4.48 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:02<00:34,  2.73it/s, est. speed input: 515.57 toks/s, output: 5.83 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:02<00:14,  6.18it/s, est. speed input: 1024.38 toks/s, output: 10.90 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.65it/s, est. speed input: 1553.10 toks/s, output: 24.39 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:02<00:02, 24.02it/s, est. speed input: 2967.05 toks/s, output: 55.99 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:02<00:01, 35.37it/s, est. speed input: 3588.64 toks/s, output: 103.84 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:03<00:01, 28.59it/s, est. speed input: 3569.23 toks/s, output: 135.59 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:03<00:01, 33.78it/s, est. speed input: 3868.38 toks/s, output: 191.11 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:03<00:01, 23.67it/s, est. speed input: 3771.83 toks/s, output: 220.60 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:03<00:01, 21.69it/s, est. speed input: 3838.64 toks/s, output: 257.87 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:04<00:01, 18.11it/s, est. speed input: 3614.64 toks/s, output: 295.21 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:04<00:02, 14.66it/s, est. speed input: 3431.42 toks/s, output: 322.90 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:05<00:02, 11.89it/s, est. speed input: 3177.49 toks/s, output: 348.53 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:01, 14.65it/s, est. speed input: 3169.34 toks/s, output: 419.15 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:05<00:01, 14.93it/s, est. speed input: 3121.60 toks/s, output: 463.78 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:05<00:01, 12.45it/s, est. speed input: 3148.03 toks/s, output: 484.39 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:05<00:01, 13.27it/s, est. speed input: 3103.96 toks/s, output: 517.96 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:06<00:01,  9.77it/s, est. speed input: 3105.02 toks/s, output: 531.84 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:06<00:01,  9.05it/s, est. speed input: 3187.03 toks/s, output: 558.00 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:01,  5.83it/s, est. speed input: 2921.16 toks/s, output: 557.56 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:07<00:01,  5.48it/s, est. speed input: 2826.94 toks/s, output: 566.95 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:02,  3.04it/s, est. speed input: 2589.80 toks/s, output: 531.24 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  3.04it/s, est. speed input: 2680.75 toks/s, output: 768.87 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.84it/s, est. speed input: 2680.75 toks/s, output: 768.87 toks/s]
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:56 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2878871)[0;0m INFO 12-28 00:43:56 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:56 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2878872)[0;0m INFO 12-28 00:43:56 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:56 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2878873)[0;0m INFO 12-28 00:43:56 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2878874)[0;0m INFO 12-28 00:43:56 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:44:01] ax.service.ax_client: Completed trial 17 with data: {'throughput(token/s)': 763.748124, 'energy(J/token)': 1.357389}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:44:03] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 68.0336092710495), ObjectiveThreshold(throughput(token/s) >= 241.7333996326782)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:44:05] ax.service.ax_client: Generated new trial 18 with parameters {'block_size': 128, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 4, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
[INFO 12-28 00:44:05] ax.service.ax_client: Completed trial 18 with data: {'throughput(token/s)': 0.0, 'energy(J/token)': 1000000000.0}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:44:07] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 168.94958800077438), ObjectiveThreshold(throughput(token/s) >= 437.46213527173825)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:44:09] ax.service.ax_client: Generated new trial 19 with parameters {'block_size': 128, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:43:56', project_name='codecarbon', run_id='902d5b23-9c8f-4a49-9135-5a56e9066d20', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=38.86612616607454, emissions=0.0005819345270405939, emissions_rate=1.4972794678687399e-05, cpu_power=30.71821209, gpu_power=271.2901392514612, ram_power=70.0, cpu_energy=0.00031472333577473217, gpu_energy=0.0014095761276582408, ram_energy=0.0007250333215048563, energy_consumed=0.002449332784937829, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 8/30 completed. Throughput: 763.75, Energy: 1.357389. Pareto size: 3.
Skipping trial: 16 GPUs required, only 4 available.
   BO Iteration 9/30 completed. Throughput: 0.00, Energy: 1000000000.000000. Pareto size: 3.
INFO 12-28 00:44:11 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:44:11 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:44:11 [model.py:1510] Using max model len 32768
INFO 12-28 00:44:11 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:44:11 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:44:18 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:21 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:21 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2880362)[0;0m WARNING 12-28 00:44:21 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_82b6acf8'), local_subscribe_addr='ipc:///tmp/04bbfdd2-3182-46f5-878b-60a3e27c76bd', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:44:27 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:44:27 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:44:27 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:44:27 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:44:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_701ea792'), local_subscribe_addr='ipc:///tmp/485c1e10-3b97-4f1e-a4a6-b2145f38e6bc', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:44:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a2dc7e3e'), local_subscribe_addr='ipc:///tmp/a925aa2f-3b63-4658-90cd-b1555a3d1b1b', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:44:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c3e615ea'), local_subscribe_addr='ipc:///tmp/b5e638ff-0912-4183-b1ad-81b7e2dfaca5', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:44:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b968e35c'), local_subscribe_addr='ipc:///tmp/32b76c66-45e4-41d0-a964-7dcc8f63e8b5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:44:33 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:44:33 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:44:33 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:44:33 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:44:33 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:44:33 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:44:33 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:44:33 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:44:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_b866dd0d'), local_subscribe_addr='ipc:///tmp/74172628-0779-4e78-83c3-7ef68e011dc4', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:44:33 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:44:33 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:44:33 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:44:33 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:44:33 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:44:33 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:44:33 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:44:33 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
WARNING 12-28 00:44:33 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:44:33 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:44:33 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:44:33 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:33 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:33 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2880625)[0;0m INFO 12-28 00:44:33 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:33 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:34 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:34 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2880625)[0;0m INFO 12-28 00:44:34 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:34 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:34 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:34 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2880625)[0;0m INFO 12-28 00:44:34 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:34 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:34 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:34 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2880625)[0;0m INFO 12-28 00:44:34 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:34 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2880622)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:35 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP0 pid=2880622)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.06it/s]
[1;36m(Worker_TP0 pid=2880622)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.84it/s]
[1;36m(Worker_TP0 pid=2880622)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.68it/s]
[1;36m(Worker_TP0 pid=2880622)[0;0m 
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:35 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP3 pid=2880625)[0;0m INFO 12-28 00:44:35 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:36 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.336195 seconds
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:36 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:36 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.621322 seconds
[1;36m(Worker_TP3 pid=2880625)[0;0m INFO 12-28 00:44:36 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.854454 seconds
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:36 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.228201 seconds
[1;36m(Worker_TP3 pid=2880625)[0;0m INFO 12-28 00:44:40 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:40 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:40 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:40 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(Worker_TP0 pid=2880622)[0;0m WARNING 12-28 00:44:41 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2880623)[0;0m WARNING 12-28 00:44:41 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2880624)[0;0m WARNING 12-28 00:44:41 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2880625)[0;0m WARNING 12-28 00:44:41 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.54 seconds
[1;36m(EngineCore_DP0 pid=2880362)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2880362)[0;0m 
[1;36m(EngineCore_DP0 pid=2880362)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2880362)[0;0m 
[1;36m(EngineCore_DP0 pid=2880362)[0;0m INFO 12-28 00:44:41 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:44:42 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1409.60it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:12,  1.34s/it, est. speed input: 116.29 toks/s, output: 0.75 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:59,  1.21s/it, est. speed input: 96.46 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.26it/s, est. speed input: 1052.03 toks/s, output: 9.69 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.32it/s, est. speed input: 1983.08 toks/s, output: 27.94 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 19.53it/s, est. speed input: 2733.95 toks/s, output: 50.97 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.31it/s, est. speed input: 3236.64 toks/s, output: 79.87 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 29.01it/s, est. speed input: 3622.24 toks/s, output: 118.88 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.16it/s, est. speed input: 3632.38 toks/s, output: 153.57 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.30it/s, est. speed input: 3511.58 toks/s, output: 195.00 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.73it/s, est. speed input: 3525.58 toks/s, output: 234.88 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 18.12it/s, est. speed input: 3286.76 toks/s, output: 267.94 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.48it/s, est. speed input: 3435.20 toks/s, output: 305.38 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.26it/s, est. speed input: 3401.19 toks/s, output: 340.42 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.38it/s, est. speed input: 3255.33 toks/s, output: 367.45 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:02, 14.91it/s, est. speed input: 3244.16 toks/s, output: 390.61 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.18it/s, est. speed input: 3189.16 toks/s, output: 397.54 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.72it/s, est. speed input: 3172.16 toks/s, output: 444.14 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 11.01it/s, est. speed input: 3071.01 toks/s, output: 469.30 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 12.05it/s, est. speed input: 3036.89 toks/s, output: 502.45 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.92it/s, est. speed input: 2950.48 toks/s, output: 527.27 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.31it/s, est. speed input: 3046.26 toks/s, output: 558.74 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.67it/s, est. speed input: 2983.47 toks/s, output: 586.48 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.09it/s, est. speed input: 2844.57 toks/s, output: 601.73 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.65it/s, est. speed input: 2788.24 toks/s, output: 613.13 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.54it/s, est. speed input: 2704.10 toks/s, output: 619.13 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.89it/s, est. speed input: 2493.96 toks/s, output: 598.27 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.28it/s, est. speed input: 2466.81 toks/s, output: 642.79 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.06it/s, est. speed input: 2339.70 toks/s, output: 638.69 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.06it/s, est. speed input: 2623.08 toks/s, output: 844.86 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.58it/s, est. speed input: 2623.08 toks/s, output: 844.86 toks/s]
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:51 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2880622)[0;0m INFO 12-28 00:44:51 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:51 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:51 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2880623)[0;0m INFO 12-28 00:44:51 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2880624)[0;0m INFO 12-28 00:44:51 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2880625)[0;0m INFO 12-28 00:44:51 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:44:56] ax.service.ax_client: Completed trial 19 with data: {'throughput(token/s)': 837.84131, 'energy(J/token)': 1.278092}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:44:58] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 59.45516961812973), ObjectiveThreshold(throughput(token/s) >= 431.20036104242075)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:

Optimization failed in `gen_candidates_scipy` with the following warning(s):
[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .')]
Because you specified `batch_initial_conditions` larger than required `num_restarts`, optimization will not be retried with new initial conditions and will proceed with the current solution. Suggested remediation: Try again with different `batch_initial_conditions`, don't provide `batch_initial_conditions`, or increase `num_restarts`.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:45:01] ax.service.ax_client: Generated new trial 20 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 2, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:44:51', project_name='codecarbon', run_id='38bf64ad-5300-496f-a2c1-3ec98a9dac15', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.84373297507409, emissions=0.0006153345373989788, emissions_rate=1.5065580263549908e-05, cpu_power=30.173961760909094, gpu_power=259.06551365955994, ram_power=70.0, cpu_energy=0.0003290036605730338, gpu_energy=0.001497446475733355, ram_energy=0.0007634615910840997, energy_consumed=0.0025899117273904884, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 10/30 completed. Throughput: 837.84, Energy: 1.278092. Pareto size: 2.
INFO 12-28 00:45:02 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:45:03 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:45:03 [model.py:1510] Using max model len 32768
INFO 12-28 00:45:03 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:45:03 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:45:09 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:12 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:12 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2881712)[0;0m WARNING 12-28 00:45:12 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_d469b7b6'), local_subscribe_addr='ipc:///tmp/bda52650-bbf0-4b38-bc12-aa64445f1376', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:45:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:45:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:45:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:45:24 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:45:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d9d8b8eb'), local_subscribe_addr='ipc:///tmp/6a673867-1fd9-41e3-a27f-e27735fb2f56', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:45:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_29cd29a2'), local_subscribe_addr='ipc:///tmp/05d6fc15-89b9-42a3-ac9e-891406e124b1', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:45:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_89952178'), local_subscribe_addr='ipc:///tmp/0bfbdc33-05e7-4fad-882a-55b7de68593c', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:45:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d9e9971b'), local_subscribe_addr='ipc:///tmp/71d1e70f-7568-4d66-9227-85052ce1e865', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:45:31 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:31 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:31 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:45:31 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:31 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:45:31 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:45:31 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:31 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:45:31 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:45:31 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:45:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_eca234b8'), local_subscribe_addr='ipc:///tmp/5f213578-7dfc-4e90-a4c2-e4616c042094', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING 12-28 00:45:31 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:45:31 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:45:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_82b8acd8'), local_subscribe_addr='ipc:///tmp/0868260d-477b-4e4f-b178-daf4e3f77b90', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:45:31 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:31 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:45:31 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:31 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:45:31 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:31 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:45:31 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:31 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:45:32 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:32 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:32 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:45:32 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:45:32 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:32 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:45:32 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:45:32 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:45:32 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:45:32 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:45:32 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
INFO 12-28 00:45:32 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
WARNING 12-28 00:45:32 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:45:32 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:45:32 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:45:32 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:32 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:32 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:32 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:32 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:32 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:32 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:33 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.07it/s]
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.00it/s]
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m 
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:34 [default_loader.py:267] Loading weights took 1.00 seconds
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:34 [default_loader.py:267] Loading weights took 1.15 seconds
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:34 [default_loader.py:267] Loading weights took 0.98 seconds
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:34 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 1.551326 seconds
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:35 [default_loader.py:267] Loading weights took 1.16 seconds
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:35 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 1.890324 seconds
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:35 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.016424 seconds
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:35 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.365958 seconds
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:37 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:37 [gpu_worker.py:298] Available KV cache memory: 15.52 GiB
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:39 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:39 [gpu_worker.py:298] Available KV cache memory: 15.63 GiB
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [kv_cache_utils.py:1087] GPU KV cache size: 512,320 tokens
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.21x
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [kv_cache_utils.py:1087] GPU KV cache size: 512,320 tokens
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.21x
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [kv_cache_utils.py:1087] GPU KV cache size: 508,672 tokens
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.99x
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [kv_cache_utils.py:1087] GPU KV cache size: 508,672 tokens
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 30.99x
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m WARNING 12-28 00:45:39 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m WARNING 12-28 00:45:39 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:39 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.99 seconds
[1;36m(EngineCore_DP0 pid=2881712)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2881712)[0;0m 
[1;36m(EngineCore_DP0 pid=2881712)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2881712)[0;0m 
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:40 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=2881712)[0;0m INFO 12-28 00:45:40 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:45:40 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(Worker_PP0_TP1 pid=2882789)[0;0m WARNING 12-28 00:45:40 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m WARNING 12-28 00:45:40 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1426.01it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(Worker_PP0_TP1 pid=2882789)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m 
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m 
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m 
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m 
Processed prompts:   1%|          | 1/100 [00:02<03:25,  2.07s/it, est. speed input: 75.26 toks/s, output: 0.48 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:02<01:20,  1.21it/s, est. speed input: 210.91 toks/s, output: 1.05 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:02<00:19,  4.72it/s, est. speed input: 966.28 toks/s, output: 6.74 toks/s]Processed prompts:  14%|â–ˆâ–        | 14/100 [00:03<00:10,  8.27it/s, est. speed input: 1219.09 toks/s, output: 13.36 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:03<00:05, 14.77it/s, est. speed input: 2130.13 toks/s, output: 29.53 toks/s]Processed prompts:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:03<00:03, 19.02it/s, est. speed input: 2321.59 toks/s, output: 43.10 toks/s]Processed prompts:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:03<00:02, 22.86it/s, est. speed input: 2638.49 toks/s, output: 58.74 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:03<00:03, 19.16it/s, est. speed input: 2657.00 toks/s, output: 74.58 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:04<00:03, 19.08it/s, est. speed input: 2661.34 toks/s, output: 94.84 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:04<00:03, 16.32it/s, est. speed input: 2638.06 toks/s, output: 107.80 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:04<00:03, 15.88it/s, est. speed input: 2751.39 toks/s, output: 124.70 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:04<00:03, 13.53it/s, est. speed input: 2611.73 toks/s, output: 141.19 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:04<00:03, 13.78it/s, est. speed input: 2569.15 toks/s, output: 155.16 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:05<00:03, 13.99it/s, est. speed input: 2603.48 toks/s, output: 169.94 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:05<00:02, 18.26it/s, est. speed input: 2640.86 toks/s, output: 205.02 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:05<00:03, 10.56it/s, est. speed input: 2582.22 toks/s, output: 215.43 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:06<00:04,  7.64it/s, est. speed input: 2390.87 toks/s, output: 220.70 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:06<00:04,  7.50it/s, est. speed input: 2452.77 toks/s, output: 235.88 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:06<00:04,  7.26it/s, est. speed input: 2363.45 toks/s, output: 251.16 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:07<00:03,  8.49it/s, est. speed input: 2482.40 toks/s, output: 281.22 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:07<00:04,  6.84it/s, est. speed input: 2363.30 toks/s, output: 291.15 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:07<00:03,  7.84it/s, est. speed input: 2353.19 toks/s, output: 322.96 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:07<00:02,  9.08it/s, est. speed input: 2432.89 toks/s, output: 347.37 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:08<00:01, 11.76it/s, est. speed input: 2443.70 toks/s, output: 386.61 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:08<00:01, 12.46it/s, est. speed input: 2421.66 toks/s, output: 409.98 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:08<00:02,  7.95it/s, est. speed input: 2295.05 toks/s, output: 415.96 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:08<00:01,  8.22it/s, est. speed input: 2259.46 toks/s, output: 437.38 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:09<00:01,  9.48it/s, est. speed input: 2240.46 toks/s, output: 463.30 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:10<00:02,  4.23it/s, est. speed input: 2028.49 toks/s, output: 444.50 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:11<00:03,  2.67it/s, est. speed input: 1844.54 toks/s, output: 421.21 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:11<00:03,  2.37it/s, est. speed input: 1754.50 toks/s, output: 418.09 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:13<00:04,  1.70it/s, est. speed input: 1600.09 toks/s, output: 399.81 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  1.70it/s, est. speed input: 1732.77 toks/s, output: 536.38 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  7.65it/s, est. speed input: 1732.77 toks/s, output: 536.38 toks/s]
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:54 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP0 pid=2882788)[0;0m INFO 12-28 00:45:54 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:54 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP1 pid=2882789)[0;0m INFO 12-28 00:45:54 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:54 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP0 pid=2882790)[0;0m INFO 12-28 00:45:54 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:54 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP1 pid=2882791)[0;0m INFO 12-28 00:45:54 [multiproc_executor.py:599] WorkerProc shutting down.
[INFO 12-28 00:45:59] ax.service.ax_client: Completed trial 20 with data: {'throughput(token/s)': 533.456251, 'energy(J/token)': 1.740927}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:46:02] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 155.82689130306244), ObjectiveThreshold(throughput(token/s) >= 768.960208018896)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:46:03] ax.service.ax_client: Generated new trial 21 with parameters {'block_size': 64, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:45:54', project_name='codecarbon', run_id='1e831636-fa12-4537-b723-25db4ddca70c', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=52.00012490898371, emissions=0.0008055348477821021, emissions_rate=1.5491017554131595e-05, cpu_power=30.51214548857143, gpu_power=269.1537073525307, ram_power=70.0, cpu_energy=0.00042065115639128325, gpu_energy=0.0020001543778986886, ram_energy=0.0009696496121271695, energy_consumed=0.003390455146417141, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 11/30 completed. Throughput: 533.46, Energy: 1.740927. Pareto size: 2.
INFO 12-28 00:46:05 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 64, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:46:05 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:46:05 [model.py:1510] Using max model len 32768
INFO 12-28 00:46:05 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:46:05 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:46:11 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:14 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:14 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2885770)[0;0m WARNING 12-28 00:46:14 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:14 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_2261d2da'), local_subscribe_addr='ipc:///tmp/fe604775-df8c-42ce-b4b4-666cb7752f12', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:46:21 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:46:21 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:46:21 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:46:21 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:46:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cbe1fe7d'), local_subscribe_addr='ipc:///tmp/a65637aa-a0b7-4726-866b-d705ea23e3aa', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:46:26 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b43388e1'), local_subscribe_addr='ipc:///tmp/5ff8da5c-9f05-4a02-9b5d-cafc7c0f2eb3', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:46:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1a170e67'), local_subscribe_addr='ipc:///tmp/70b32394-f522-41dc-b792-14d30a065626', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:46:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cb10119f'), local_subscribe_addr='ipc:///tmp/c716bbba-a087-4720-9bf9-440bd3a6e166', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:46:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:46:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:46:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:46:28 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:46:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:46:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:46:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:46:28 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:46:28 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:46:28 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:46:28 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:46:28 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:46:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_fc0245fe'), local_subscribe_addr='ipc:///tmp/a9fd6485-076e-4a85-9cee-8c593f8429f7', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:46:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:46:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:46:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:46:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:46:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:46:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:46:28 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:46:28 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:46:28 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:46:28 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:46:28 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:46:28 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 00:46:28 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:46:28 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:46:28 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:46:28 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP3 pid=2886248)[0;0m INFO 12-28 00:46:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:29 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2886248)[0;0m INFO 12-28 00:46:29 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:29 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:29 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:29 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2886248)[0;0m INFO 12-28 00:46:29 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:29 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:29 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:29 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:29 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:29 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:29 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2886248)[0;0m INFO 12-28 00:46:29 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2886245)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2886245)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.79it/s]
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:30 [default_loader.py:267] Loading weights took 0.88 seconds
[1;36m(Worker_TP0 pid=2886245)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.45it/s]
[1;36m(Worker_TP0 pid=2886245)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.32it/s]
[1;36m(Worker_TP0 pid=2886245)[0;0m 
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:31 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP3 pid=2886248)[0;0m INFO 12-28 00:46:31 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:31 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.488136 seconds
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:31 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:31 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.746304 seconds
[1;36m(Worker_TP3 pid=2886248)[0;0m INFO 12-28 00:46:31 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.906142 seconds
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:32 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.265650 seconds
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:36 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:36 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2886248)[0;0m INFO 12-28 00:46:36 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:36 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:36 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:36 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:36 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:36 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:36 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:36 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:36 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:36 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(Worker_TP0 pid=2886245)[0;0m WARNING 12-28 00:46:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2886247)[0;0m WARNING 12-28 00:46:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2886246)[0;0m WARNING 12-28 00:46:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2886248)[0;0m WARNING 12-28 00:46:36 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:37 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.80 seconds
[1;36m(EngineCore_DP0 pid=2885770)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2885770)[0;0m 
[1;36m(EngineCore_DP0 pid=2885770)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2885770)[0;0m 
[1;36m(EngineCore_DP0 pid=2885770)[0;0m INFO 12-28 00:46:37 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:46:37 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1804.71it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:13,  1.35s/it, est. speed input: 115.33 toks/s, output: 0.74 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:59,  1.22s/it, est. speed input: 96.03 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.24it/s, est. speed input: 1047.67 toks/s, output: 9.65 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.31it/s, est. speed input: 1976.64 toks/s, output: 27.84 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 20.96it/s, est. speed input: 2925.16 toks/s, output: 58.06 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:02<00:02, 25.32it/s, est. speed input: 3445.95 toks/s, output: 91.14 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:03<00:02, 29.42it/s, est. speed input: 3771.43 toks/s, output: 125.22 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:03<00:02, 22.67it/s, est. speed input: 3699.47 toks/s, output: 162.27 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:03<00:02, 21.02it/s, est. speed input: 3505.01 toks/s, output: 205.43 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:04<00:02, 20.79it/s, est. speed input: 3481.64 toks/s, output: 244.37 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 17.62it/s, est. speed input: 3287.94 toks/s, output: 268.04 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.11it/s, est. speed input: 3436.79 toks/s, output: 305.52 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 18.99it/s, est. speed input: 3402.91 toks/s, output: 340.60 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.16it/s, est. speed input: 3256.95 toks/s, output: 367.63 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:02, 14.68it/s, est. speed input: 3244.11 toks/s, output: 390.60 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.03it/s, est. speed input: 3188.54 toks/s, output: 397.46 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.59it/s, est. speed input: 3170.80 toks/s, output: 443.95 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.95it/s, est. speed input: 3071.23 toks/s, output: 469.33 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.99it/s, est. speed input: 3036.65 toks/s, output: 502.41 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.87it/s, est. speed input: 2950.00 toks/s, output: 527.18 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.27it/s, est. speed input: 3045.47 toks/s, output: 558.60 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.62it/s, est. speed input: 2982.24 toks/s, output: 586.24 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.05it/s, est. speed input: 2842.60 toks/s, output: 601.31 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.59it/s, est. speed input: 2785.38 toks/s, output: 612.51 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.52it/s, est. speed input: 2702.13 toks/s, output: 618.68 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.88it/s, est. speed input: 2492.43 toks/s, output: 597.90 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.27it/s, est. speed input: 2465.24 toks/s, output: 642.38 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.05it/s, est. speed input: 2337.27 toks/s, output: 638.02 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.05it/s, est. speed input: 2619.91 toks/s, output: 843.84 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.57it/s, est. speed input: 2619.91 toks/s, output: 843.84 toks/s]
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:47 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2886245)[0;0m INFO 12-28 00:46:47 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:47 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2886246)[0;0m INFO 12-28 00:46:47 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:47 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2886247)[0;0m INFO 12-28 00:46:47 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2886248)[0;0m INFO 12-28 00:46:47 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:46:52] ax.service.ax_client: Completed trial 21 with data: {'throughput(token/s)': 838.347426, 'energy(J/token)': 1.32338}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:46:56] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 31817476.497276187), ObjectiveThreshold(throughput(token/s) >= 791.928487994483)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

[INFO 12-28 00:46:57] ax.service.ax_client: Generated new trial 22 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 4096, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 2, 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:46:47', project_name='codecarbon', run_id='8651b567-4d53-4962-8261-ec44ec3c200d', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=42.287793638999574, emissions=0.0006371382492489735, emissions_rate=1.5066717707905621e-05, cpu_power=30.76363591153847, gpu_power=242.7695671805883, ram_power=70.0, cpu_energy=0.00034357472035848003, gpu_energy=0.0015465426261220827, ram_energy=0.0007915650935691602, energy_consumed=0.0026816824400497227, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 12/30 completed. Throughput: 838.35, Energy: 1.323380. Pareto size: 1.
INFO 12-28 00:46:59 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 2, 'tensor_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:47:00 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:47:00 [model.py:1510] Using max model len 32768
INFO 12-28 00:47:00 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=4096.
INFO 12-28 00:47:00 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:47:06 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:08 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:08 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2888025)[0;0m WARNING 12-28 00:47:08 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_455e93b7'), local_subscribe_addr='ipc:///tmp/556a7b11-cfd4-4eff-8d44-119988b7b7dc', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:47:15 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:47:15 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:47:15 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:47:15 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:47:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b3b7a57f'), local_subscribe_addr='ipc:///tmp/bbb3e8e8-3c74-45b4-81cd-3d42e02e6c4e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:47:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c4f45656'), local_subscribe_addr='ipc:///tmp/1a638d2c-949b-4a0c-b79f-9e4da523a1f8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:47:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c3f220c9'), local_subscribe_addr='ipc:///tmp/8f3440ec-370f-459b-81e3-09cf6322103b', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:47:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_768900b1'), local_subscribe_addr='ipc:///tmp/54cef144-ae63-4a31-8fab-136f4e2c3520', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:47:20 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:20 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:20 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:20 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:47:20 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:47:20 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:47:20 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:20 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:47:21 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:47:21 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:47:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_f398edc1'), local_subscribe_addr='ipc:///tmp/90cee09c-7574-48c8-bcd5-b03b33d70db0', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
WARNING 12-28 00:47:21 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:47:21 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:47:21 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_92eced75'), local_subscribe_addr='ipc:///tmp/cf9137d1-981a-4e53-a37f-8c76eab9c3a6', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:47:21 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:21 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:47:21 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:21 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:47:21 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:21 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:21 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:47:21 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:47:21 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:21 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:21 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:47:21 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:47:21 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:21 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:47:21 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:47:21 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:47:21 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
INFO 12-28 00:47:21 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 1, TP rank 1, EP rank 1
INFO 12-28 00:47:21 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:47:21 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 00:47:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:47:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:47:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:47:21 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:21 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:21 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m INFO 12-28 00:47:21 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:21 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:21 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:21 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m INFO 12-28 00:47:21 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:21 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:21 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:21 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m INFO 12-28 00:47:21 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:21 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:22 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m INFO 12-28 00:47:22 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:22 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:22 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:23 [default_loader.py:267] Loading weights took 0.96 seconds
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m INFO 12-28 00:47:23 [default_loader.py:267] Loading weights took 0.98 seconds
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.08it/s]
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.06it/s]
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m 
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:23 [default_loader.py:267] Loading weights took 0.97 seconds
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:24 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 1.518753 seconds
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:24 [default_loader.py:267] Loading weights took 0.96 seconds
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m INFO 12-28 00:47:24 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 1.732707 seconds
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:24 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 1.972385 seconds
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:24 [gpu_model_runner.py:2653] Model loading took 3.3812 GiB and 2.239524 seconds
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:26 [gpu_worker.py:298] Available KV cache memory: 16.00 GiB
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m INFO 12-28 00:47:26 [gpu_worker.py:298] Available KV cache memory: 16.00 GiB
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:27 [gpu_worker.py:298] Available KV cache memory: 16.15 GiB
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:27 [gpu_worker.py:298] Available KV cache memory: 16.15 GiB
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [kv_cache_utils.py:1087] GPU KV cache size: 529,216 tokens
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.35x
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [kv_cache_utils.py:1087] GPU KV cache size: 529,216 tokens
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.35x
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [kv_cache_utils.py:1087] GPU KV cache size: 524,320 tokens
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 63.75x
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [kv_cache_utils.py:1087] GPU KV cache size: 524,320 tokens
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 63.75x
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m WARNING 12-28 00:47:27 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m WARNING 12-28 00:47:27 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:27 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.97 seconds
[1;36m(EngineCore_DP0 pid=2888025)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2888025)[0;0m 
[1;36m(EngineCore_DP0 pid=2888025)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2888025)[0;0m 
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:28 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=2888025)[0;0m INFO 12-28 00:47:28 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:47:28 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(Worker_PP0_TP0 pid=2888382)[0;0m WARNING 12-28 00:47:28 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m WARNING 12-28 00:47:28 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1828.71it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][1;36m(Worker_PP0_TP0 pid=2888382)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m 
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m 
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m 
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m 
Processed prompts:   1%|          | 1/100 [00:00<01:18,  1.26it/s, est. speed input: 196.11 toks/s, output: 1.26 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:01<00:31,  3.11it/s, est. speed input: 374.06 toks/s, output: 2.71 toks/s]Processed prompts:   4%|â–         | 4/100 [00:01<00:41,  2.31it/s, est. speed input: 507.83 toks/s, output: 2.28 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:02<00:37,  2.53it/s, est. speed input: 652.15 toks/s, output: 2.42 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:02<00:31,  3.02it/s, est. speed input: 614.96 toks/s, output: 3.53 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:07, 11.10it/s, est. speed input: 1668.79 toks/s, output: 13.50 toks/s]Processed prompts:  21%|â–ˆâ–ˆ        | 21/100 [00:02<00:04, 19.74it/s, est. speed input: 2131.15 toks/s, output: 32.51 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:02<00:03, 21.34it/s, est. speed input: 2354.33 toks/s, output: 46.04 toks/s]Processed prompts:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:02<00:02, 23.69it/s, est. speed input: 2719.63 toks/s, output: 61.91 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:03<00:03, 21.06it/s, est. speed input: 2688.56 toks/s, output: 78.73 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:03<00:03, 21.06it/s, est. speed input: 2596.16 toks/s, output: 94.08 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 23.93it/s, est. speed input: 2824.80 toks/s, output: 118.36 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:03<00:02, 23.17it/s, est. speed input: 2904.65 toks/s, output: 140.80 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:03<00:02, 20.16it/s, est. speed input: 3011.10 toks/s, output: 158.65 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:03<00:02, 21.54it/s, est. speed input: 3099.04 toks/s, output: 180.53 toks/s]Processed prompts:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:04<00:03, 15.50it/s, est. speed input: 2884.17 toks/s, output: 194.27 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:04<00:02, 15.77it/s, est. speed input: 2889.13 toks/s, output: 217.36 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:04<00:02, 15.10it/s, est. speed input: 2928.01 toks/s, output: 231.80 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:04<00:02, 15.74it/s, est. speed input: 2958.21 toks/s, output: 248.53 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:02, 16.31it/s, est. speed input: 2950.41 toks/s, output: 266.63 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:04<00:02, 12.65it/s, est. speed input: 2882.27 toks/s, output: 276.14 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:05<00:03, 11.23it/s, est. speed input: 2776.19 toks/s, output: 288.71 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:05<00:03,  9.04it/s, est. speed input: 2765.62 toks/s, output: 296.87 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:04,  6.95it/s, est. speed input: 2608.07 toks/s, output: 301.79 toks/s]WARNING 12-28 00:47:34 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:06<00:04,  6.94it/s, est. speed input: 2684.47 toks/s, output: 308.99 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:06<00:04,  6.77it/s, est. speed input: 2625.03 toks/s, output: 315.84 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:06<00:03,  6.95it/s, est. speed input: 2684.94 toks/s, output: 332.05 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:06<00:04,  5.98it/s, est. speed input: 2593.28 toks/s, output: 335.05 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:06<00:03,  6.41it/s, est. speed input: 2562.57 toks/s, output: 345.21 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:07<00:03,  6.39it/s, est. speed input: 2512.82 toks/s, output: 353.61 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:07<00:02,  8.32it/s, est. speed input: 2613.77 toks/s, output: 379.49 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:07<00:03,  6.14it/s, est. speed input: 2530.95 toks/s, output: 380.42 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:07<00:02,  6.42it/s, est. speed input: 2523.36 toks/s, output: 390.46 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:07<00:02,  6.45it/s, est. speed input: 2538.74 toks/s, output: 399.70 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:08<00:03,  5.55it/s, est. speed input: 2471.41 toks/s, output: 404.79 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:08<00:01,  8.76it/s, est. speed input: 2472.35 toks/s, output: 447.47 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:08<00:02,  5.84it/s, est. speed input: 2362.11 toks/s, output: 445.00 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:08<00:02,  5.73it/s, est. speed input: 2319.30 toks/s, output: 453.63 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:09<00:03,  3.61it/s, est. speed input: 2209.97 toks/s, output: 443.42 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:09<00:02,  4.10it/s, est. speed input: 2176.69 toks/s, output: 455.19 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:10<00:02,  3.01it/s, est. speed input: 2130.92 toks/s, output: 448.73 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:10<00:02,  3.02it/s, est. speed input: 2066.63 toks/s, output: 454.08 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:12<00:05,  1.29it/s, est. speed input: 1756.08 toks/s, output: 405.50 toks/s]Processed prompts:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:12<00:00,  3.62it/s, est. speed input: 1800.75 toks/s, output: 502.70 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  3.62it/s, est. speed input: 1809.10 toks/s, output: 542.91 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:12<00:00,  7.99it/s, est. speed input: 1809.10 toks/s, output: 542.91 toks/s]
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:41 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP0 pid=2888382)[0;0m INFO 12-28 00:47:41 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:41 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0_TP1 pid=2888383)[0;0m INFO 12-28 00:47:41 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:41 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP1_TP0 pid=2888384)[0;0m INFO 12-28 00:47:41 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1_TP1 pid=2888385)[0;0m INFO 12-28 00:47:41 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:47:46] ax.service.ax_client: Completed trial 22 with data: {'throughput(token/s)': 540.497272, 'energy(J/token)': 1.494059}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:47:50] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 30434109.897948265), ObjectiveThreshold(throughput(token/s) >= 792.6497335495965)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:47:52] ax.service.ax_client: Generated new trial 23 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 4096, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:47:41', project_name='codecarbon', run_id='c06e3a0f-d892-4490-b560-ffe802f2562a', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=42.298193274997175, emissions=0.0006702066617045668, emissions_rate=1.584480588443317e-05, cpu_power=30.38672685692308, gpu_power=271.5595253019625, ram_power=70.0, cpu_energy=0.00034499620875124207, gpu_energy=0.0016841382917522907, ram_energy=0.0007917312033151069, energy_consumed=0.0028208657038186397, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 13/30 completed. Throughput: 540.50, Energy: 1.494059. Pareto size: 1.
INFO 12-28 00:47:54 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:47:54 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:47:54 [model.py:1510] Using max model len 32768
INFO 12-28 00:47:54 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=4096.
INFO 12-28 00:47:54 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:48:01 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:04 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:04 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2890106)[0;0m WARNING 12-28 00:48:04 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:04 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_fc594ab8'), local_subscribe_addr='ipc:///tmp/7e945d18-5229-4374-a57f-352a026d276e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:48:10 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:48:10 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:48:10 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:48:10 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:48:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ae9a7219'), local_subscribe_addr='ipc:///tmp/aa9a5986-a8fa-4eb4-b3b2-4b84cdcb10c9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:48:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3190b415'), local_subscribe_addr='ipc:///tmp/0288e22e-ce9a-4544-b37a-27631235460b', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:48:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_321776c1'), local_subscribe_addr='ipc:///tmp/237e9a21-728e-44c3-a7bd-4d0c7fef0ed0', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:48:15 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1c2f93f9'), local_subscribe_addr='ipc:///tmp/be3f69b8-76d2-490a-80ec-3aac057d98f6', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:48:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:48:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:48:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:48:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:48:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:48:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:48:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:48:16 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:48:16 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:48:16 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:48:16 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:48:16 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:48:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2adb6e57'), local_subscribe_addr='ipc:///tmp/078ed81b-90c3-4600-89f2-c4f5f137ceee', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:48:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:48:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:48:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:48:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:48:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:48:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:48:16 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:48:16 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:48:16 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:48:16 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:48:16 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:48:16 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 00:48:17 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:48:17 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:48:17 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:48:17 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:17 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2890493)[0;0m INFO 12-28 00:48:17 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:17 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:17 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:17 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2890493)[0;0m INFO 12-28 00:48:17 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:17 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:17 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:17 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2890493)[0;0m INFO 12-28 00:48:17 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:17 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:17 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:17 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:17 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2890493)[0;0m INFO 12-28 00:48:17 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:17 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2890490)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:18 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:19 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP3 pid=2890493)[0;0m INFO 12-28 00:48:19 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP0 pid=2890490)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.05it/s]
[1;36m(Worker_TP0 pid=2890490)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.82it/s]
[1;36m(Worker_TP0 pid=2890490)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.67it/s]
[1;36m(Worker_TP0 pid=2890490)[0;0m 
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:19 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.685306 seconds
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:19 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:19 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.611598 seconds
[1;36m(Worker_TP3 pid=2890493)[0;0m INFO 12-28 00:48:19 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.991667 seconds
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:20 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.264117 seconds
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:23 [gpu_worker.py:298] Available KV cache memory: 16.06 GiB
[1;36m(Worker_TP3 pid=2890493)[0;0m INFO 12-28 00:48:23 [gpu_worker.py:298] Available KV cache memory: 16.06 GiB
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:23 [gpu_worker.py:298] Available KV cache memory: 16.06 GiB
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:23 [gpu_worker.py:298] Available KV cache memory: 16.06 GiB
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:23 [kv_cache_utils.py:1087] GPU KV cache size: 526,336 tokens
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.00x
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:23 [kv_cache_utils.py:1087] GPU KV cache size: 526,336 tokens
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.00x
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:23 [kv_cache_utils.py:1087] GPU KV cache size: 526,336 tokens
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.00x
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:23 [kv_cache_utils.py:1087] GPU KV cache size: 526,336 tokens
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:23 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 64.00x
[1;36m(Worker_TP0 pid=2890490)[0;0m WARNING 12-28 00:48:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2890493)[0;0m WARNING 12-28 00:48:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2890491)[0;0m WARNING 12-28 00:48:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2890492)[0;0m WARNING 12-28 00:48:23 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:24 [core.py:210] init engine (profile, create kv cache, warmup model) took 3.75 seconds
[1;36m(EngineCore_DP0 pid=2890106)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2890106)[0;0m 
[1;36m(EngineCore_DP0 pid=2890106)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2890106)[0;0m 
[1;36m(EngineCore_DP0 pid=2890106)[0;0m INFO 12-28 00:48:24 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:48:24 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1248.67it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:00<00:43,  2.30it/s, est. speed input: 358.42 toks/s, output: 2.30 toks/s]Processed prompts:   2%|â–         | 2/100 [00:00<00:42,  2.28it/s, est. speed input: 244.52 toks/s, output: 3.43 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:01<00:31,  2.98it/s, est. speed input: 598.88 toks/s, output: 4.55 toks/s]Processed prompts:   6%|â–Œ         | 6/100 [00:02<00:34,  2.75it/s, est. speed input: 520.74 toks/s, output: 5.89 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:02<00:14,  6.22it/s, est. speed input: 1034.84 toks/s, output: 11.01 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.70it/s, est. speed input: 1567.88 toks/s, output: 24.63 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 23.82it/s, est. speed input: 2807.35 toks/s, output: 53.04 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:02<00:01, 35.70it/s, est. speed input: 3595.86 toks/s, output: 97.36 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:01, 29.84it/s, est. speed input: 3624.97 toks/s, output: 130.51 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:03<00:01, 32.15it/s, est. speed input: 3761.28 toks/s, output: 176.83 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:03<00:01, 30.06it/s, est. speed input: 3907.24 toks/s, output: 214.29 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:03<00:01, 20.60it/s, est. speed input: 3628.51 toks/s, output: 248.47 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:04<00:01, 18.99it/s, est. speed input: 3715.45 toks/s, output: 288.46 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:04<00:02, 14.71it/s, est. speed input: 3446.47 toks/s, output: 312.52 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:04<00:01, 14.36it/s, est. speed input: 3359.85 toks/s, output: 349.53 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:05<00:02, 12.02it/s, est. speed input: 3203.50 toks/s, output: 367.97 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:05<00:01, 15.19it/s, est. speed input: 3187.75 toks/s, output: 439.47 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:05<00:01, 12.94it/s, est. speed input: 3072.16 toks/s, output: 473.90 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:05<00:01, 13.61it/s, est. speed input: 3172.71 toks/s, output: 507.70 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01,  9.91it/s, est. speed input: 3028.94 toks/s, output: 518.82 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01, 10.02it/s, est. speed input: 3232.46 toks/s, output: 550.45 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:06<00:01,  6.41it/s, est. speed input: 2993.57 toks/s, output: 549.26 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:01,  5.89it/s, est. speed input: 2922.79 toks/s, output: 557.86 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:07<00:01,  5.46it/s, est. speed input: 2827.49 toks/s, output: 567.06 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:02,  2.86it/s, est. speed input: 2585.28 toks/s, output: 530.31 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  2.86it/s, est. speed input: 2675.67 toks/s, output: 767.41 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.81it/s, est. speed input: 2675.67 toks/s, output: 767.41 toks/s]
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:33 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2890490)[0;0m INFO 12-28 00:48:33 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:33 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:33 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2890491)[0;0m INFO 12-28 00:48:33 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2890492)[0;0m INFO 12-28 00:48:33 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2890493)[0;0m INFO 12-28 00:48:33 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:48:39] ax.service.ax_client: Completed trial 23 with data: {'throughput(token/s)': 760.138278, 'energy(J/token)': 1.396464}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:48:43] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 29165956.797703236), ObjectiveThreshold(throughput(token/s) >= 794.2285114633034)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

[INFO 12-28 00:48:45] ax.service.ax_client: Generated new trial 24 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:48:33', project_name='codecarbon', run_id='e2ad0d12-4f11-48c4-993a-065b1e9778a8', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.30772686994169, emissions=0.000598686555499108, emissions_rate=1.485289799225967e-05, cpu_power=30.588815091818184, gpu_power=265.80440876332955, ram_power=70.0, cpu_energy=0.00032620531213514915, gpu_energy=0.0014405447635459367, ram_energy=0.0007530911438160628, energy_consumed=0.002519841219497149, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 14/30 completed. Throughput: 760.14, Energy: 1.396464. Pareto size: 1.
INFO 12-28 00:48:47 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:48:47 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:48:47 [model.py:1510] Using max model len 32768
INFO 12-28 00:48:47 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:48:47 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:48:53 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:48:56 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:48:56 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2891856)[0;0m WARNING 12-28 00:48:56 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:48:56 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_4b41b032'), local_subscribe_addr='ipc:///tmp/a6f7a752-b8b1-4e47-aecc-f4bafe0fd0d0', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:49:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:49:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:49:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:49:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:49:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1ac48f2f'), local_subscribe_addr='ipc:///tmp/81732936-a4ec-4979-864d-909a9b093297', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:49:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9bed5b0f'), local_subscribe_addr='ipc:///tmp/751a1924-6a3d-4807-acb5-fc2086abfc9f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:49:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_75b89020'), local_subscribe_addr='ipc:///tmp/35365898-578d-4fdd-b1fc-0c53dd86d2aa', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:49:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_93010497'), local_subscribe_addr='ipc:///tmp/eb57bcbb-0a83-4ba3-8364-20214bc71a16', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:49:09 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:49:09 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:49:09 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:49:09 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:49:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ea8fa1f1'), local_subscribe_addr='ipc:///tmp/dee93ef2-7d35-4a04-b595-fa755ebc3e59', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:49:09 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:49:09 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:49:09 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:49:09 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
WARNING 12-28 00:49:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:49:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:49:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:49:09 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2892115)[0;0m INFO 12-28 00:49:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2892114)[0;0m INFO 12-28 00:49:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:09 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2892114)[0;0m INFO 12-28 00:49:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2892115)[0;0m INFO 12-28 00:49:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:10 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2892114)[0;0m INFO 12-28 00:49:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2892115)[0;0m INFO 12-28 00:49:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:10 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2892114)[0;0m INFO 12-28 00:49:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2892115)[0;0m INFO 12-28 00:49:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:10 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2892112)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP2 pid=2892114)[0;0m INFO 12-28 00:49:11 [default_loader.py:267] Loading weights took 0.74 seconds
[1;36m(Worker_TP3 pid=2892115)[0;0m INFO 12-28 00:49:11 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP0 pid=2892112)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.94it/s]
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:11 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP2 pid=2892114)[0;0m INFO 12-28 00:49:12 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.395781 seconds
[1;36m(Worker_TP0 pid=2892112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.72it/s]
[1;36m(Worker_TP0 pid=2892112)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.56it/s]
[1;36m(Worker_TP0 pid=2892112)[0;0m 
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:12 [default_loader.py:267] Loading weights took 0.80 seconds
[1;36m(Worker_TP3 pid=2892115)[0;0m INFO 12-28 00:49:12 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.634868 seconds
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:12 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.881604 seconds
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:12 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.116313 seconds
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:16 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:16 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2892114)[0;0m INFO 12-28 00:49:16 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2892115)[0;0m INFO 12-28 00:49:16 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(Worker_TP0 pid=2892112)[0;0m WARNING 12-28 00:49:17 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2892113)[0;0m WARNING 12-28 00:49:17 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2892114)[0;0m WARNING 12-28 00:49:17 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2892115)[0;0m WARNING 12-28 00:49:17 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.45 seconds
[1;36m(EngineCore_DP0 pid=2891856)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2891856)[0;0m 
[1;36m(EngineCore_DP0 pid=2891856)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2891856)[0;0m 
[1;36m(EngineCore_DP0 pid=2891856)[0;0m INFO 12-28 00:49:17 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:49:18 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1842.69it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:12,  1.34s/it, est. speed input: 43.30 toks/s, output: 1.49 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:02<01:13,  1.32it/s, est. speed input: 204.09 toks/s, output: 1.63 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:02<00:13,  6.40it/s, est. speed input: 1071.93 toks/s, output: 10.93 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 11.72it/s, est. speed input: 1677.13 toks/s, output: 26.99 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:02<00:04, 15.80it/s, est. speed input: 2328.40 toks/s, output: 41.88 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 19.49it/s, est. speed input: 2708.83 toks/s, output: 60.64 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:03<00:03, 21.08it/s, est. speed input: 2812.39 toks/s, output: 84.89 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:03<00:02, 26.26it/s, est. speed input: 2856.15 toks/s, output: 124.08 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:03<00:02, 25.39it/s, est. speed input: 3099.67 toks/s, output: 156.28 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:03<00:02, 25.73it/s, est. speed input: 3314.73 toks/s, output: 189.37 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:03<00:01, 27.54it/s, est. speed input: 3395.86 toks/s, output: 224.71 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:04<00:02, 20.89it/s, est. speed input: 3260.45 toks/s, output: 254.98 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 17.86it/s, est. speed input: 3097.90 toks/s, output: 279.13 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:02, 16.53it/s, est. speed input: 3166.86 toks/s, output: 310.27 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:02, 16.38it/s, est. speed input: 3384.35 toks/s, output: 344.40 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:01, 17.74it/s, est. speed input: 3343.53 toks/s, output: 383.24 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:04<00:01, 18.10it/s, est. speed input: 3388.99 toks/s, output: 409.11 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:01, 15.99it/s, est. speed input: 3495.04 toks/s, output: 429.65 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:01, 13.53it/s, est. speed input: 3514.53 toks/s, output: 461.19 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:01, 13.09it/s, est. speed input: 3473.78 toks/s, output: 486.80 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 10.78it/s, est. speed input: 3311.83 toks/s, output: 504.93 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:02,  8.97it/s, est. speed input: 3310.78 toks/s, output: 522.20 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 10.40it/s, est. speed input: 3270.28 toks/s, output: 557.70 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:02,  6.07it/s, est. speed input: 3010.30 toks/s, output: 549.39 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:02,  5.40it/s, est. speed input: 2783.17 toks/s, output: 574.91 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:08<00:02,  4.01it/s, est. speed input: 2586.44 toks/s, output: 561.29 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.62it/s, est. speed input: 2472.99 toks/s, output: 564.87 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  3.62it/s, est. speed input: 2625.31 toks/s, output: 860.42 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.59it/s, est. speed input: 2625.31 toks/s, output: 860.42 toks/s]
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:27 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2892112)[0;0m INFO 12-28 00:49:27 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:27 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2892113)[0;0m INFO 12-28 00:49:27 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2892114)[0;0m INFO 12-28 00:49:27 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2892115)[0;0m INFO 12-28 00:49:27 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:49:32] ax.service.ax_client: Completed trial 24 with data: {'throughput(token/s)': 854.902352, 'energy(J/token)': 1.250944}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:49:37] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 250.79525750875473), ObjectiveThreshold(throughput(token/s) >= 836.7065194709635)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:49:39] ax.service.ax_client: Generated new trial 25 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:49:27', project_name='codecarbon', run_id='e82f8296-4b1b-4cc7-8621-8a50d60f1781', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.626345955068246, emissions=0.0006128315618015044, emissions_rate=1.5084584827768691e-05, cpu_power=30.533359840909096, gpu_power=261.94533013273235, ram_power=70.0, cpu_energy=0.0003282413231780597, gpu_energy=0.0014918011934392794, ram_energy=0.0007593343139661657, energy_consumed=0.002579376830583505, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 15/30 completed. Throughput: 854.90, Energy: 1.250944. Pareto size: 2.
INFO 12-28 00:49:41 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:49:41 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:49:41 [model.py:1510] Using max model len 32768
INFO 12-28 00:49:41 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:49:41 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:49:47 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:49:50 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:49:50 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2893597)[0;0m WARNING 12-28 00:49:50 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:49:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_408a27ff'), local_subscribe_addr='ipc:///tmp/60adfe1f-cd35-4189-8016-45076221c566', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:49:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:49:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:49:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:50:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_538ac25f'), local_subscribe_addr='ipc:///tmp/3a09c66c-13e4-44cb-9765-d0fbbad2976e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:50:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_20addc0a'), local_subscribe_addr='ipc:///tmp/e4368b2e-2a6a-4dd2-ada4-e0485e280dc9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:50:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9359d854'), local_subscribe_addr='ipc:///tmp/959c2502-0a7b-4739-be95-36f236a41137', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:50:02 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:50:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3ca300c4'), local_subscribe_addr='ipc:///tmp/aaa06b57-91e8-46c6-a129-164701823c99', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:50:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:07 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:50:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:07 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:50:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:50:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:50:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:50:07 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:50:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_429ce075'), local_subscribe_addr='ipc:///tmp/995f9f27-bca3-49d4-b3bd-a0c81cf5fee1', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:50:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:07 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:07 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:07 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:50:07 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:50:07 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:50:07 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 00:50:07 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:50:07 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:50:07 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:50:07 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP3 pid=2893859)[0;0m INFO 12-28 00:50:07 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:07 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:07 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:07 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2893859)[0;0m INFO 12-28 00:50:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2893859)[0;0m INFO 12-28 00:50:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:08 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:08 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:08 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:08 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2893859)[0;0m INFO 12-28 00:50:08 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2893856)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:09 [default_loader.py:267] Loading weights took 0.74 seconds
[1;36m(Worker_TP0 pid=2893856)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.77it/s]
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:10 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP0 pid=2893856)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.41it/s]
[1;36m(Worker_TP0 pid=2893856)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.29it/s]
[1;36m(Worker_TP0 pid=2893856)[0;0m 
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:10 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.399856 seconds
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:10 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(Worker_TP3 pid=2893859)[0;0m INFO 12-28 00:50:10 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:10 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.777614 seconds
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:10 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.007977 seconds
[1;36m(Worker_TP3 pid=2893859)[0;0m INFO 12-28 00:50:11 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.278726 seconds
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:15 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2893859)[0;0m INFO 12-28 00:50:15 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:15 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:15 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(Worker_TP3 pid=2893859)[0;0m WARNING 12-28 00:50:15 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2893858)[0;0m WARNING 12-28 00:50:15 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2893857)[0;0m WARNING 12-28 00:50:15 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2893856)[0;0m WARNING 12-28 00:50:15 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:15 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.69 seconds
[1;36m(EngineCore_DP0 pid=2893597)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2893597)[0;0m 
[1;36m(EngineCore_DP0 pid=2893597)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2893597)[0;0m 
[1;36m(EngineCore_DP0 pid=2893597)[0;0m INFO 12-28 00:50:16 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:50:16 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1399.45it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:13,  1.34s/it, est. speed input: 116.11 toks/s, output: 0.74 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:59,  1.22s/it, est. speed input: 96.40 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.23it/s, est. speed input: 1048.46 toks/s, output: 9.66 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.26it/s, est. speed input: 1976.25 toks/s, output: 27.84 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 19.45it/s, est. speed input: 2725.45 toks/s, output: 50.82 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.25it/s, est. speed input: 3227.57 toks/s, output: 79.65 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 28.96it/s, est. speed input: 3612.95 toks/s, output: 118.58 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.15it/s, est. speed input: 3624.53 toks/s, output: 153.24 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.21it/s, est. speed input: 3501.33 toks/s, output: 194.43 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.63it/s, est. speed input: 3514.82 toks/s, output: 234.17 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 18.03it/s, est. speed input: 3275.91 toks/s, output: 267.06 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.35it/s, est. speed input: 3422.85 toks/s, output: 304.28 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.10it/s, est. speed input: 3387.68 toks/s, output: 339.07 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.24it/s, est. speed input: 3241.08 toks/s, output: 365.84 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:02, 14.75it/s, est. speed input: 3228.63 toks/s, output: 388.74 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.06it/s, est. speed input: 3171.98 toks/s, output: 395.40 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.59it/s, est. speed input: 3154.75 toks/s, output: 441.70 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.91it/s, est. speed input: 3054.33 toks/s, output: 466.75 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.93it/s, est. speed input: 3019.90 toks/s, output: 499.64 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.78it/s, est. speed input: 2932.61 toks/s, output: 524.07 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.17it/s, est. speed input: 3027.22 toks/s, output: 555.25 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.56it/s, est. speed input: 2964.85 toks/s, output: 582.82 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:07<00:01,  8.02it/s, est. speed input: 2826.66 toks/s, output: 597.94 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.58it/s, est. speed input: 2770.45 toks/s, output: 609.22 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.49it/s, est. speed input: 2687.01 toks/s, output: 615.22 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.88it/s, est. speed input: 2480.20 toks/s, output: 594.97 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.27it/s, est. speed input: 2453.34 toks/s, output: 639.28 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.04it/s, est. speed input: 2326.43 toks/s, output: 635.06 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.04it/s, est. speed input: 2608.12 toks/s, output: 840.05 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.51it/s, est. speed input: 2608.12 toks/s, output: 840.05 toks/s]
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:25 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:25 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2893856)[0;0m INFO 12-28 00:50:25 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2893857)[0;0m INFO 12-28 00:50:25 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:25 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2893858)[0;0m INFO 12-28 00:50:25 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2893859)[0;0m INFO 12-28 00:50:25 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:50:30] ax.service.ax_client: Completed trial 25 with data: {'throughput(token/s)': 833.037213, 'energy(J/token)': 1.413726}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:50:33] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 179.98072457313538), ObjectiveThreshold(throughput(token/s) >= 837.7848477355501)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

[INFO 12-28 00:50:35] ax.service.ax_client: Generated new trial 26 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:50:25', project_name='codecarbon', run_id='fa2a2f03-d6b5-4a5f-a275-15df75fe13c0', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=45.25566674908623, emissions=0.0006806349231631145, emissions_rate=1.5039772299385189e-05, cpu_power=30.351192813750004, gpu_power=228.48082773437932, ram_power=70.0, cpu_energy=0.00036690153356123777, gpu_energy=0.0016484446520887985, ram_energy=0.0008494115395102805, energy_consumed=0.0028647577251603167, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 16/30 completed. Throughput: 833.04, Energy: 1.413726. Pareto size: 2.
INFO 12-28 00:50:37 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:50:37 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:50:37 [model.py:1510] Using max model len 32768
INFO 12-28 00:50:37 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:50:37 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:50:44 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:50:46 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:50:47 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2896601)[0;0m WARNING 12-28 00:50:47 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:50:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_dda8d79f'), local_subscribe_addr='ipc:///tmp/a100ee30-8682-4c99-97b4-b425cc8861a4', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:50:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:50:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:50:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:50:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:50:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_80a9ba13'), local_subscribe_addr='ipc:///tmp/6de6faa8-751b-44d2-b715-312d17b1a5a7', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:50:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0c36cd9f'), local_subscribe_addr='ipc:///tmp/d4a38244-5344-4d5d-ae3e-4a03e277b484', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:50:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_98f7a675'), local_subscribe_addr='ipc:///tmp/425ba39c-54b1-42f3-89d7-210eaa321210', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:50:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_677ca008'), local_subscribe_addr='ipc:///tmp/543950f9-81d0-47db-bb5d-2dc0ccc81202', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:50:59 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:59 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:59 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:59 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:50:59 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:59 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:59 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:59 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:50:59 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:50:59 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:50:59 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:50:59 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:50:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_f1c0fa5a'), local_subscribe_addr='ipc:///tmp/090a0559-9c52-4cc7-8199-850bde150968', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:50:59 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:59 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:59 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:59 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:59 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:50:59 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:59 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:59 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:50:59 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:50:59 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:50:59 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:50:59 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 00:51:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:51:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:51:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:51:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:00 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:00 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2896816)[0;0m INFO 12-28 00:51:00 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:00 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:00 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:00 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2896816)[0;0m INFO 12-28 00:51:00 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:00 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:00 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:00 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2896816)[0;0m INFO 12-28 00:51:00 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:00 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2896816)[0;0m INFO 12-28 00:51:00 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2896813)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2896813)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.12it/s]
[1;36m(Worker_TP0 pid=2896813)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.89it/s]
[1;36m(Worker_TP0 pid=2896813)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.74it/s]
[1;36m(Worker_TP0 pid=2896813)[0;0m 
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:01 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:02 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:02 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.323899 seconds
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:02 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP3 pid=2896816)[0;0m INFO 12-28 00:51:02 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:02 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.802316 seconds
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:02 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.892746 seconds
[1;36m(Worker_TP3 pid=2896816)[0;0m INFO 12-28 00:51:03 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.266082 seconds
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:07 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:07 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2896816)[0;0m INFO 12-28 00:51:07 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:07 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:07 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:07 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:07 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:07 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:07 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:07 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:07 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:07 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(Worker_TP0 pid=2896813)[0;0m WARNING 12-28 00:51:07 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2896816)[0;0m WARNING 12-28 00:51:07 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2896814)[0;0m WARNING 12-28 00:51:07 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2896815)[0;0m WARNING 12-28 00:51:07 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:08 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.79 seconds
[1;36m(EngineCore_DP0 pid=2896601)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2896601)[0;0m 
[1;36m(EngineCore_DP0 pid=2896601)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2896601)[0;0m 
[1;36m(EngineCore_DP0 pid=2896601)[0;0m INFO 12-28 00:51:08 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:51:08 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1789.05it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:13,  1.35s/it, est. speed input: 115.43 toks/s, output: 0.74 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:59,  1.22s/it, est. speed input: 96.03 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.24it/s, est. speed input: 1048.32 toks/s, output: 9.66 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.31it/s, est. speed input: 1977.41 toks/s, output: 27.86 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 20.96it/s, est. speed input: 2926.21 toks/s, output: 58.08 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:02<00:02, 25.29it/s, est. speed input: 3445.87 toks/s, output: 91.14 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:03<00:02, 29.38it/s, est. speed input: 3770.98 toks/s, output: 125.20 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:03<00:02, 22.61it/s, est. speed input: 3697.23 toks/s, output: 162.17 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:03<00:02, 20.95it/s, est. speed input: 3501.59 toks/s, output: 205.23 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:04<00:02, 20.75it/s, est. speed input: 3478.90 toks/s, output: 244.18 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 17.58it/s, est. speed input: 3284.91 toks/s, output: 267.79 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.07it/s, est. speed input: 3433.53 toks/s, output: 305.23 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 18.93it/s, est. speed input: 3399.15 toks/s, output: 340.22 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.13it/s, est. speed input: 3253.35 toks/s, output: 367.22 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:02, 14.68it/s, est. speed input: 3241.32 toks/s, output: 390.27 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02,  9.99it/s, est. speed input: 3183.93 toks/s, output: 396.89 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.53it/s, est. speed input: 3165.41 toks/s, output: 443.19 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.88it/s, est. speed input: 3064.91 toks/s, output: 468.37 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.93it/s, est. speed input: 3030.88 toks/s, output: 501.46 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.81it/s, est. speed input: 2943.98 toks/s, output: 526.10 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.21it/s, est. speed input: 3039.03 toks/s, output: 557.42 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.59it/s, est. speed input: 2976.24 toks/s, output: 585.06 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.04it/s, est. speed input: 2837.57 toks/s, output: 600.25 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.60it/s, est. speed input: 2781.30 toks/s, output: 611.61 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.50it/s, est. speed input: 2697.03 toks/s, output: 617.51 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.88it/s, est. speed input: 2488.05 toks/s, output: 596.85 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.27it/s, est. speed input: 2461.11 toks/s, output: 641.31 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.05it/s, est. speed input: 2334.16 toks/s, output: 637.18 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.05it/s, est. speed input: 2616.78 toks/s, output: 842.84 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.55it/s, est. speed input: 2616.78 toks/s, output: 842.84 toks/s]
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2896813)[0;0m INFO 12-28 00:51:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2896815)[0;0m INFO 12-28 00:51:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2896814)[0;0m INFO 12-28 00:51:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2896816)[0;0m INFO 12-28 00:51:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:51:23] ax.service.ax_client: Completed trial 26 with data: {'throughput(token/s)': 837.318619, 'energy(J/token)': 1.303563}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:51:26] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 163.14950734376907), ObjectiveThreshold(throughput(token/s) >= 837.9980475455867)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

[INFO 12-28 00:51:28] ax.service.ax_client: Generated new trial 27 with parameters {'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:51:18', project_name='codecarbon', run_id='1fc09cfe-c09b-43e8-83c6-2cf05a699cca', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=41.65159944095649, emissions=0.0006275973233650936, emissions_rate=1.5067784473793101e-05, cpu_power=30.271041997500006, gpu_power=249.50624043015634, ram_power=70.0, cpu_energy=0.0003367880905961795, gpu_energy=0.0015254998315095136, ram_energy=0.0007792372468419168, energy_consumed=0.00264152516894761, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 17/30 completed. Throughput: 837.32, Energy: 1.303563. Pareto size: 2.
INFO 12-28 00:51:30 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:51:30 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:51:30 [model.py:1510] Using max model len 32768
INFO 12-28 00:51:30 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:51:30 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:51:37 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:51:39 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:51:39 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2897896)[0;0m WARNING 12-28 00:51:39 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:51:39 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_848e6348'), local_subscribe_addr='ipc:///tmp/cca29d54-118d-4be9-91c9-08c960b5b039', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:51:46 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:51:46 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:51:46 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:51:46 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:51:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0600400e'), local_subscribe_addr='ipc:///tmp/bd9893a9-c55c-44ed-90c7-1e2b9c312c6d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:51:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_cf447d00'), local_subscribe_addr='ipc:///tmp/53ab7f95-4e00-4dfe-824d-0bfdf9c8c231', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:51:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_39028fc8'), local_subscribe_addr='ipc:///tmp/c1c76501-57e9-4e2c-9d84-4c060ed58bd1', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:51:51 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d4d8d4e9'), local_subscribe_addr='ipc:///tmp/8240b78e-4d32-402e-8b81-f25126b19780', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:51:52 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:51:52 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:51:52 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:51:52 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:51:52 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:51:52 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:51:52 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:51:52 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:51:52 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:51:52 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:51:52 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:51:52 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:51:52 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_dceac699'), local_subscribe_addr='ipc:///tmp/dd37b4e0-204f-4682-b7f9-dd26e3d28a09', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:51:52 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:51:52 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:51:52 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:51:52 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:51:52 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:51:52 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:51:52 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:51:52 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:51:52 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:51:52 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:51:52 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:51:52 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
WARNING 12-28 00:51:52 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:51:52 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:51:52 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:51:52 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2898103)[0;0m INFO 12-28 00:51:52 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:51:52 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:51:52 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2898105)[0;0m INFO 12-28 00:51:52 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2898103)[0;0m INFO 12-28 00:51:53 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:51:53 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:51:53 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2898103)[0;0m INFO 12-28 00:51:53 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2898105)[0;0m INFO 12-28 00:51:53 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:51:53 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:51:53 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2898105)[0;0m INFO 12-28 00:51:53 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2898103)[0;0m INFO 12-28 00:51:53 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:51:53 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:51:53 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2898105)[0;0m INFO 12-28 00:51:53 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2898102)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2898102)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.04it/s]
[1;36m(Worker_TP1 pid=2898103)[0;0m INFO 12-28 00:51:54 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP0 pid=2898102)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.82it/s]
[1;36m(Worker_TP0 pid=2898102)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.66it/s]
[1;36m(Worker_TP0 pid=2898102)[0;0m 
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:51:54 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:51:54 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP1 pid=2898103)[0;0m INFO 12-28 00:51:55 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.326180 seconds
[1;36m(Worker_TP3 pid=2898105)[0;0m INFO 12-28 00:51:55 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:51:55 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.515681 seconds
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:51:55 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.747161 seconds
[1;36m(Worker_TP3 pid=2898105)[0;0m INFO 12-28 00:51:55 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.972920 seconds
[1;36m(Worker_TP3 pid=2898105)[0;0m INFO 12-28 00:51:59 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:51:59 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2898103)[0;0m INFO 12-28 00:51:59 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:51:59 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(Worker_TP3 pid=2898105)[0;0m WARNING 12-28 00:52:00 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2898102)[0;0m WARNING 12-28 00:52:00 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2898103)[0;0m WARNING 12-28 00:52:00 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2898104)[0;0m WARNING 12-28 00:52:00 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.62 seconds
[1;36m(EngineCore_DP0 pid=2897896)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2897896)[0;0m 
[1;36m(EngineCore_DP0 pid=2897896)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2897896)[0;0m 
[1;36m(EngineCore_DP0 pid=2897896)[0;0m INFO 12-28 00:52:00 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:52:01 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1300.91it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:11,  1.33s/it, est. speed input: 117.04 toks/s, output: 0.75 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:58,  1.21s/it, est. speed input: 96.71 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.28it/s, est. speed input: 1055.77 toks/s, output: 9.73 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.38it/s, est. speed input: 1991.09 toks/s, output: 28.05 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 21.04it/s, est. speed input: 2944.19 toks/s, output: 58.43 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:02<00:02, 25.39it/s, est. speed input: 3467.06 toks/s, output: 91.70 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:03<00:02, 29.46it/s, est. speed input: 3792.74 toks/s, output: 125.93 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:03<00:02, 22.66it/s, est. speed input: 3716.84 toks/s, output: 163.03 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:03<00:02, 20.92it/s, est. speed input: 3516.21 toks/s, output: 206.09 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:03<00:02, 20.74it/s, est. speed input: 3493.03 toks/s, output: 245.17 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 17.58it/s, est. speed input: 3297.44 toks/s, output: 268.81 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.05it/s, est. speed input: 3445.82 toks/s, output: 306.32 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 18.93it/s, est. speed input: 3411.16 toks/s, output: 341.42 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.11it/s, est. speed input: 3263.79 toks/s, output: 368.40 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:02, 14.69it/s, est. speed input: 3252.00 toks/s, output: 391.55 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.03it/s, est. speed input: 3195.64 toks/s, output: 398.35 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.59it/s, est. speed input: 3177.58 toks/s, output: 444.90 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.93it/s, est. speed input: 3076.86 toks/s, output: 470.19 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.99it/s, est. speed input: 3042.71 toks/s, output: 503.42 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.86it/s, est. speed input: 2955.25 toks/s, output: 528.12 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.25it/s, est. speed input: 3050.62 toks/s, output: 559.54 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.61it/s, est. speed input: 2987.05 toks/s, output: 587.18 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.05it/s, est. speed input: 2847.32 toks/s, output: 602.31 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.61it/s, est. speed input: 2790.70 toks/s, output: 613.67 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.51it/s, est. speed input: 2706.14 toks/s, output: 619.60 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:02,  3.88it/s, est. speed input: 2495.82 toks/s, output: 598.72 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.27it/s, est. speed input: 2468.62 toks/s, output: 643.27 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.05it/s, est. speed input: 2340.83 toks/s, output: 639.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.05it/s, est. speed input: 2624.31 toks/s, output: 845.26 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.59it/s, est. speed input: 2624.31 toks/s, output: 845.26 toks/s]
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:52:10 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2898102)[0;0m INFO 12-28 00:52:10 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:52:10 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2898104)[0;0m INFO 12-28 00:52:10 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2898103)[0;0m INFO 12-28 00:52:10 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2898105)[0;0m INFO 12-28 00:52:10 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:52:15] ax.service.ax_client: Completed trial 27 with data: {'throughput(token/s)': 837.692692, 'energy(J/token)': 1.271572}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:52:19] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 118.55502730607986), ObjectiveThreshold(throughput(token/s) >= 837.9028532902823)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:

Optimization failed in `gen_candidates_scipy` with the following warning(s):
[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .')]
Because you specified `batch_initial_conditions` larger than required `num_restarts`, optimization will not be retried with new initial conditions and will proceed with the current solution. Suggested remediation: Try again with different `batch_initial_conditions`, don't provide `batch_initial_conditions`, or increase `num_restarts`.

[INFO 12-28 00:52:21] ax.service.ax_client: Generated new trial 28 with parameters {'block_size': 64, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:52:10', project_name='codecarbon', run_id='7fea1b04-b76e-4956-b9df-01d7de299038', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.497825618018396, emissions=0.0006121955512026058, emissions_rate=1.5116751130713205e-05, cpu_power=30.244721470909095, gpu_power=265.4789578670949, ram_power=70.0, cpu_energy=0.000325826907254212, gpu_energy=0.0014940370285616567, ram_energy=0.0007568359585504774, energy_consumed=0.0025766998943663457, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 18/30 completed. Throughput: 837.69, Energy: 1.271572. Pareto size: 2.
INFO 12-28 00:52:23 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 64, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:52:23 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:52:23 [model.py:1510] Using max model len 32768
INFO 12-28 00:52:23 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:52:23 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:52:30 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:32 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:32 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2899302)[0;0m WARNING 12-28 00:52:32 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_f8481606'), local_subscribe_addr='ipc:///tmp/0acddbd0-d2d3-4a8a-9ab2-1f725284a212', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:52:39 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:52:39 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:52:39 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:52:39 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:52:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2de6024a'), local_subscribe_addr='ipc:///tmp/6843462f-ae6a-4ac8-acff-f34c3de1ae7a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:52:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_afbe64ba'), local_subscribe_addr='ipc:///tmp/37e66f7e-536f-46a4-877a-05947233e1fa', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:52:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a5b25826'), local_subscribe_addr='ipc:///tmp/3e81a56c-f358-4929-ab4e-7d8f63eec5f4', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:52:44 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_50403b2f'), local_subscribe_addr='ipc:///tmp/2057a375-fd52-4ad1-aa29-9f4bcb6a8793', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:52:45 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:52:45 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:52:45 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:52:45 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:52:45 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:52:45 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:52:45 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:52:45 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:52:45 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:52:45 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:52:45 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:52:45 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:52:45 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_ebdc2efb'), local_subscribe_addr='ipc:///tmp/702c1cf4-7636-4cab-8e16-78ff6f9cb319', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:52:45 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:52:45 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:52:45 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:52:45 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:52:45 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:52:45 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:52:45 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:52:45 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:52:45 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:52:45 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:52:45 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:52:45 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 00:52:45 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:52:45 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:52:45 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:52:45 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP2 pid=2899541)[0;0m INFO 12-28 00:52:45 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2899542)[0;0m INFO 12-28 00:52:45 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:52:45 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:52:45 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2899541)[0;0m INFO 12-28 00:52:46 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2899542)[0;0m INFO 12-28 00:52:46 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:52:46 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:52:46 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2899541)[0;0m INFO 12-28 00:52:46 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2899542)[0;0m INFO 12-28 00:52:46 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:52:46 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:52:46 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:52:46 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2899542)[0;0m INFO 12-28 00:52:46 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:52:46 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2899541)[0;0m INFO 12-28 00:52:46 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2899539)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:52:47 [default_loader.py:267] Loading weights took 0.81 seconds
[1;36m(Worker_TP2 pid=2899541)[0;0m INFO 12-28 00:52:47 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP0 pid=2899539)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.00it/s]
[1;36m(Worker_TP3 pid=2899542)[0;0m INFO 12-28 00:52:48 [default_loader.py:267] Loading weights took 0.79 seconds
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:52:48 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.382050 seconds
[1;36m(Worker_TP0 pid=2899539)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.79it/s]
[1;36m(Worker_TP0 pid=2899539)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.64it/s]
[1;36m(Worker_TP0 pid=2899539)[0;0m 
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:52:48 [default_loader.py:267] Loading weights took 0.78 seconds
[1;36m(Worker_TP2 pid=2899541)[0;0m INFO 12-28 00:52:48 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.594777 seconds
[1;36m(Worker_TP3 pid=2899542)[0;0m INFO 12-28 00:52:48 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.836883 seconds
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:52:48 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.056205 seconds
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:52:52 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:52:52 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2899542)[0;0m INFO 12-28 00:52:52 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2899541)[0;0m INFO 12-28 00:52:52 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.55x
[1;36m(Worker_TP0 pid=2899539)[0;0m WARNING 12-28 00:52:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2899541)[0;0m WARNING 12-28 00:52:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2899540)[0;0m WARNING 12-28 00:52:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2899542)[0;0m WARNING 12-28 00:52:53 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.29 seconds
[1;36m(EngineCore_DP0 pid=2899302)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2899302)[0;0m 
[1;36m(EngineCore_DP0 pid=2899302)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2899302)[0;0m 
[1;36m(EngineCore_DP0 pid=2899302)[0;0m INFO 12-28 00:52:53 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:52:53 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1812.32it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:13,  1.35s/it, est. speed input: 115.33 toks/s, output: 0.74 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:59,  1.22s/it, est. speed input: 95.96 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.24it/s, est. speed input: 1047.64 toks/s, output: 9.65 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.30it/s, est. speed input: 1975.96 toks/s, output: 27.84 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 19.52it/s, est. speed input: 2725.41 toks/s, output: 50.81 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.32it/s, est. speed input: 3227.98 toks/s, output: 79.66 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 29.04it/s, est. speed input: 3613.56 toks/s, output: 118.60 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.17it/s, est. speed input: 3624.43 toks/s, output: 153.23 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.26it/s, est. speed input: 3502.83 toks/s, output: 194.51 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.63it/s, est. speed input: 3514.85 toks/s, output: 234.17 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 18.03it/s, est. speed input: 3275.93 toks/s, output: 267.06 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.40it/s, est. speed input: 3424.18 toks/s, output: 304.40 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.21it/s, est. speed input: 3390.91 toks/s, output: 339.40 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.37it/s, est. speed input: 3246.59 toks/s, output: 366.46 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:02, 14.87it/s, est. speed input: 3234.75 toks/s, output: 389.48 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.15it/s, est. speed input: 3179.78 toks/s, output: 396.37 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.69it/s, est. speed input: 3163.02 toks/s, output: 442.86 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 11.00it/s, est. speed input: 3062.75 toks/s, output: 468.04 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 12.04it/s, est. speed input: 3028.95 toks/s, output: 501.14 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.88it/s, est. speed input: 2941.94 toks/s, output: 525.74 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.27it/s, est. speed input: 3037.32 toks/s, output: 557.10 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.63it/s, est. speed input: 2974.55 toks/s, output: 584.73 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.07it/s, est. speed input: 2836.49 toks/s, output: 600.02 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.62it/s, est. speed input: 2780.14 toks/s, output: 611.35 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.52it/s, est. speed input: 2696.36 toks/s, output: 617.36 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.89it/s, est. speed input: 2487.66 toks/s, output: 596.76 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.28it/s, est. speed input: 2460.62 toks/s, output: 641.18 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.05it/s, est. speed input: 2333.51 toks/s, output: 637.00 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.05it/s, est. speed input: 2616.05 toks/s, output: 842.60 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.55it/s, est. speed input: 2616.05 toks/s, output: 842.60 toks/s]
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:53:03 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2899539)[0;0m INFO 12-28 00:53:03 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:53:03 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2899540)[0;0m INFO 12-28 00:53:03 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2899541)[0;0m INFO 12-28 00:53:03 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2899542)[0;0m INFO 12-28 00:53:03 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:53:08] ax.service.ax_client: Completed trial 28 with data: {'throughput(token/s)': 837.132692, 'energy(J/token)': 1.270294}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:53:09] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.358401979038693), ObjectiveThreshold(throughput(token/s) >= 235.08286531768405)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:

Optimization failed in `gen_candidates_scipy` with the following warning(s):
[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .')]
Because you specified `batch_initial_conditions` larger than required `num_restarts`, optimization will not be retried with new initial conditions and will proceed with the current solution. Suggested remediation: Try again with different `batch_initial_conditions`, don't provide `batch_initial_conditions`, or increase `num_restarts`.

[INFO 12-28 00:53:11] ax.service.ax_client: Generated new trial 29 with parameters {'block_size': 128, 'max_num_seqs': 64, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:53:03', project_name='codecarbon', run_id='de0c663c-5a12-46c8-9e4f-33b55babbfb7', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=40.50972125097178, emissions=0.0006115802992511406, emissions_rate=1.5097124353490078e-05, cpu_power=30.157274214545456, gpu_power=265.414187431989, ram_power=70.0, cpu_energy=0.0003256708005280386, gpu_energy=0.001491344804184891, ram_energy=0.0007570947255260156, energy_consumed=0.002574110330238945, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 19/30 completed. Throughput: 837.13, Energy: 1.270294. Pareto size: 5.
INFO 12-28 00:53:13 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 128, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:53:13 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:53:13 [model.py:1510] Using max model len 32768
INFO 12-28 00:53:13 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:53:13 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:53:21 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:24 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:24 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2900500)[0;0m WARNING 12-28 00:53:24 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_be928d1d'), local_subscribe_addr='ipc:///tmp/816f7a50-750b-4cf2-b8e7-3cb957f467a9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:53:30 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:53:30 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:53:31 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:53:32 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:53:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ae514cbc'), local_subscribe_addr='ipc:///tmp/ad09dd04-6c38-408d-b70f-d112285e1733', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:53:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8738e0f4'), local_subscribe_addr='ipc:///tmp/dc521265-012c-40ee-935f-832ad1e8558a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:53:36 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0673e37c'), local_subscribe_addr='ipc:///tmp/9821f93b-e966-405a-afe4-2ede8fcbeed6', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:53:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b0144b57'), local_subscribe_addr='ipc:///tmp/ef5684bc-016c-40df-bdc8-fb7df097590f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:53:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:53:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:53:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:53:37 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:53:37 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:53:37 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:53:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:53:37 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:53:37 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:53:37 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:53:37 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:53:37 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:53:37 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_803391b3'), local_subscribe_addr='ipc:///tmp/54f2b536-e220-46d2-a6d4-5ff291cd12f3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:53:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:53:37 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:53:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:53:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:53:37 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:53:37 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:53:37 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:53:37 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:53:37 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:53:37 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:53:37 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:53:37 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
WARNING 12-28 00:53:38 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:53:38 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:53:38 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:53:38 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:38 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:38 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2900722)[0;0m INFO 12-28 00:53:38 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2900721)[0;0m INFO 12-28 00:53:38 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:38 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2900722)[0;0m INFO 12-28 00:53:38 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:38 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2900721)[0;0m INFO 12-28 00:53:38 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:38 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2900722)[0;0m INFO 12-28 00:53:38 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:38 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2900721)[0;0m INFO 12-28 00:53:38 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2900722)[0;0m INFO 12-28 00:53:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2900721)[0;0m INFO 12-28 00:53:38 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2900719)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2900719)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.13it/s]
[1;36m(Worker_TP0 pid=2900719)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.87it/s]
[1;36m(Worker_TP0 pid=2900719)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.73it/s]
[1;36m(Worker_TP0 pid=2900719)[0;0m 
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:39 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP3 pid=2900722)[0;0m INFO 12-28 00:53:40 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP2 pid=2900721)[0;0m INFO 12-28 00:53:40 [default_loader.py:267] Loading weights took 0.88 seconds
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:40 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.369220 seconds
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:40 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(Worker_TP3 pid=2900722)[0;0m INFO 12-28 00:53:40 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.729084 seconds
[1;36m(Worker_TP2 pid=2900721)[0;0m INFO 12-28 00:53:41 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.902776 seconds
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:41 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.191540 seconds
[1;36m(Worker_TP3 pid=2900722)[0;0m INFO 12-28 00:53:45 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:45 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2900721)[0;0m INFO 12-28 00:53:45 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:45 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(Worker_TP3 pid=2900722)[0;0m WARNING 12-28 00:53:45 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2900720)[0;0m WARNING 12-28 00:53:45 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2900721)[0;0m WARNING 12-28 00:53:45 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2900719)[0;0m WARNING 12-28 00:53:45 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:45 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.24 seconds
[1;36m(EngineCore_DP0 pid=2900500)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2900500)[0;0m 
[1;36m(EngineCore_DP0 pid=2900500)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2900500)[0;0m 
[1;36m(EngineCore_DP0 pid=2900500)[0;0m INFO 12-28 00:53:46 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:53:46 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1558.47it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:10,  1.32s/it, est. speed input: 118.05 toks/s, output: 0.76 toks/s]Processed prompts:   2%|â–         | 2/100 [00:01<01:10,  1.38it/s, est. speed input: 421.13 toks/s, output: 2.46 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:01<00:21,  4.35it/s, est. speed input: 1105.07 toks/s, output: 8.10 toks/s]WARNING 12-28 00:53:48 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:   8%|â–Š         | 8/100 [00:01<00:13,  7.02it/s, est. speed input: 1419.05 toks/s, output: 16.32 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:07, 12.18it/s, est. speed input: 2087.96 toks/s, output: 34.47 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:02<00:06, 13.67it/s, est. speed input: 2270.51 toks/s, output: 46.80 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:02<00:03, 19.68it/s, est. speed input: 2738.97 toks/s, output: 70.59 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:02<00:03, 20.84it/s, est. speed input: 2839.18 toks/s, output: 75.64 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 21.47it/s, est. speed input: 2941.56 toks/s, output: 93.27 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:02<00:03, 19.12it/s, est. speed input: 3051.55 toks/s, output: 104.29 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:02<00:03, 21.24it/s, est. speed input: 2992.45 toks/s, output: 120.57 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:03<00:03, 19.82it/s, est. speed input: 3224.63 toks/s, output: 135.97 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:03<00:02, 20.86it/s, est. speed input: 3380.83 toks/s, output: 154.40 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.83it/s, est. speed input: 3560.38 toks/s, output: 184.12 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 19.92it/s, est. speed input: 3520.49 toks/s, output: 218.34 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:03<00:02, 20.15it/s, est. speed input: 3447.97 toks/s, output: 244.07 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:04<00:02, 16.36it/s, est. speed input: 3304.39 toks/s, output: 263.11 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:04<00:02, 14.88it/s, est. speed input: 3195.37 toks/s, output: 294.10 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:04<00:01, 18.62it/s, est. speed input: 3371.10 toks/s, output: 352.89 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:04<00:02, 14.27it/s, est. speed input: 3216.45 toks/s, output: 371.05 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:05<00:02, 13.91it/s, est. speed input: 3191.20 toks/s, output: 396.23 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:01, 14.15it/s, est. speed input: 3240.25 toks/s, output: 436.18 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:01, 13.95it/s, est. speed input: 3201.73 toks/s, output: 472.65 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:01, 13.39it/s, est. speed input: 3301.72 toks/s, output: 495.71 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 10.98it/s, est. speed input: 3161.77 toks/s, output: 514.03 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:02,  8.63it/s, est. speed input: 2993.43 toks/s, output: 529.40 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01,  9.15it/s, est. speed input: 3047.98 toks/s, output: 560.02 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.00it/s, est. speed input: 3013.30 toks/s, output: 594.35 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01, 10.18it/s, est. speed input: 3016.95 toks/s, output: 624.09 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:06<00:00, 11.38it/s, est. speed input: 2985.44 toks/s, output: 660.48 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:08<00:02,  4.39it/s, est. speed input: 2571.00 toks/s, output: 621.54 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  4.41it/s, est. speed input: 2518.67 toks/s, output: 633.42 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.53it/s, est. speed input: 2480.00 toks/s, output: 648.82 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00,  8.20it/s, est. speed input: 2510.68 toks/s, output: 811.21 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00,  8.20it/s, est. speed input: 2510.68 toks/s, output: 811.21 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00, 11.08it/s, est. speed input: 2510.68 toks/s, output: 811.21 toks/s]
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:56 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2900719)[0;0m INFO 12-28 00:53:56 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:56 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2900720)[0;0m INFO 12-28 00:53:56 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2900721)[0;0m INFO 12-28 00:53:56 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2900722)[0;0m INFO 12-28 00:53:56 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:54:01] ax.service.ax_client: Completed trial 29 with data: {'throughput(token/s)': 805.347388, 'energy(J/token)': 1.337995}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:54:01] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.358400357116528), ObjectiveThreshold(throughput(token/s) >= 235.08315109938778)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

[INFO 12-28 00:54:04] ax.service.ax_client: Generated new trial 30 with parameters {'block_size': 128, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:53:56', project_name='codecarbon', run_id='cfcb7728-4e25-4787-8975-76aebfd75419', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=43.016398890991695, emissions=0.0006462056837338958, emissions_rate=1.5022310104838212e-05, cpu_power=30.17214489692308, gpu_power=242.96167311101684, ram_power=70.0, cpu_energy=0.0003462696390119493, gpu_energy=0.0015679073654357722, ram_energy=0.0008056698055219993, energy_consumed=0.0027198468099697204, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 20/30 completed. Throughput: 805.35, Energy: 1.337995. Pareto size: 5.
INFO 12-28 00:54:05 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 128, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:54:06 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:54:06 [model.py:1510] Using max model len 32768
INFO 12-28 00:54:06 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:54:06 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:54:13 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:16 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:16 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2901701)[0;0m WARNING 12-28 00:54:16 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:16 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_ad82f5b6'), local_subscribe_addr='ipc:///tmp/e56c32a1-7630-4cb4-a913-9e02b56f3412', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:54:23 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:54:23 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:54:23 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:54:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_5fadd536'), local_subscribe_addr='ipc:///tmp/fbfe83ed-45a2-485f-bad0-6e5766530113', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:54:28 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:54:28 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ef81ee80'), local_subscribe_addr='ipc:///tmp/a44981fd-25b3-4cca-80a8-26070eb75647', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:54:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_314d9de0'), local_subscribe_addr='ipc:///tmp/4fc15fd0-1427-4a34-8d7e-8a07064bde9f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:54:33 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ae9fc2f0'), local_subscribe_addr='ipc:///tmp/5d9281ec-f7b0-4239-8756-1d9175fdd141', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:54:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:54:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:54:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:54:34 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:54:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:54:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:54:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:54:34 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:54:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:54:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:54:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:54:34 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:54:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_50cf9ab9'), local_subscribe_addr='ipc:///tmp/015358c3-effb-495d-a80e-88a541d09f8e', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:54:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:54:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:54:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:54:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:54:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:54:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:54:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:54:34 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:54:35 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:54:35 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:54:35 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:54:35 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
WARNING 12-28 00:54:35 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:54:35 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:54:35 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:54:35 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:35 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:35 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:35 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:35 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:35 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:35 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:36 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2901932)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2901932)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.10it/s]
[1;36m(Worker_TP0 pid=2901932)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.80it/s]
[1;36m(Worker_TP0 pid=2901932)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.67it/s]
[1;36m(Worker_TP0 pid=2901932)[0;0m 
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:37 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:37 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:37 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:37 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.347920 seconds
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:37 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:38 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.675892 seconds
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:38 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.936251 seconds
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:38 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.164641 seconds
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:42 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:42 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:42 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:42 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(Worker_TP2 pid=2901934)[0;0m WARNING 12-28 00:54:42 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2901933)[0;0m WARNING 12-28 00:54:42 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2901935)[0;0m WARNING 12-28 00:54:42 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2901932)[0;0m WARNING 12-28 00:54:42 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:42 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.34 seconds
[1;36m(EngineCore_DP0 pid=2901701)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2901701)[0;0m 
[1;36m(EngineCore_DP0 pid=2901701)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2901701)[0;0m 
[1;36m(EngineCore_DP0 pid=2901701)[0;0m INFO 12-28 00:54:43 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:54:43 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1381.24it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:12,  1.34s/it, est. speed input: 116.83 toks/s, output: 0.75 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:58,  1.21s/it, est. speed input: 96.69 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.27it/s, est. speed input: 1054.55 toks/s, output: 9.71 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.36it/s, est. speed input: 1988.23 toks/s, output: 28.01 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 19.59it/s, est. speed input: 2741.55 toks/s, output: 51.12 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.40it/s, est. speed input: 3246.50 toks/s, output: 80.12 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 29.11it/s, est. speed input: 3632.88 toks/s, output: 119.23 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.18it/s, est. speed input: 3641.14 toks/s, output: 153.94 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.27it/s, est. speed input: 3517.82 toks/s, output: 195.34 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.74it/s, est. speed input: 3532.40 toks/s, output: 235.34 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 18.16it/s, est. speed input: 3294.16 toks/s, output: 268.54 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.54it/s, est. speed input: 3443.29 toks/s, output: 306.10 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.32it/s, est. speed input: 3409.34 toks/s, output: 341.24 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.45it/s, est. speed input: 3263.97 toks/s, output: 368.42 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:02, 14.98it/s, est. speed input: 3253.06 toks/s, output: 391.68 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.22it/s, est. speed input: 3197.94 toks/s, output: 398.63 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.78it/s, est. speed input: 3181.48 toks/s, output: 445.44 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 11.09it/s, est. speed input: 3081.39 toks/s, output: 470.89 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 12.13it/s, est. speed input: 3046.98 toks/s, output: 504.12 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.95it/s, est. speed input: 2959.32 toks/s, output: 528.84 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.33it/s, est. speed input: 3054.92 toks/s, output: 560.33 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.67it/s, est. speed input: 2991.28 toks/s, output: 588.02 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.08it/s, est. speed input: 2851.28 toks/s, output: 603.15 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.64it/s, est. speed input: 2794.48 toks/s, output: 614.51 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.52it/s, est. speed input: 2709.43 toks/s, output: 620.35 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:07<00:02,  3.89it/s, est. speed input: 2498.67 toks/s, output: 599.40 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.28it/s, est. speed input: 2471.49 toks/s, output: 644.01 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.06it/s, est. speed input: 2343.58 toks/s, output: 639.75 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.06it/s, est. speed input: 2627.38 toks/s, output: 846.25 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.60it/s, est. speed input: 2627.38 toks/s, output: 846.25 toks/s]
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:52 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2901932)[0;0m INFO 12-28 00:54:52 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:52 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:52 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2901934)[0;0m INFO 12-28 00:54:52 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2901933)[0;0m INFO 12-28 00:54:52 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:52 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2901935)[0;0m INFO 12-28 00:54:52 [multiproc_executor.py:599] WorkerProc shutting down.
[INFO 12-28 00:54:58] ax.service.ax_client: Completed trial 30 with data: {'throughput(token/s)': 839.0642, 'energy(J/token)': 1.453598}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:54:58] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.3584148584349287), ObjectiveThreshold(throughput(token/s) >= 235.08357594781023)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:

Optimization failed in `gen_candidates_scipy` with the following warning(s):
[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .')]
Because you specified `batch_initial_conditions` larger than required `num_restarts`, optimization will not be retried with new initial conditions and will proceed with the current solution. Suggested remediation: Try again with different `batch_initial_conditions`, don't provide `batch_initial_conditions`, or increase `num_restarts`.

[INFO 12-28 00:55:00] ax.service.ax_client: Generated new trial 31 with parameters {'block_size': 128, 'max_num_seqs': 64, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:54:52', project_name='codecarbon', run_id='de06415c-9fd6-4baa-a24a-cdc162595d32', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=47.55773742601741, emissions=0.0006998316112366037, emissions_rate=1.4715410133320322e-05, cpu_power=30.152034750000006, gpu_power=233.52752945224998, ram_power=70.0, cpu_energy=0.0003800998713941232, gpu_energy=0.001681643567535751, ram_energy=0.0008838121685965841, energy_consumed=0.0029455556075264583, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 21/30 completed. Throughput: 839.06, Energy: 1.453598. Pareto size: 5.
INFO 12-28 00:55:02 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 128, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:55:02 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:55:02 [model.py:1510] Using max model len 32768
INFO 12-28 00:55:02 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:55:02 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:55:09 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:12 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:12 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2902964)[0;0m WARNING 12-28 00:55:12 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_1a57ba65'), local_subscribe_addr='ipc:///tmp/c9a31568-e9ec-4385-80b9-524d09db2265', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:55:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:55:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:55:19 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:55:20 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:55:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b914b8e4'), local_subscribe_addr='ipc:///tmp/e4183fe9-1625-48ab-9522-dd19a8d4445a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:55:24 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e88cb24d'), local_subscribe_addr='ipc:///tmp/d714ea4b-9a53-42ad-8bc1-f77026ead6d6', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:55:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c7520ecc'), local_subscribe_addr='ipc:///tmp/fd1ee25b-77cb-4472-bb3f-0e07e9fda057', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:55:25 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3729b317'), local_subscribe_addr='ipc:///tmp/8ea0812b-e7be-4416-8b62-8b4466e380c9', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:55:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:55:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:55:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:55:27 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:55:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:55:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:55:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:55:27 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:55:27 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:55:27 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:55:27 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:55:27 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:55:27 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_4f46673d'), local_subscribe_addr='ipc:///tmp/9718dc0d-5bf5-42c5-8506-32c5f1a8ae86', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:55:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:55:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:55:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:55:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:55:27 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:55:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:55:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:55:27 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:55:27 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:55:27 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:55:27 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:55:27 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 00:55:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:55:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:55:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:55:27 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP3 pid=2903693)[0;0m INFO 12-28 00:55:27 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:27 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:27 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:27 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2903693)[0;0m INFO 12-28 00:55:28 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:28 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:28 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:28 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2903693)[0;0m INFO 12-28 00:55:28 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:28 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:28 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:28 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2903693)[0;0m INFO 12-28 00:55:28 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:28 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:28 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:28 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2903690)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP3 pid=2903693)[0;0m INFO 12-28 00:55:29 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:29 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:30 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP0 pid=2903690)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.76it/s]
[1;36m(Worker_TP3 pid=2903693)[0;0m INFO 12-28 00:55:30 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.410179 seconds
[1;36m(Worker_TP0 pid=2903690)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.41it/s]
[1;36m(Worker_TP0 pid=2903690)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.28it/s]
[1;36m(Worker_TP0 pid=2903690)[0;0m 
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:30 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:30 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.717357 seconds
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:30 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.820937 seconds
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:31 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.212241 seconds
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:34 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2903693)[0;0m INFO 12-28 00:55:34 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:34 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:34 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(Worker_TP3 pid=2903693)[0;0m WARNING 12-28 00:55:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2903690)[0;0m WARNING 12-28 00:55:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2903692)[0;0m WARNING 12-28 00:55:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2903691)[0;0m WARNING 12-28 00:55:35 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.37 seconds
[1;36m(EngineCore_DP0 pid=2902964)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2902964)[0;0m 
[1;36m(EngineCore_DP0 pid=2902964)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2902964)[0;0m 
[1;36m(EngineCore_DP0 pid=2902964)[0;0m INFO 12-28 00:55:35 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:55:36 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1403.18it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:08,  1.30s/it, est. speed input: 44.65 toks/s, output: 1.54 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:01<00:43,  2.23it/s, est. speed input: 463.75 toks/s, output: 3.74 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:01<00:15,  5.99it/s, est. speed input: 1314.27 toks/s, output: 10.36 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:01<00:11,  7.66it/s, est. speed input: 1395.29 toks/s, output: 15.70 toks/s]Processed prompts:  14%|â–ˆâ–        | 14/100 [00:02<00:07, 11.80it/s, est. speed input: 1970.39 toks/s, output: 30.36 toks/s]Processed prompts:  19%|â–ˆâ–‰        | 19/100 [00:02<00:04, 16.67it/s, est. speed input: 2851.10 toks/s, output: 46.97 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:02<00:03, 19.06it/s, est. speed input: 2775.19 toks/s, output: 66.64 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 21.39it/s, est. speed input: 3004.25 toks/s, output: 80.76 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:02<00:03, 20.69it/s, est. speed input: 2898.89 toks/s, output: 91.87 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:02<00:03, 19.78it/s, est. speed input: 2946.60 toks/s, output: 105.87 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:03<00:03, 18.23it/s, est. speed input: 2929.12 toks/s, output: 117.20 toks/s]Processed prompts:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:03<00:02, 20.23it/s, est. speed input: 3084.82 toks/s, output: 140.61 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:03<00:02, 20.92it/s, est. speed input: 3270.04 toks/s, output: 163.75 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:03<00:02, 20.77it/s, est. speed input: 3174.71 toks/s, output: 188.15 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.92it/s, est. speed input: 3329.70 toks/s, output: 214.82 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:03<00:02, 20.47it/s, est. speed input: 3318.76 toks/s, output: 241.62 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:04<00:02, 19.59it/s, est. speed input: 3340.88 toks/s, output: 262.77 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:04<00:02, 18.13it/s, est. speed input: 3218.56 toks/s, output: 295.40 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:04<00:02, 17.35it/s, est. speed input: 3271.64 toks/s, output: 322.33 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:02, 16.79it/s, est. speed input: 3209.75 toks/s, output: 345.72 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:04<00:02, 15.57it/s, est. speed input: 3182.89 toks/s, output: 368.74 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:04<00:02, 13.50it/s, est. speed input: 3282.23 toks/s, output: 388.32 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:05<00:02, 11.39it/s, est. speed input: 3300.09 toks/s, output: 407.97 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:05<00:01, 14.14it/s, est. speed input: 3463.55 toks/s, output: 452.91 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:05<00:02, 11.54it/s, est. speed input: 3314.83 toks/s, output: 468.21 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:05<00:01, 11.28it/s, est. speed input: 3231.40 toks/s, output: 496.27 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:05<00:01, 11.10it/s, est. speed input: 3203.88 toks/s, output: 524.82 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:06<00:01, 15.30it/s, est. speed input: 3173.78 toks/s, output: 600.83 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:06<00:01, 13.79it/s, est. speed input: 3256.50 toks/s, output: 628.62 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:06<00:01, 10.38it/s, est. speed input: 3112.22 toks/s, output: 643.76 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:06<00:01,  8.02it/s, est. speed input: 3000.76 toks/s, output: 657.73 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:07<00:01,  4.91it/s, est. speed input: 2697.98 toks/s, output: 639.98 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  3.79it/s, est. speed input: 2534.53 toks/s, output: 627.06 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:08<00:00,  7.77it/s, est. speed input: 2628.18 toks/s, output: 791.73 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  7.77it/s, est. speed input: 2541.81 toks/s, output: 790.18 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.22it/s, est. speed input: 2541.81 toks/s, output: 790.18 toks/s]
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:45 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2903690)[0;0m INFO 12-28 00:55:45 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:45 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2903691)[0;0m INFO 12-28 00:55:45 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:45 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2903692)[0;0m INFO 12-28 00:55:45 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2903693)[0;0m INFO 12-28 00:55:45 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:55:51] ax.service.ax_client: Completed trial 31 with data: {'throughput(token/s)': 783.771348, 'energy(J/token)': 1.412953}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:55:51] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.3584116189357538), ObjectiveThreshold(throughput(token/s) >= 235.08285588977247)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:55:53] ax.service.ax_client: Generated new trial 32 with parameters {'block_size': 32, 'max_num_seqs': 64, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:55:45', project_name='codecarbon', run_id='b0245a86-5b7a-4871-a596-90bc0c0149c4', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=43.69507145800162, emissions=0.000656577450806296, emissions_rate=1.5026350315901838e-05, cpu_power=30.20231381571429, gpu_power=244.08118547768416, ram_power=70.0, cpu_energy=0.0003526968787660236, gpu_energy=0.001591890995734424, ram_energy=0.0008189131748170541, energy_consumed=0.002763501049317502, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 22/30 completed. Throughput: 783.77, Energy: 1.412953. Pareto size: 5.
INFO 12-28 00:55:55 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:55:55 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:55:55 [model.py:1510] Using max model len 32768
INFO 12-28 00:55:55 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:55:55 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:56:02 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:05 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:05 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2905668)[0;0m WARNING 12-28 00:56:05 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_80d2b0de'), local_subscribe_addr='ipc:///tmp/9b2f3a61-1864-45ab-93b7-66c967e2f2f4', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:56:12 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:56:12 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:56:12 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:56:13 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:56:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8662a5c1'), local_subscribe_addr='ipc:///tmp/4ca8f837-e966-43b5-bced-4e387322c4d5', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:56:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_93d20ace'), local_subscribe_addr='ipc:///tmp/86755a1c-f928-4329-b782-f4b161809d10', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:56:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3335a79d'), local_subscribe_addr='ipc:///tmp/240918de-4763-4566-be73-c002372b209f', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:56:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_836327e8'), local_subscribe_addr='ipc:///tmp/be4866af-67e5-4c8a-928f-f6b749950c94', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:56:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:56:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:56:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:56:18 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:56:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:56:18 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:56:18 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:56:18 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:56:19 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:56:19 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:56:19 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:56:19 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:56:19 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_994804e9'), local_subscribe_addr='ipc:///tmp/e6bb0c1c-13f4-4159-93f4-9c50b7adea4b', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:56:19 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:56:19 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:56:19 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:56:19 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:56:19 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:56:19 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:56:19 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:56:19 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:56:19 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:56:19 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:56:19 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:56:19 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
WARNING 12-28 00:56:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:56:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:56:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:56:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2905864)[0;0m INFO 12-28 00:56:19 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2905864)[0;0m INFO 12-28 00:56:19 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:19 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2905864)[0;0m INFO 12-28 00:56:20 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP3 pid=2905864)[0;0m INFO 12-28 00:56:20 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2905861)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:21 [default_loader.py:267] Loading weights took 0.74 seconds
[1;36m(Worker_TP0 pid=2905861)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.06it/s]
[1;36m(Worker_TP3 pid=2905864)[0;0m INFO 12-28 00:56:21 [default_loader.py:267] Loading weights took 0.89 seconds
[1;36m(Worker_TP0 pid=2905861)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.85it/s]
[1;36m(Worker_TP0 pid=2905861)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.70it/s]
[1;36m(Worker_TP0 pid=2905861)[0;0m 
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:21 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:21 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.338935 seconds
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:22 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP3 pid=2905864)[0;0m INFO 12-28 00:56:22 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.674162 seconds
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:22 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.885248 seconds
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:22 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.100530 seconds
[1;36m(Worker_TP3 pid=2905864)[0;0m INFO 12-28 00:56:26 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:26 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:26 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:26 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [kv_cache_utils.py:1087] GPU KV cache size: 518,976 tokens
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.61x
[1;36m(Worker_TP1 pid=2905862)[0;0m WARNING 12-28 00:56:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2905863)[0;0m WARNING 12-28 00:56:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2905864)[0;0m WARNING 12-28 00:56:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2905861)[0;0m WARNING 12-28 00:56:26 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:26 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.23 seconds
[1;36m(EngineCore_DP0 pid=2905668)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2905668)[0;0m 
[1;36m(EngineCore_DP0 pid=2905668)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2905668)[0;0m 
[1;36m(EngineCore_DP0 pid=2905668)[0;0m INFO 12-28 00:56:27 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:56:27 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1389.60it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:10,  1.31s/it, est. speed input: 118.79 toks/s, output: 0.76 toks/s]Processed prompts:   2%|â–         | 2/100 [00:01<01:10,  1.39it/s, est. speed input: 423.13 toks/s, output: 2.47 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:01<00:21,  4.36it/s, est. speed input: 1109.83 toks/s, output: 8.13 toks/s]WARNING 12-28 00:56:29 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:   8%|â–Š         | 8/100 [00:01<00:13,  7.04it/s, est. speed input: 1424.61 toks/s, output: 16.38 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:07, 12.20it/s, est. speed input: 2095.42 toks/s, output: 34.59 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:02<00:06, 13.69it/s, est. speed input: 2277.98 toks/s, output: 46.96 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:02<00:03, 19.70it/s, est. speed input: 2747.19 toks/s, output: 70.81 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:02<00:03, 20.76it/s, est. speed input: 2844.31 toks/s, output: 75.78 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 21.40it/s, est. speed input: 2946.33 toks/s, output: 93.42 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:02<00:03, 19.07it/s, est. speed input: 3055.89 toks/s, output: 104.43 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:02<00:03, 21.15it/s, est. speed input: 2995.43 toks/s, output: 120.69 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:03<00:03, 19.79it/s, est. speed input: 3228.19 toks/s, output: 136.12 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:03<00:02, 20.85it/s, est. speed input: 3384.67 toks/s, output: 154.57 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.65it/s, est. speed input: 3560.49 toks/s, output: 184.13 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 19.82it/s, est. speed input: 3520.19 toks/s, output: 218.32 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:03<00:02, 20.04it/s, est. speed input: 3446.49 toks/s, output: 243.97 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:04<00:02, 16.20it/s, est. speed input: 3299.49 toks/s, output: 262.72 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:04<00:02, 14.75it/s, est. speed input: 3189.60 toks/s, output: 293.57 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:04<00:01, 18.48it/s, est. speed input: 3365.21 toks/s, output: 352.27 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:04<00:02, 14.21it/s, est. speed input: 3211.20 toks/s, output: 370.44 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:05<00:02, 13.86it/s, est. speed input: 3185.74 toks/s, output: 395.55 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:01, 14.09it/s, est. speed input: 3234.18 toks/s, output: 435.37 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:01, 13.89it/s, est. speed input: 3195.56 toks/s, output: 471.73 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:01, 13.33it/s, est. speed input: 3295.00 toks/s, output: 494.71 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 10.93it/s, est. speed input: 3155.00 toks/s, output: 512.93 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:02,  8.60it/s, est. speed input: 2987.17 toks/s, output: 528.30 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01,  9.10it/s, est. speed input: 3041.02 toks/s, output: 558.74 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01,  9.95it/s, est. speed input: 3006.11 toks/s, output: 592.94 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01, 10.12it/s, est. speed input: 3009.28 toks/s, output: 622.50 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:06<00:00, 11.32it/s, est. speed input: 2977.71 toks/s, output: 658.77 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:08<00:02,  4.35it/s, est. speed input: 2561.70 toks/s, output: 619.29 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  4.37it/s, est. speed input: 2508.74 toks/s, output: 630.92 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.48it/s, est. speed input: 2469.53 toks/s, output: 646.08 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00,  8.07it/s, est. speed input: 2497.72 toks/s, output: 807.02 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00,  8.07it/s, est. speed input: 2497.72 toks/s, output: 807.02 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:09<00:00, 11.03it/s, est. speed input: 2497.72 toks/s, output: 807.02 toks/s]
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:37 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2905861)[0;0m INFO 12-28 00:56:37 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:37 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2905862)[0;0m INFO 12-28 00:56:37 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:37 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2905863)[0;0m INFO 12-28 00:56:37 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2905864)[0;0m INFO 12-28 00:56:37 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:56:42] ax.service.ax_client: Completed trial 32 with data: {'throughput(token/s)': 800.530841, 'energy(J/token)': 1.320396}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:56:42] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.358411680916408), ObjectiveThreshold(throughput(token/s) >= 235.08310488747804)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:56:44] ax.service.ax_client: Generated new trial 33 with parameters {'block_size': 32, 'max_num_seqs': 128, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:56:37', project_name='codecarbon', run_id='ef28d624-7480-41b6-ac9d-936f9217626d', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=42.25696449098177, emissions=0.0006377059414095741, emissions_rate=1.5091144124790825e-05, cpu_power=30.18002011153846, gpu_power=247.56335519244917, ram_power=70.0, cpu_energy=0.00034027649364493513, gpu_energy=0.0015527695755466198, ram_energy=0.000791025758248159, energy_consumed=0.0026840718274397136, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 23/30 completed. Throughput: 800.53, Energy: 1.320396. Pareto size: 5.
INFO 12-28 00:56:46 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:56:47 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:56:47 [model.py:1510] Using max model len 32768
INFO 12-28 00:56:47 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:56:47 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:56:54 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:56:57 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:56:57 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2906852)[0;0m WARNING 12-28 00:56:57 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:56:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_4bba7700'), local_subscribe_addr='ipc:///tmp/918e0b3f-3089-4c90-a055-cafc04c4b407', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:57:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:57:04 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:57:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f631121b'), local_subscribe_addr='ipc:///tmp/53e2e8cb-b58b-485d-a3ed-3f84b7ee3aa2', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:57:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2e0c33dd'), local_subscribe_addr='ipc:///tmp/8997c71f-0518-4990-a4f9-1cd9d3110684', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:57:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:57:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:57:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:57:10 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:57:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:57:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:57:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_abfae4af'), local_subscribe_addr='ipc:///tmp/f043b820-5384-49d8-a05d-4b9f60e55106', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:57:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:57:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:57:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:57:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:57:10 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 00:57:10 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 00:57:10 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:57:10 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2907080)[0;0m INFO 12-28 00:57:10 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:10 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:11 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2907080)[0;0m INFO 12-28 00:57:11 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:11 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2907080)[0;0m INFO 12-28 00:57:11 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2907080)[0;0m INFO 12-28 00:57:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2907079)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2907079)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.45s/it]
[1;36m(Worker_TP1 pid=2907080)[0;0m INFO 12-28 00:57:13 [default_loader.py:267] Loading weights took 1.80 seconds
[1;36m(Worker_TP0 pid=2907079)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.04s/it]
[1;36m(Worker_TP0 pid=2907079)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
[1;36m(Worker_TP0 pid=2907079)[0;0m 
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:13 [default_loader.py:267] Loading weights took 2.23 seconds
[1;36m(Worker_TP1 pid=2907080)[0;0m INFO 12-28 00:57:14 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 2.610349 seconds
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:14 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 2.835316 seconds
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:18 [gpu_worker.py:298] Available KV cache memory: 12.24 GiB
[1;36m(Worker_TP1 pid=2907080)[0;0m INFO 12-28 00:57:18 [gpu_worker.py:298] Available KV cache memory: 12.24 GiB
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:57:19 [kv_cache_utils.py:1087] GPU KV cache size: 200,576 tokens
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:57:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.22x
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:57:19 [kv_cache_utils.py:1087] GPU KV cache size: 200,576 tokens
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:57:19 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.22x
[1;36m(Worker_TP0 pid=2907079)[0;0m WARNING 12-28 00:57:19 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2907080)[0;0m WARNING 12-28 00:57:19 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:57:19 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.65 seconds
[1;36m(EngineCore_DP0 pid=2906852)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2906852)[0;0m 
[1;36m(EngineCore_DP0 pid=2906852)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2906852)[0;0m 
[1;36m(EngineCore_DP0 pid=2906852)[0;0m INFO 12-28 00:57:19 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:57:20 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1394.57it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<03:10,  1.93s/it, est. speed input: 81.03 toks/s, output: 0.52 toks/s]Processed prompts:   2%|â–         | 2/100 [00:03<02:50,  1.74s/it, est. speed input: 67.40 toks/s, output: 0.57 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:32,  2.83it/s, est. speed input: 668.50 toks/s, output: 4.66 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:03<00:13,  6.30it/s, est. speed input: 959.49 toks/s, output: 11.44 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:03<00:08,  9.74it/s, est. speed input: 1376.73 toks/s, output: 19.39 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:03<00:04, 15.81it/s, est. speed input: 1797.38 toks/s, output: 32.98 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:04<00:04, 16.77it/s, est. speed input: 2110.93 toks/s, output: 45.14 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:04<00:03, 19.76it/s, est. speed input: 2241.82 toks/s, output: 58.39 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:04<00:02, 24.33it/s, est. speed input: 2445.56 toks/s, output: 81.04 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:04<00:02, 20.54it/s, est. speed input: 2656.76 toks/s, output: 94.50 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:05<00:03, 16.58it/s, est. speed input: 2544.52 toks/s, output: 111.10 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:05<00:03, 13.49it/s, est. speed input: 2390.47 toks/s, output: 125.15 toks/s]WARNING 12-28 00:57:25 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:05<00:03, 13.61it/s, est. speed input: 2388.49 toks/s, output: 155.51 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:06<00:03, 12.54it/s, est. speed input: 2334.90 toks/s, output: 167.58 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:06<00:03, 10.68it/s, est. speed input: 2335.21 toks/s, output: 184.58 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:06<00:03,  9.87it/s, est. speed input: 2267.38 toks/s, output: 197.37 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:06<00:02, 11.91it/s, est. speed input: 2303.92 toks/s, output: 225.05 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:07<00:02, 11.84it/s, est. speed input: 2293.38 toks/s, output: 240.85 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:07<00:02, 10.55it/s, est. speed input: 2234.69 toks/s, output: 254.61 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:07<00:02,  9.70it/s, est. speed input: 2213.68 toks/s, output: 268.67 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:08<00:03,  7.34it/s, est. speed input: 2103.78 toks/s, output: 277.03 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:08<00:01, 10.71it/s, est. speed input: 2208.97 toks/s, output: 330.69 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:08<00:01, 11.64it/s, est. speed input: 2227.35 toks/s, output: 351.57 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:08<00:02,  8.25it/s, est. speed input: 2126.00 toks/s, output: 359.05 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:09<00:01,  8.28it/s, est. speed input: 2162.94 toks/s, output: 385.56 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:09<00:01,  9.20it/s, est. speed input: 2221.61 toks/s, output: 408.16 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:09<00:01,  7.94it/s, est. speed input: 2154.45 toks/s, output: 422.90 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:09<00:00,  8.34it/s, est. speed input: 2125.36 toks/s, output: 443.94 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:10<00:01,  3.77it/s, est. speed input: 1934.37 toks/s, output: 419.01 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:13<00:03,  1.56it/s, est. speed input: 1603.52 toks/s, output: 365.65 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:13<00:02,  1.78it/s, est. speed input: 1575.95 toks/s, output: 377.34 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  1.78it/s, est. speed input: 1678.65 toks/s, output: 471.30 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  7.41it/s, est. speed input: 1678.65 toks/s, output: 471.30 toks/s]
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:34 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2907079)[0;0m INFO 12-28 00:57:34 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2907080)[0;0m INFO 12-28 00:57:34 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:57:38] ax.service.ax_client: Completed trial 33 with data: {'throughput(token/s)': 468.756251, 'energy(J/token)': 1.312788}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:57:39] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.3584105632900734), ObjectiveThreshold(throughput(token/s) >= 235.08316908324088)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:57:41] ax.service.ax_client: Generated new trial 34 with parameters {'block_size': 32, 'max_num_seqs': 128, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 1, 'pipeline_parallel_size': 2, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:57:34', project_name='codecarbon', run_id='a0eb118c-ae31-4a20-974f-39ed64b21b8b', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=47.89896411204245, emissions=0.0005509438830028442, emissions_rate=1.1502208726562614e-05, cpu_power=30.033711389999997, gpu_power=124.19206882588529, ram_power=70.0, cpu_energy=0.00038236533425922866, gpu_energy=0.0010462505592210292, ram_energy=0.0008902788491334328, energy_consumed=0.0023188947426136908, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 24/30 completed. Throughput: 468.76, Energy: 1.312788. Pareto size: 5.
INFO 12-28 00:57:43 [utils.py:233] non-default args: {'seed': 42, 'pipeline_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 128, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:57:43 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:57:43 [model.py:1510] Using max model len 32768
INFO 12-28 00:57:43 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:57:43 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:57:52 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:57:55 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:57:55 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=2, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2907999)[0;0m WARNING 12-28 00:57:55 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:57:55 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_657c0c96'), local_subscribe_addr='ipc:///tmp/54e36f3e-fc77-48c8-b75c-fee595e62ef9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:58:02 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:58:02 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:58:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d4f38dee'), local_subscribe_addr='ipc:///tmp/234efa88-9c80-4bbb-9653-70f5dd51884d', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:58:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3679a6b9'), local_subscribe_addr='ipc:///tmp/84277fdf-59f5-469a-b29a-d7b78855d368', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 00:58:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:58:08 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:58:08 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:58:08 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 00:58:08 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 1, TP rank 0, EP rank 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
INFO 12-28 00:58:08 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 00:58:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:58:08 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=2908252)[0;0m INFO 12-28 00:58:08 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_PP1 pid=2908252)[0;0m INFO 12-28 00:58:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:08 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_PP1 pid=2908252)[0;0m INFO 12-28 00:58:09 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:09 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP1 pid=2908252)[0;0m INFO 12-28 00:58:09 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_PP0 pid=2908251)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_PP0 pid=2908251)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.23it/s]
[1;36m(Worker_PP0 pid=2908251)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.28it/s]
[1;36m(Worker_PP0 pid=2908251)[0;0m 
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:10 [default_loader.py:267] Loading weights took 0.88 seconds
[1;36m(Worker_PP1 pid=2908252)[0;0m INFO 12-28 00:58:10 [default_loader.py:267] Loading weights took 0.88 seconds
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:11 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 1.516979 seconds
[1;36m(Worker_PP1 pid=2908252)[0;0m INFO 12-28 00:58:11 [gpu_model_runner.py:2653] Model loading took 6.7523 GiB and 1.763978 seconds
[1;36m(Worker_PP1 pid=2908252)[0;0m INFO 12-28 00:58:13 [gpu_worker.py:298] Available KV cache memory: 11.68 GiB
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:13 [gpu_worker.py:298] Available KV cache memory: 11.77 GiB
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:58:13 [kv_cache_utils.py:1087] GPU KV cache size: 192,864 tokens
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:58:13 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 11.75x
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:58:13 [kv_cache_utils.py:1087] GPU KV cache size: 191,296 tokens
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:58:13 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 11.65x
[1;36m(Worker_PP1 pid=2908252)[0;0m WARNING 12-28 00:58:13 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:58:14 [core.py:210] init engine (profile, create kv cache, warmup model) took 2.78 seconds
[1;36m(EngineCore_DP0 pid=2907999)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2907999)[0;0m 
[1;36m(EngineCore_DP0 pid=2907999)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2907999)[0;0m 
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:58:14 [core.py:149] Batch queue is enabled with size 2
[1;36m(EngineCore_DP0 pid=2907999)[0;0m INFO 12-28 00:58:14 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:58:14 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s][1;36m(Worker_PP0 pid=2908251)[0;0m WARNING 12-28 00:58:14 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_PP0 pid=2908251)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/distributed/parallel_state.py:516: UserWarning:
[1;36m(Worker_PP0 pid=2908251)[0;0m 
[1;36m(Worker_PP0 pid=2908251)[0;0m The given buffer is not writable, and PyTorch does not support non-writable tensors. This means you can write to the underlying (supposedly non-writable) buffer using the tensor. You may want to copy the buffer to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:1578.)
[1;36m(Worker_PP0 pid=2908251)[0;0m 
Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1398.39it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:03<05:16,  3.19s/it, est. speed input: 48.87 toks/s, output: 0.31 toks/s]Processed prompts:   3%|â–Ž         | 3/100 [00:04<02:06,  1.31s/it, est. speed input: 134.36 toks/s, output: 0.67 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:04<01:03,  1.50it/s, est. speed input: 258.60 toks/s, output: 1.73 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:04<00:19,  4.49it/s, est. speed input: 711.26 toks/s, output: 5.70 toks/s]Processed prompts:  15%|â–ˆâ–Œ        | 15/100 [00:04<00:12,  6.81it/s, est. speed input: 809.79 toks/s, output: 9.66 toks/s]Processed prompts:  21%|â–ˆâ–ˆ        | 21/100 [00:04<00:07, 11.27it/s, est. speed input: 1276.69 toks/s, output: 17.25 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:05<00:05, 12.99it/s, est. speed input: 1467.94 toks/s, output: 23.14 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:05<00:04, 16.37it/s, est. speed input: 1602.20 toks/s, output: 32.55 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:05<00:03, 17.33it/s, est. speed input: 1706.55 toks/s, output: 40.78 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:05<00:03, 16.83it/s, est. speed input: 1757.11 toks/s, output: 47.58 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:05<00:03, 16.47it/s, est. speed input: 1819.62 toks/s, output: 55.30 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:06<00:04, 13.29it/s, est. speed input: 1743.87 toks/s, output: 62.76 toks/s]Processed prompts:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:06<00:06,  8.74it/s, est. speed input: 1616.88 toks/s, output: 66.44 toks/s]Processed prompts:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:06<00:05,  9.84it/s, est. speed input: 1677.05 toks/s, output: 74.58 toks/s]Processed prompts:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:07<00:06,  8.43it/s, est. speed input: 1617.49 toks/s, output: 80.94 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:07<00:06,  8.16it/s, est. speed input: 1573.42 toks/s, output: 88.27 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:07<00:04,  9.35it/s, est. speed input: 1617.89 toks/s, output: 105.75 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:08<00:04, 10.06it/s, est. speed input: 1630.75 toks/s, output: 115.35 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:08<00:07,  5.78it/s, est. speed input: 1587.43 toks/s, output: 116.92 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:08<00:04,  8.87it/s, est. speed input: 1626.85 toks/s, output: 141.42 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:09<00:04,  8.28it/s, est. speed input: 1727.32 toks/s, output: 150.10 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:10<00:07,  4.46it/s, est. speed input: 1560.03 toks/s, output: 148.74 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:10<00:06,  4.60it/s, est. speed input: 1542.35 toks/s, output: 158.44 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:11<00:08,  3.49it/s, est. speed input: 1457.74 toks/s, output: 157.25 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:11<00:07,  3.92it/s, est. speed input: 1448.91 toks/s, output: 163.80 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:11<00:06,  4.48it/s, est. speed input: 1460.49 toks/s, output: 170.48 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:11<00:06,  4.41it/s, est. speed input: 1432.91 toks/s, output: 175.32 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:11<00:05,  4.89it/s, est. speed input: 1501.10 toks/s, output: 181.68 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:12<00:06,  3.88it/s, est. speed input: 1454.46 toks/s, output: 184.25 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:12<00:07,  3.37it/s, est. speed input: 1421.59 toks/s, output: 187.28 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:13<00:05,  4.04it/s, est. speed input: 1446.98 toks/s, output: 199.47 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:13<00:04,  4.54it/s, est. speed input: 1442.40 toks/s, output: 206.43 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:13<00:04,  4.03it/s, est. speed input: 1412.71 toks/s, output: 210.53 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:13<00:04,  4.25it/s, est. speed input: 1397.21 toks/s, output: 216.69 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:13<00:03,  4.61it/s, est. speed input: 1384.83 toks/s, output: 223.40 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:14<00:02,  5.58it/s, est. speed input: 1369.49 toks/s, output: 237.81 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:14<00:02,  5.70it/s, est. speed input: 1355.32 toks/s, output: 244.57 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:14<00:03,  4.18it/s, est. speed input: 1374.02 toks/s, output: 247.18 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:16<00:07,  1.82it/s, est. speed input: 1259.71 toks/s, output: 236.00 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:17<00:08,  1.35it/s, est. speed input: 1170.83 toks/s, output: 229.74 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:18<00:07,  1.46it/s, est. speed input: 1139.97 toks/s, output: 233.32 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:18<00:05,  1.75it/s, est. speed input: 1125.66 toks/s, output: 240.32 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:19<00:07,  1.23it/s, est. speed input: 1059.97 toks/s, output: 234.39 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:21<00:08,  1.09s/it, est. speed input: 978.25 toks/s, output: 226.43 toks/s] Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:22<00:06,  1.03it/s, est. speed input: 951.78 toks/s, output: 231.07 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:22<00:00,  1.03it/s, est. speed input: 1021.14 toks/s, output: 311.54 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:22<00:00,  4.51it/s, est. speed input: 1021.14 toks/s, output: 311.54 toks/s]
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:37 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_PP0 pid=2908251)[0;0m INFO 12-28 00:58:37 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_PP1 pid=2908252)[0;0m INFO 12-28 00:58:37 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:58:42] ax.service.ax_client: Completed trial 34 with data: {'throughput(token/s)': 310.520595, 'energy(J/token)': 1.446191}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:58:42] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.358410325078568), ObjectiveThreshold(throughput(token/s) >= 235.0832910232093)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 00:58:45] ax.service.ax_client: Generated new trial 35 with parameters {'block_size': 128, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:58:37', project_name='codecarbon', run_id='582b31b1-212c-4b54-8f61-e59b2df5070f', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=54.712569342926145, emissions=0.0006595195068576463, emissions_rate=1.2054259465022115e-05, cpu_power=30.028258713000003, gpu_power=139.09591957605883, ram_power=70.0, cpu_energy=0.00043885634438542626, gpu_energy=0.0013141932735756257, ram_energy=0.0010228343954303354, energy_consumed=0.002775884013391387, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 25/30 completed. Throughput: 310.52, Energy: 1.446191. Pareto size: 5.
INFO 12-28 00:58:47 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 128, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:58:47 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:58:47 [model.py:1510] Using max model len 32768
INFO 12-28 00:58:47 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:58:47 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:58:54 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:58:57 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:58:57 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2909244)[0;0m WARNING 12-28 00:58:57 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:58:57 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_8c72e73c'), local_subscribe_addr='ipc:///tmp/945cc08d-4374-482b-8356-fce768609e0e', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:59:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:59:03 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:59:04 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:59:04 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:59:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_ebc0f313'), local_subscribe_addr='ipc:///tmp/458c7c2c-73c0-4071-9990-6b9e23381bc9', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:59:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_18ea4110'), local_subscribe_addr='ipc:///tmp/367ac4f2-7e22-47fd-ba3f-2c8d43562727', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:59:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_91ab5ae0'), local_subscribe_addr='ipc:///tmp/cfa50e8b-1172-4072-bb54-20fa8cdfea49', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:59:09 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a3f0456e'), local_subscribe_addr='ipc:///tmp/89ace004-7cb0-49fd-a24e-12e7e69ee6cc', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:59:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:59:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:59:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:59:10 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:59:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:59:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:59:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:59:10 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 00:59:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:59:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:59:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 00:59:10 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 00:59:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_59daae7f'), local_subscribe_addr='ipc:///tmp/33dc75b0-970b-412f-9014-c74991f70314', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 00:59:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:59:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:59:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:59:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:59:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:59:10 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 00:59:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:59:10 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 00:59:10 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 00:59:10 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 00:59:10 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 00:59:10 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 00:59:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:59:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:59:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 00:59:11 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:11 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2909453)[0;0m INFO 12-28 00:59:11 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:11 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:11 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2909453)[0;0m INFO 12-28 00:59:11 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:11 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:11 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:11 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2909453)[0;0m INFO 12-28 00:59:11 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:11 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:11 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:11 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2909453)[0;0m INFO 12-28 00:59:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:11 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2909450)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP3 pid=2909453)[0;0m INFO 12-28 00:59:12 [default_loader.py:267] Loading weights took 0.75 seconds
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:13 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP0 pid=2909450)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.04it/s]
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:13 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP3 pid=2909453)[0;0m INFO 12-28 00:59:13 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.384591 seconds
[1;36m(Worker_TP0 pid=2909450)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.84it/s]
[1;36m(Worker_TP0 pid=2909450)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.68it/s]
[1;36m(Worker_TP0 pid=2909450)[0;0m 
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:13 [default_loader.py:267] Loading weights took 0.77 seconds
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:13 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.593184 seconds
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:13 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.860219 seconds
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:14 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.113360 seconds
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:18 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:18 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2909453)[0;0m INFO 12-28 00:59:18 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:18 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(Worker_TP0 pid=2909450)[0;0m WARNING 12-28 00:59:18 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP3 pid=2909453)[0;0m WARNING 12-28 00:59:18 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2909451)[0;0m WARNING 12-28 00:59:18 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2909452)[0;0m WARNING 12-28 00:59:18 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:18 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.53 seconds
[1;36m(EngineCore_DP0 pid=2909244)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2909244)[0;0m 
[1;36m(EngineCore_DP0 pid=2909244)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2909244)[0;0m 
[1;36m(EngineCore_DP0 pid=2909244)[0;0m INFO 12-28 00:59:19 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 00:59:19 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1406.70it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:12,  1.34s/it, est. speed input: 116.71 toks/s, output: 0.75 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:58,  1.21s/it, est. speed input: 96.63 toks/s, output: 0.81 toks/s] Processed prompts:   9%|â–‰         | 9/100 [00:02<00:17,  5.27it/s, est. speed input: 1053.93 toks/s, output: 9.71 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:02<00:06, 12.34it/s, est. speed input: 1986.63 toks/s, output: 27.99 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:02<00:03, 19.57it/s, est. speed input: 2739.39 toks/s, output: 51.08 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:02<00:02, 23.36it/s, est. speed input: 3243.20 toks/s, output: 80.04 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:03<00:02, 29.07it/s, est. speed input: 3629.38 toks/s, output: 119.12 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.15it/s, est. speed input: 3637.55 toks/s, output: 153.79 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 21.22it/s, est. speed input: 3513.42 toks/s, output: 195.10 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:03<00:02, 21.63it/s, est. speed input: 3525.94 toks/s, output: 234.91 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:04<00:02, 18.01it/s, est. speed input: 3284.67 toks/s, output: 267.77 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:04<00:01, 19.31it/s, est. speed input: 3431.08 toks/s, output: 305.01 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:04<00:01, 19.08it/s, est. speed input: 3395.77 toks/s, output: 339.88 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:04<00:02, 15.23it/s, est. speed input: 3248.56 toks/s, output: 366.68 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:05<00:02, 14.75it/s, est. speed input: 3235.91 toks/s, output: 389.62 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:02, 10.03it/s, est. speed input: 3176.64 toks/s, output: 395.98 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:02, 11.55it/s, est. speed input: 3158.82 toks/s, output: 442.27 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:02, 10.91it/s, est. speed input: 3059.09 toks/s, output: 467.48 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.96it/s, est. speed input: 3025.36 toks/s, output: 500.55 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:01, 10.84it/s, est. speed input: 2939.07 toks/s, output: 525.23 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01, 11.24it/s, est. speed input: 3034.31 toks/s, output: 556.55 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.60it/s, est. speed input: 2971.36 toks/s, output: 584.10 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01,  8.04it/s, est. speed input: 2832.88 toks/s, output: 599.25 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:07<00:01,  7.60it/s, est. speed input: 2776.71 toks/s, output: 610.60 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:07<00:01,  6.51it/s, est. speed input: 2693.07 toks/s, output: 616.61 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:08<00:02,  3.88it/s, est. speed input: 2484.74 toks/s, output: 596.06 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  5.27it/s, est. speed input: 2457.89 toks/s, output: 640.47 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.05it/s, est. speed input: 2331.21 toks/s, output: 636.37 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  4.05it/s, est. speed input: 2613.54 toks/s, output: 841.79 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.54it/s, est. speed input: 2613.54 toks/s, output: 841.79 toks/s]
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:28 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2909450)[0;0m INFO 12-28 00:59:28 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:28 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2909451)[0;0m INFO 12-28 00:59:28 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:28 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2909452)[0;0m INFO 12-28 00:59:28 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2909453)[0;0m INFO 12-28 00:59:28 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 00:59:33] ax.service.ax_client: Completed trial 35 with data: {'throughput(token/s)': 834.807378, 'energy(J/token)': 1.310495}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 00:59:34] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.3584088060022734), ObjectiveThreshold(throughput(token/s) >= 235.08409868711237)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

[INFO 12-28 00:59:36] ax.service.ax_client: Generated new trial 36 with parameters {'block_size': 128, 'max_num_seqs': 64, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T00:59:28', project_name='codecarbon', run_id='4fa83361-162f-4974-9dac-2c321285f311', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=41.8214401070727, emissions=0.000630934704302929, emissions_rate=1.508639355047526e-05, cpu_power=30.180021892500005, gpu_power=247.92976897569568, ram_power=70.0, cpu_energy=0.00033659645474611526, gpu_energy=0.001536515951433337, ram_energy=0.0007824596291543762, energy_consumed=0.0026555720353338285, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 26/30 completed. Throughput: 834.81, Energy: 1.310495. Pareto size: 5.
INFO 12-28 00:59:38 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 2, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 00:59:38 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 00:59:38 [model.py:1510] Using max model len 32768
INFO 12-28 00:59:38 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 00:59:38 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 00:59:47 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 00:59:50 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 00:59:50 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2910396)[0;0m WARNING 12-28 00:59:50 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 00:59:50 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_c437e38a'), local_subscribe_addr='ipc:///tmp/23f15c60-b0e7-4dde-84fe-c068adc0c4eb', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 00:59:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 00:59:57 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 01:00:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7371e0aa'), local_subscribe_addr='ipc:///tmp/da38c00c-d983-4248-9dcd-5b65faa22f4b', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 01:00:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e40ef1d1'), local_subscribe_addr='ipc:///tmp/0b669c6c-6288-481e-8a5e-d6cefd8883f3', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 01:00:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:00:02 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:00:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:00:02 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 01:00:02 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 01:00:02 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 01:00:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_eaf0b197'), local_subscribe_addr='ipc:///tmp/80bd6ffd-a530-483e-bf29-4a065f98fc09', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 01:00:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:00:02 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:00:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:00:02 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:00:02 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 01:00:02 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 01:00:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 01:00:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:03 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:03 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:03 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:03 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:03 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:03 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2910650)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2910650)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.46s/it]
[1;36m(Worker_TP0 pid=2910650)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.03s/it]
[1;36m(Worker_TP0 pid=2910650)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
[1;36m(Worker_TP0 pid=2910650)[0;0m 
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:06 [default_loader.py:267] Loading weights took 2.23 seconds
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:06 [default_loader.py:267] Loading weights took 2.24 seconds
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:07 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 2.857576 seconds
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:07 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 3.137090 seconds
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:11 [gpu_worker.py:298] Available KV cache memory: 12.24 GiB
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:11 [gpu_worker.py:298] Available KV cache memory: 12.24 GiB
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 01:00:12 [kv_cache_utils.py:1087] GPU KV cache size: 200,576 tokens
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 01:00:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.15x
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 01:00:12 [kv_cache_utils.py:1087] GPU KV cache size: 200,576 tokens
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 01:00:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.15x
[1;36m(Worker_TP0 pid=2910650)[0;0m WARNING 12-28 01:00:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2910651)[0;0m WARNING 12-28 01:00:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 01:00:12 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.89 seconds
[1;36m(EngineCore_DP0 pid=2910396)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2910396)[0;0m 
[1;36m(EngineCore_DP0 pid=2910396)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2910396)[0;0m 
[1;36m(EngineCore_DP0 pid=2910396)[0;0m INFO 12-28 01:00:12 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 01:00:12 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1378.93it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<03:09,  1.91s/it, est. speed input: 81.59 toks/s, output: 0.52 toks/s]Processed prompts:   2%|â–         | 2/100 [00:02<01:42,  1.04s/it, est. speed input: 291.77 toks/s, output: 1.70 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:02<00:31,  3.03it/s, est. speed input: 768.07 toks/s, output: 5.63 toks/s]WARNING 12-28 01:00:15 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:   7%|â–‹         | 7/100 [00:02<00:20,  4.44it/s, est. speed input: 888.46 toks/s, output: 9.12 toks/s]Processed prompts:   9%|â–‰         | 9/100 [00:02<00:14,  6.14it/s, est. speed input: 1024.56 toks/s, output: 13.86 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:02<00:11,  7.71it/s, est. speed input: 1319.76 toks/s, output: 18.09 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:09,  9.47it/s, est. speed input: 1439.51 toks/s, output: 23.76 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:03<00:07, 10.65it/s, est. speed input: 1566.97 toks/s, output: 32.30 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:03<00:05, 15.29it/s, est. speed input: 1856.55 toks/s, output: 41.66 toks/s]Processed prompts:  23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:03<00:04, 17.61it/s, est. speed input: 1901.60 toks/s, output: 49.53 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:03<00:04, 15.94it/s, est. speed input: 1931.26 toks/s, output: 56.26 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:03<00:04, 15.46it/s, est. speed input: 1909.80 toks/s, output: 64.37 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:04<00:04, 15.54it/s, est. speed input: 1979.54 toks/s, output: 67.53 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:04<00:03, 17.69it/s, est. speed input: 2079.70 toks/s, output: 81.78 toks/s]Processed prompts:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:04<00:04, 13.43it/s, est. speed input: 2020.80 toks/s, output: 84.50 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:04<00:03, 18.42it/s, est. speed input: 2329.00 toks/s, output: 99.47 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:04<00:03, 17.76it/s, est. speed input: 2404.37 toks/s, output: 111.92 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:05<00:03, 16.53it/s, est. speed input: 2437.88 toks/s, output: 133.96 toks/s]Processed prompts:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:05<00:02, 16.25it/s, est. speed input: 2434.73 toks/s, output: 160.15 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:05<00:02, 15.04it/s, est. speed input: 2387.62 toks/s, output: 172.97 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:05<00:02, 15.52it/s, est. speed input: 2473.26 toks/s, output: 191.15 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:06<00:03, 12.09it/s, est. speed input: 2376.07 toks/s, output: 197.09 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:06<00:04,  7.22it/s, est. speed input: 2211.63 toks/s, output: 196.72 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:07<00:04,  6.80it/s, est. speed input: 2302.28 toks/s, output: 210.17 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:07<00:04,  7.62it/s, est. speed input: 2365.33 toks/s, output: 226.48 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:07<00:03,  9.56it/s, est. speed input: 2378.33 toks/s, output: 257.82 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:07<00:03,  8.30it/s, est. speed input: 2287.87 toks/s, output: 271.78 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:08<00:03,  7.29it/s, est. speed input: 2320.28 toks/s, output: 283.24 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:08<00:04,  5.02it/s, est. speed input: 2188.24 toks/s, output: 278.77 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:08<00:04,  5.42it/s, est. speed input: 2165.17 toks/s, output: 289.28 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:08<00:03,  6.76it/s, est. speed input: 2144.37 toks/s, output: 312.95 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:09<00:02,  8.14it/s, est. speed input: 2134.41 toks/s, output: 356.70 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:09<00:02,  7.86it/s, est. speed input: 2110.56 toks/s, output: 366.16 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:09<00:02,  7.30it/s, est. speed input: 2071.82 toks/s, output: 373.42 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:10<00:03,  4.18it/s, est. speed input: 1976.63 toks/s, output: 364.45 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:10<00:02,  4.35it/s, est. speed input: 1917.57 toks/s, output: 381.06 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:10<00:02,  4.58it/s, est. speed input: 1894.89 toks/s, output: 391.82 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:11<00:02,  4.46it/s, est. speed input: 1880.22 toks/s, output: 400.35 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:11<00:01,  4.73it/s, est. speed input: 1920.56 toks/s, output: 411.54 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:11<00:01,  5.21it/s, est. speed input: 1903.14 toks/s, output: 424.07 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:12<00:02,  2.94it/s, est. speed input: 1794.07 toks/s, output: 414.94 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:12<00:02,  2.48it/s, est. speed input: 1724.94 toks/s, output: 414.55 toks/s]Processed prompts:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:13<00:01,  2.87it/s, est. speed input: 1662.26 toks/s, output: 434.30 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  2.87it/s, est. speed input: 1697.34 toks/s, output: 509.30 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  7.49it/s, est. speed input: 1697.34 toks/s, output: 509.30 toks/s]
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:26 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2910650)[0;0m INFO 12-28 01:00:26 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:26 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2910651)[0;0m INFO 12-28 01:00:26 [multiproc_executor.py:599] WorkerProc shutting down.
[INFO 12-28 01:00:31] ax.service.ax_client: Completed trial 36 with data: {'throughput(token/s)': 506.491451, 'energy(J/token)': 1.254875}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 01:00:32] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.3584090299329303), ObjectiveThreshold(throughput(token/s) >= 235.08327208107914)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 01:00:34] ax.service.ax_client: Generated new trial 37 with parameters {'block_size': 128, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T01:00:26', project_name='codecarbon', run_id='cf173750-3f60-4d97-8323-60b633262bfb', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=49.15815603395458, emissions=0.0005628307137011563, emissions_rate=1.1449386207904079e-05, cpu_power=30.054971028, gpu_power=131.31462855281987, ram_power=70.0, cpu_energy=0.00039314362960848995, gpu_energy=0.0010609222376265492, ram_energy=0.0009148599403619302, energy_consumed=0.002368925807596969, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 27/30 completed. Throughput: 506.49, Energy: 1.254875. Pareto size: 6.
INFO 12-28 01:00:36 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 2, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 256, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 01:00:37 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 01:00:37 [model.py:1510] Using max model len 32768
INFO 12-28 01:00:37 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 01:00:37 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 01:00:45 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:00:49 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:00:49 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2914595)[0;0m WARNING 12-28 01:00:49 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:00:49 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_387af695'), local_subscribe_addr='ipc:///tmp/5c00b26a-8f8d-4115-9504-6fc5ba6eb546', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 01:00:55 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 01:00:56 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 01:00:59 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_68779425'), local_subscribe_addr='ipc:///tmp/233fb0ea-1696-44f3-bae4-8700ff11d1c8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 01:01:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_825d32e1'), local_subscribe_addr='ipc:///tmp/6464f126-2a52-480c-8ee8-e002f4a9a945', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 01:01:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:01:02 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:01:02 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:01:02 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 01:01:02 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 01:01:02 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 01:01:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_1907717a'), local_subscribe_addr='ipc:///tmp/6673431c-f993-4f62-a523-574987675ab5', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-28 01:01:03 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:01:03 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:01:03 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:01:03 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:01:03 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-28 01:01:03 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-28 01:01:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 01:01:03 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=2914838)[0;0m INFO 12-28 01:01:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:03 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2914838)[0;0m INFO 12-28 01:01:03 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:03 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2914838)[0;0m INFO 12-28 01:01:03 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:03 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:04 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2914838)[0;0m INFO 12-28 01:01:04 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2914837)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=2914837)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.37s/it]
[1;36m(Worker_TP0 pid=2914837)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.09it/s]
[1;36m(Worker_TP0 pid=2914837)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.01it/s]
[1;36m(Worker_TP0 pid=2914837)[0;0m 
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:06 [default_loader.py:267] Loading weights took 2.00 seconds
[1;36m(Worker_TP1 pid=2914838)[0;0m INFO 12-28 01:01:06 [default_loader.py:267] Loading weights took 2.23 seconds
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:06 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 2.636440 seconds
[1;36m(Worker_TP1 pid=2914838)[0;0m INFO 12-28 01:01:07 [gpu_model_runner.py:2653] Model loading took 6.7545 GiB and 3.117560 seconds
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:11 [gpu_worker.py:298] Available KV cache memory: 12.24 GiB
[1;36m(Worker_TP1 pid=2914838)[0;0m INFO 12-28 01:01:11 [gpu_worker.py:298] Available KV cache memory: 12.24 GiB
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:01:12 [kv_cache_utils.py:1087] GPU KV cache size: 200,576 tokens
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:01:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.15x
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:01:12 [kv_cache_utils.py:1087] GPU KV cache size: 200,576 tokens
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:01:12 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 12.15x
[1;36m(Worker_TP0 pid=2914837)[0;0m WARNING 12-28 01:01:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2914838)[0;0m WARNING 12-28 01:01:12 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:01:12 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.70 seconds
[1;36m(EngineCore_DP0 pid=2914595)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2914595)[0;0m 
[1;36m(EngineCore_DP0 pid=2914595)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2914595)[0;0m 
[1;36m(EngineCore_DP0 pid=2914595)[0;0m INFO 12-28 01:01:12 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 01:01:12 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1429.19it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<03:11,  1.93s/it, est. speed input: 80.82 toks/s, output: 0.52 toks/s]Processed prompts:   2%|â–         | 2/100 [00:03<02:50,  1.74s/it, est. speed input: 67.28 toks/s, output: 0.57 toks/s]Processed prompts:   7%|â–‹         | 7/100 [00:03<00:32,  2.83it/s, est. speed input: 667.32 toks/s, output: 4.65 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:03<00:13,  6.28it/s, est. speed input: 957.47 toks/s, output: 11.41 toks/s]Processed prompts:  18%|â–ˆâ–Š        | 18/100 [00:03<00:08,  9.72it/s, est. speed input: 1373.79 toks/s, output: 19.35 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:03<00:04, 15.78it/s, est. speed input: 1793.84 toks/s, output: 32.91 toks/s]Processed prompts:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:04<00:04, 16.78it/s, est. speed input: 2108.14 toks/s, output: 45.08 toks/s]Processed prompts:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:04<00:03, 19.81it/s, est. speed input: 2239.46 toks/s, output: 58.33 toks/s]Processed prompts:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:04<00:02, 24.37it/s, est. speed input: 2443.01 toks/s, output: 80.96 toks/s]Processed prompts:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:04<00:02, 20.59it/s, est. speed input: 2654.66 toks/s, output: 94.43 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:05<00:03, 16.63it/s, est. speed input: 2543.38 toks/s, output: 111.06 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:05<00:03, 13.51it/s, est. speed input: 2389.55 toks/s, output: 125.10 toks/s]WARNING 12-28 01:01:18 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:05<00:03, 13.63it/s, est. speed input: 2387.67 toks/s, output: 155.46 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:06<00:03, 12.55it/s, est. speed input: 2333.96 toks/s, output: 167.51 toks/s]Processed prompts:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:06<00:03, 10.68it/s, est. speed input: 2334.35 toks/s, output: 184.51 toks/s]Processed prompts:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:06<00:03,  9.89it/s, est. speed input: 2267.01 toks/s, output: 197.34 toks/s]Processed prompts:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:06<00:02, 11.92it/s, est. speed input: 2303.54 toks/s, output: 225.01 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:07<00:02, 11.87it/s, est. speed input: 2293.28 toks/s, output: 240.84 toks/s]Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:07<00:02, 10.56it/s, est. speed input: 2234.64 toks/s, output: 254.61 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:07<00:02,  9.72it/s, est. speed input: 2213.84 toks/s, output: 268.69 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:08<00:03,  7.35it/s, est. speed input: 2104.05 toks/s, output: 277.07 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:08<00:01, 10.74it/s, est. speed input: 2209.74 toks/s, output: 330.81 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:08<00:01, 11.68it/s, est. speed input: 2228.28 toks/s, output: 351.72 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:08<00:02,  8.27it/s, est. speed input: 2127.06 toks/s, output: 359.22 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:09<00:01,  8.30it/s, est. speed input: 2164.05 toks/s, output: 385.76 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:09<00:01,  9.22it/s, est. speed input: 2222.82 toks/s, output: 408.38 toks/s]Processed prompts:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:09<00:01,  7.96it/s, est. speed input: 2155.71 toks/s, output: 423.15 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:09<00:00,  8.36it/s, est. speed input: 2126.81 toks/s, output: 444.24 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:10<00:01,  3.77it/s, est. speed input: 1935.46 toks/s, output: 419.25 toks/s]Processed prompts:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:13<00:03,  1.56it/s, est. speed input: 1603.94 toks/s, output: 365.74 toks/s]Processed prompts:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:13<00:02,  1.78it/s, est. speed input: 1576.30 toks/s, output: 377.43 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  1.78it/s, est. speed input: 1678.99 toks/s, output: 471.40 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:13<00:00,  7.41it/s, est. speed input: 1678.99 toks/s, output: 471.40 toks/s]
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:27 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2914837)[0;0m INFO 12-28 01:01:27 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2914838)[0;0m INFO 12-28 01:01:27 [multiproc_executor.py:558] Parent process exited, terminating worker
[INFO 12-28 01:01:31] ax.service.ax_client: Completed trial 37 with data: {'throughput(token/s)': 468.911606, 'energy(J/token)': 1.385476}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 01:01:32] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.3584089128404144), ObjectiveThreshold(throughput(token/s) >= 235.0831598587202)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 01:01:34] ax.service.ax_client: Generated new trial 38 with parameters {'block_size': 128, 'max_num_seqs': 64, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T01:01:27', project_name='codecarbon', run_id='674ffd1c-5c83-4187-80e5-5f8b0616f7e9', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=51.024423004011624, emissions=0.0005814491131220094, emissions_rate=1.1395505894820113e-05, cpu_power=30.089705385000002, gpu_power=135.84954981449815, ram_power=70.0, cpu_energy=0.000408486381917623, gpu_energy=0.0010877178146184718, ram_energy=0.0009510855059417534, energy_consumed=0.002447289702477848, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=2, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 28/30 completed. Throughput: 468.91, Energy: 1.385476. Pareto size: 6.
INFO 12-28 01:01:36 [utils.py:233] non-default args: {'seed': 42, 'tensor_parallel_size': 4, 'block_size': 128, 'enable_prefix_caching': False, 'max_num_batched_tokens': 12288, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': False, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 01:01:36 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 01:01:36 [model.py:1510] Using max model len 32768
INFO 12-28 01:01:36 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 01:01:36 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 01:01:44 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:01:47 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:01:47 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_DP0 pid=2917152)[0;0m WARNING 12-28 01:01:47 [multiproc_executor.py:720] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:01:47 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_225cc7e3'), local_subscribe_addr='ipc:///tmp/85b0f7ae-a751-4494-a9ca-7b825e6f27f7', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 01:01:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 01:01:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 01:01:53 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 01:01:54 [__init__.py:216] Automatically detected platform cuda.
INFO 12-28 01:01:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_426f9ce7'), local_subscribe_addr='ipc:///tmp/3b6823f5-859e-489c-8416-4161e909d219', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 01:01:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_a157ae91'), local_subscribe_addr='ipc:///tmp/3d7fe55a-d8aa-4f08-bc2e-36359615f2d8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 01:01:58 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_1ba764ff'), local_subscribe_addr='ipc:///tmp/54f5cb15-e740-4cc2-83bb-e644f65f419a', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-28 01:02:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b1006871'), local_subscribe_addr='ipc:///tmp/5d42dcb9-41df-494f-8e14-5536f1690990', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 01:02:00 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:02:00 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:02:00 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:02:00 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:02:00 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:02:00 [pynccl.py:103] vLLM is using nccl==2.27.3
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 01:02:00 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:02:00 [pynccl.py:103] vLLM is using nccl==2.27.3
WARNING 12-28 01:02:00 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 01:02:00 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 01:02:00 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
WARNING 12-28 01:02:00 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.9 not supported, communicator is not available.
INFO 12-28 01:02:00 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_2f282141'), local_subscribe_addr='ipc:///tmp/6934457c-fb8f-4ac1-85b2-7ba7798af680', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
INFO 12-28 01:02:00 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:02:00 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:02:00 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:02:00 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:02:00 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-28 01:02:00 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:02:00 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:02:00 [pynccl.py:103] vLLM is using nccl==2.27.3
INFO 12-28 01:02:00 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
INFO 12-28 01:02:00 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
INFO 12-28 01:02:00 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 12-28 01:02:00 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
WARNING 12-28 01:02:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 01:02:00 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 01:02:01 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-28 01:02:01 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:01 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:01 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:01 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:01 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:01 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:01 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:01 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:01 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:01 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:01 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:01 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:01 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:01 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(Worker_TP0 pid=2917398)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:02 [default_loader.py:267] Loading weights took 0.74 seconds
[1;36m(Worker_TP0 pid=2917398)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.77it/s]
[1;36m(Worker_TP0 pid=2917398)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.43it/s]
[1;36m(Worker_TP0 pid=2917398)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.30it/s]
[1;36m(Worker_TP0 pid=2917398)[0;0m 
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:03 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:03 [default_loader.py:267] Loading weights took 0.76 seconds
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:03 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.342757 seconds
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:03 [default_loader.py:267] Loading weights took 0.90 seconds
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:03 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.622259 seconds
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:03 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 1.779984 seconds
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:04 [gpu_model_runner.py:2653] Model loading took 3.3805 GiB and 2.133593 seconds
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:08 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:08 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:08 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:08 [gpu_worker.py:298] Available KV cache memory: 15.84 GiB
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [kv_cache_utils.py:1087] GPU KV cache size: 518,912 tokens
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 31.43x
[1;36m(Worker_TP3 pid=2917401)[0;0m WARNING 12-28 01:02:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP1 pid=2917399)[0;0m WARNING 12-28 01:02:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP0 pid=2917398)[0;0m WARNING 12-28 01:02:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(Worker_TP2 pid=2917400)[0;0m WARNING 12-28 01:02:08 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:08 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.44 seconds
[1;36m(EngineCore_DP0 pid=2917152)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2917152)[0;0m 
[1;36m(EngineCore_DP0 pid=2917152)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2917152)[0;0m 
[1;36m(EngineCore_DP0 pid=2917152)[0;0m INFO 12-28 01:02:09 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 01:02:09 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1181.29it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:01<02:09,  1.31s/it, est. speed input: 119.00 toks/s, output: 0.76 toks/s]Processed prompts:   2%|â–         | 2/100 [00:01<01:10,  1.39it/s, est. speed input: 423.89 toks/s, output: 2.48 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:01<00:21,  4.37it/s, est. speed input: 1111.81 toks/s, output: 8.15 toks/s]WARNING 12-28 01:02:11 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:   8%|â–Š         | 8/100 [00:01<00:13,  7.05it/s, est. speed input: 1427.21 toks/s, output: 16.41 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:02<00:07, 12.17it/s, est. speed input: 2096.06 toks/s, output: 34.60 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:02<00:06, 13.66it/s, est. speed input: 2278.35 toks/s, output: 46.97 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:02<00:03, 19.66it/s, est. speed input: 2747.29 toks/s, output: 70.81 toks/s]Processed prompts:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:02<00:03, 20.81it/s, est. speed input: 2846.84 toks/s, output: 75.85 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:02<00:03, 21.41it/s, est. speed input: 2948.18 toks/s, output: 93.48 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:02<00:03, 19.11it/s, est. speed input: 3058.53 toks/s, output: 104.52 toks/s]Processed prompts:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:02<00:03, 21.20it/s, est. speed input: 2998.38 toks/s, output: 120.81 toks/s]Processed prompts:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:03<00:03, 19.84it/s, est. speed input: 3231.88 toks/s, output: 136.28 toks/s]Processed prompts:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:03<00:02, 20.93it/s, est. speed input: 3389.65 toks/s, output: 154.80 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:03<00:02, 23.92it/s, est. speed input: 3569.78 toks/s, output: 184.61 toks/s]Processed prompts:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:03<00:02, 20.04it/s, est. speed input: 3531.70 toks/s, output: 219.03 toks/s]Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:03<00:02, 20.27it/s, est. speed input: 3459.23 toks/s, output: 244.87 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:04<00:02, 16.41it/s, est. speed input: 3314.21 toks/s, output: 263.89 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:04<00:02, 14.92it/s, est. speed input: 3204.68 toks/s, output: 294.96 toks/s]Processed prompts:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:04<00:01, 18.68it/s, est. speed input: 3381.17 toks/s, output: 353.94 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:04<00:02, 14.32it/s, est. speed input: 3226.31 toks/s, output: 372.19 toks/s]Processed prompts:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:05<00:02, 13.99it/s, est. speed input: 3201.78 toks/s, output: 397.54 toks/s]Processed prompts:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:05<00:01, 14.24it/s, est. speed input: 3251.51 toks/s, output: 437.70 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:05<00:01, 14.05it/s, est. speed input: 3213.74 toks/s, output: 474.42 toks/s]Processed prompts:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:05<00:01, 13.48it/s, est. speed input: 3314.39 toks/s, output: 497.62 toks/s]Processed prompts:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:05<00:01, 11.07it/s, est. speed input: 3174.76 toks/s, output: 516.14 toks/s]Processed prompts:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:06<00:02,  8.72it/s, est. speed input: 3007.16 toks/s, output: 531.83 toks/s]Processed prompts:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:06<00:01,  9.24it/s, est. speed input: 3062.48 toks/s, output: 562.69 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:06<00:01, 10.11it/s, est. speed input: 3028.21 toks/s, output: 597.30 toks/s]Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:06<00:01, 10.29it/s, est. speed input: 3032.60 toks/s, output: 627.33 toks/s]Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:06<00:00, 11.50it/s, est. speed input: 3001.03 toks/s, output: 663.92 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:08<00:02,  4.42it/s, est. speed input: 2584.90 toks/s, output: 624.90 toks/s]Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:08<00:01,  4.44it/s, est. speed input: 2532.00 toks/s, output: 636.77 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:08<00:01,  4.55it/s, est. speed input: 2492.75 toks/s, output: 652.16 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  8.24it/s, est. speed input: 2523.69 toks/s, output: 815.41 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00,  8.24it/s, est. speed input: 2523.69 toks/s, output: 815.41 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:08<00:00, 11.14it/s, est. speed input: 2523.69 toks/s, output: 815.41 toks/s]
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP0 pid=2917398)[0;0m INFO 12-28 01:02:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP1 pid=2917399)[0;0m INFO 12-28 01:02:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP2 pid=2917400)[0;0m INFO 12-28 01:02:18 [multiproc_executor.py:599] WorkerProc shutting down.
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:18 [multiproc_executor.py:558] Parent process exited, terminating worker
[1;36m(Worker_TP3 pid=2917401)[0;0m INFO 12-28 01:02:18 [multiproc_executor.py:599] WorkerProc shutting down.
[INFO 12-28 01:02:24] ax.service.ax_client: Completed trial 38 with data: {'throughput(token/s)': 807.652524, 'energy(J/token)': 1.339941}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 01:02:24] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.3584090229133596), ObjectiveThreshold(throughput(token/s) >= 235.08281417116075)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

/home/wyn23/r244/venv/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:

A not p.d., added jitter of 1.0e-08 to the diagonal

[INFO 12-28 01:02:26] ax.service.ax_client: Generated new trial 39 with parameters {'block_size': 32, 'max_num_seqs': 64, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 1, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1} using model BoTorch.
EmissionsData(timestamp='2025-12-28T01:02:18', project_name='codecarbon', run_id='223ac05d-3d22-4788-af3b-d1363f5fa137', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=43.02876517199911, emissions=0.0006471453851064055, emissions_rate=1.5039831668874713e-05, cpu_power=30.17475579692308, gpu_power=243.08883847311282, ram_power=70.0, cpu_energy=0.0003466053189417097, gpu_energy=0.0015711407013565548, ram_energy=0.0008060559448870158, energy_consumed=0.0027238019651852803, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=4, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 29/30 completed. Throughput: 807.65, Energy: 1.339941. Pareto size: 6.
INFO 12-28 01:02:28 [utils.py:233] non-default args: {'seed': 42, 'block_size': 32, 'enable_prefix_caching': True, 'max_num_batched_tokens': 12288, 'max_num_seqs': 64, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True, 'enable_chunked_prefill': True, 'model': 'mistralai/Mistral-7B-Instruct-v0.1'}
INFO 12-28 01:02:29 [model.py:547] Resolved architecture: MistralForCausalLM
INFO 12-28 01:02:29 [model.py:1510] Using max model len 32768
INFO 12-28 01:02:29 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=12288.
INFO 12-28 01:02:29 [__init__.py:381] Cudagraph is disabled under eager mode
/home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:

It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.

INFO 12-28 01:02:36 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:39 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:39 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistralai/Mistral-7B-Instruct-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=mistralai/Mistral-7B-Instruct-v0.1, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:41 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=2919837)[0;0m WARNING 12-28 01:02:41 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:41 [gpu_model_runner.py:2602] Starting to load model mistralai/Mistral-7B-Instruct-v0.1...
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:41 [gpu_model_runner.py:2634] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:42 [cuda.py:366] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:42 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=2919837)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=2919837)[0;0m Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.11s/it]
[1;36m(EngineCore_DP0 pid=2919837)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.25it/s]
[1;36m(EngineCore_DP0 pid=2919837)[0;0m Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.18it/s]
[1;36m(EngineCore_DP0 pid=2919837)[0;0m 
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:44 [default_loader.py:267] Loading weights took 1.73 seconds
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:44 [gpu_model_runner.py:2653] Model loading took 13.4967 GiB and 2.305767 seconds
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:48 [gpu_worker.py:298] Available KV cache memory: 5.03 GiB
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:48 [kv_cache_utils.py:1087] GPU KV cache size: 41,152 tokens
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:48 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 2.51x
[1;36m(EngineCore_DP0 pid=2919837)[0;0m WARNING 12-28 01:02:48 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:49 [core.py:210] init engine (profile, create kv cache, warmup model) took 4.28 seconds
[1;36m(EngineCore_DP0 pid=2919837)[0;0m /home/wyn23/r244/venv/lib/python3.12/site-packages/vllm/transformers_utils/tokenizer.py:286: FutureWarning:
[1;36m(EngineCore_DP0 pid=2919837)[0;0m 
[1;36m(EngineCore_DP0 pid=2919837)[0;0m It is strongly recommended to run mistral models with `--tokenizer-mode "mistral"` to ensure correct encoding and decoding.
[1;36m(EngineCore_DP0 pid=2919837)[0;0m 
[1;36m(EngineCore_DP0 pid=2919837)[0;0m INFO 12-28 01:02:49 [__init__.py:381] Cudagraph is disabled under eager mode
INFO 12-28 01:02:49 [llm.py:306] Supported_tasks: ['generate']
Adding requests:   0%|          | 0/100 [00:00<?, ?it/s]Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 1409.64it/s]
Processed prompts:   0%|          | 0/100 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Processed prompts:   1%|          | 1/100 [00:03<05:12,  3.16s/it, est. speed input: 49.38 toks/s, output: 0.32 toks/s]Processed prompts:   2%|â–         | 2/100 [00:03<02:49,  1.73s/it, est. speed input: 176.09 toks/s, output: 1.03 toks/s]Processed prompts:   5%|â–Œ         | 5/100 [00:04<00:52,  1.82it/s, est. speed input: 462.29 toks/s, output: 3.39 toks/s]WARNING 12-28 01:02:54 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:   7%|â–‹         | 7/100 [00:04<00:35,  2.65it/s, est. speed input: 533.35 toks/s, output: 5.47 toks/s]Processed prompts:   8%|â–Š         | 8/100 [00:04<00:30,  2.99it/s, est. speed input: 590.00 toks/s, output: 6.78 toks/s]Processed prompts:  11%|â–ˆ         | 11/100 [00:04<00:18,  4.79it/s, est. speed input: 789.85 toks/s, output: 10.83 toks/s]Processed prompts:  13%|â–ˆâ–Ž        | 13/100 [00:04<00:14,  5.82it/s, est. speed input: 862.10 toks/s, output: 14.23 toks/s]Processed prompts:  16%|â–ˆâ–Œ        | 16/100 [00:05<00:13,  6.45it/s, est. speed input: 937.62 toks/s, output: 19.33 toks/s]Processed prompts:  20%|â–ˆâ–ˆ        | 20/100 [00:05<00:08,  9.26it/s, est. speed input: 1112.34 toks/s, output: 24.96 toks/s]Processed prompts:  22%|â–ˆâ–ˆâ–       | 22/100 [00:05<00:08,  9.40it/s, est. speed input: 1129.32 toks/s, output: 29.11 toks/s]Processed prompts:  24%|â–ˆâ–ˆâ–       | 24/100 [00:05<00:07,  9.54it/s, est. speed input: 1176.38 toks/s, output: 28.80 toks/s]Processed prompts:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:06<00:07,  9.86it/s, est. speed input: 1154.28 toks/s, output: 33.62 toks/s]Processed prompts:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:06<00:07,  9.20it/s, est. speed input: 1138.44 toks/s, output: 38.37 toks/s]Processed prompts:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:06<00:07,  9.20it/s, est. speed input: 1179.04 toks/s, output: 40.22 toks/s]Processed prompts:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:06<00:07,  9.41it/s, est. speed input: 1263.15 toks/s, output: 45.55 toks/s]Processed prompts:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:07<00:05, 11.30it/s, est. speed input: 1251.35 toks/s, output: 49.67 toks/s]Processed prompts:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:07<00:08,  7.73it/s, est. speed input: 1202.53 toks/s, output: 50.49 toks/s]Processed prompts:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:07<00:05, 10.98it/s, est. speed input: 1384.04 toks/s, output: 59.37 toks/s]Processed prompts:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:08<00:04, 11.34it/s, est. speed input: 1504.80 toks/s, output: 68.33 toks/s]Processed prompts:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:08<00:04, 10.52it/s, est. speed input: 1475.69 toks/s, output: 73.21 toks/s]Processed prompts:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:08<00:04, 10.92it/s, est. speed input: 1510.42 toks/s, output: 80.30 toks/s]Processed prompts:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:08<00:04, 10.17it/s, est. speed input: 1487.27 toks/s, output: 87.51 toks/s]WARNING 12-28 01:02:58 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:08<00:04,  9.69it/s, est. speed input: 1619.91 toks/s, output: 93.16 toks/s]Processed prompts:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:09<00:06,  7.21it/s, est. speed input: 1546.51 toks/s, output: 98.00 toks/s]Processed prompts:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:09<00:06,  7.07it/s, est. speed input: 1552.78 toks/s, output: 101.83 toks/s]Processed prompts:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:09<00:06,  6.96it/s, est. speed input: 1538.26 toks/s, output: 105.77 toks/s]Processed prompts:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:09<00:05,  6.86it/s, est. speed input: 1519.67 toks/s, output: 107.98 toks/s]Processed prompts:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:10<00:05,  6.78it/s, est. speed input: 1501.95 toks/s, output: 112.10 toks/s]Processed prompts:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:10<00:05,  7.45it/s, est. speed input: 1476.81 toks/s, output: 121.10 toks/s]Processed prompts:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:10<00:03, 10.46it/s, est. speed input: 1581.17 toks/s, output: 133.15 toks/s]Processed prompts:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:11<00:06,  5.21it/s, est. speed input: 1486.20 toks/s, output: 135.59 toks/s]Processed prompts:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:11<00:05,  5.42it/s, est. speed input: 1470.60 toks/s, output: 140.47 toks/s]WARNING 12-28 01:03:01 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:12<00:07,  4.03it/s, est. speed input: 1412.39 toks/s, output: 145.04 toks/s]Processed prompts:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:12<00:10,  2.78it/s, est. speed input: 1327.80 toks/s, output: 143.35 toks/s]Processed prompts:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:13<00:07,  3.70it/s, est. speed input: 1339.33 toks/s, output: 155.65 toks/s]Processed prompts:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:13<00:06,  3.84it/s, est. speed input: 1317.87 toks/s, output: 160.10 toks/s]Processed prompts:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:13<00:06,  3.73it/s, est. speed input: 1293.74 toks/s, output: 164.41 toks/s]Processed prompts:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:13<00:05,  4.20it/s, est. speed input: 1285.00 toks/s, output: 170.26 toks/s]Processed prompts:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:14<00:04,  4.65it/s, est. speed input: 1258.23 toks/s, output: 181.67 toks/s]Processed prompts:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:14<00:03,  5.69it/s, est. speed input: 1248.67 toks/s, output: 194.11 toks/s]Processed prompts:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:14<00:02,  6.07it/s, est. speed input: 1234.73 toks/s, output: 206.56 toks/s]Processed prompts:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:15<00:02,  5.92it/s, est. speed input: 1265.92 toks/s, output: 217.42 toks/s]Processed prompts:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:15<00:02,  6.18it/s, est. speed input: 1255.56 toks/s, output: 223.17 toks/s]Processed prompts:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:15<00:02,  6.44it/s, est. speed input: 1313.32 toks/s, output: 228.18 toks/s]WARNING 12-28 01:03:05 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:16<00:04,  2.90it/s, est. speed input: 1257.16 toks/s, output: 222.40 toks/s]Processed prompts:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:18<00:09,  1.22it/s, est. speed input: 1114.82 toks/s, output: 204.82 toks/s]WARNING 12-28 01:03:09 [detokenizer.py:246] Encountered invalid prefix detokenization error for request 48, resetting decode stream.
Processed prompts:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:19<00:10,  1.02it/s, est. speed input: 1039.73 toks/s, output: 199.45 toks/s]Processed prompts:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:21<00:07,  1.23it/s, est. speed input: 989.56 toks/s, output: 207.84 toks/s] Processed prompts:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:22<00:08,  1.02s/it, est. speed input: 919.50 toks/s, output: 203.19 toks/s]Processed prompts:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:23<00:06,  1.12it/s, est. speed input: 902.97 toks/s, output: 209.77 toks/s]Processed prompts:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:23<00:00,  2.90it/s, est. speed input: 942.48 toks/s, output: 269.32 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  2.45it/s, est. speed input: 916.94 toks/s, output: 270.92 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  2.45it/s, est. speed input: 916.94 toks/s, output: 270.92 toks/s]Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:24<00:00,  4.05it/s, est. speed input: 916.94 toks/s, output: 270.92 toks/s]
[rank0]:[W1228 01:03:15.935240197 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[INFO 12-28 01:03:17] ax.service.ax_client: Completed trial 39 with data: {'throughput(token/s)': 270.130414, 'energy(J/token)': 1.049446}.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

/home/wyn23/r244/venv/lib/python3.12/site-packages/gpytorch/likelihoods/noise_models.py:150: NumericalWarning:

Very small noise values detected. This will likely lead to numerical instabilities. Rounding small noise values up to 1e-06.

[INFO 12-28 01:03:18] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.358408671971921), ObjectiveThreshold(throughput(token/s) >= 235.08239947689782)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
/home/wyn23/r244/venv/lib/python3.12/site-packages/ax/adapter/transforms/winsorize.py:123: AxOptimizationWarning:

Encountered a `MultiObjective` without objective thresholds. We will winsorize each objective separately. We strongly recommend specifying the objective thresholds when using multi-objective optimization.

[INFO 12-28 01:03:19] ax.service.utils.best_point: Using inferred objective thresholds: [ObjectiveThreshold(energy(J/token) <= 1.358408671971921), ObjectiveThreshold(throughput(token/s) >= 235.08239947689782)], as objective thresholds were not specified as part of the optimization configuration on the experiment.
EmissionsData(timestamp='2025-12-28T01:03:15', project_name='codecarbon', run_id='d6fc6de9-b984-4744-b81f-0101976a7713', experiment_id='5b0fa12a-3dd7-45bb-9766-cc326314d9f1', duration=46.942435003002174, emissions=0.00046348946816986814, emissions_rate=9.873571069336859e-06, cpu_power=30.001109160000002, gpu_power=46.32273791920349, ram_power=70.0, cpu_energy=0.0003742214747707806, gpu_energy=0.0007048316749767025, ram_energy=0.0008717504151055538, energy_consumed=0.0019508035648530368, water_consumed=0.0, country_name='United Kingdom', country_iso_code='GBR', region=None, cloud_provider='', cloud_region='', os='Linux-6.14.0-37-generic-x86_64-with-glibc2.39', python_version='3.12.3', codecarbon_version='3.1.1', cpu_count=64, cpu_model='INTEL(R) XEON(R) SILVER 4514Y', gpu_count=1, gpu_model='8 x NVIDIA L4', longitude=None, latitude=None, ram_total_size=503.3951835632324, tracking_mode='machine', on_cloud='N', pue=1.0, wue=0.0)
   BO Iteration 30/30 completed. Throughput: 270.13, Energy: 1.049446. Pareto size: 6.
======================================================================
Optimization Complete.
======================================================================

ðŸ“Š Found 6 Pareto-optimal configurations:
  - 28: ({'block_size': 64, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1}, ({'energy(J/token)': np.float64(1.2703117350687754), 'throughput(token/s)': np.float64(837.2077926672505)}, {'energy(J/token)': {'energy(J/token)': 1.9787949394143324e-07, 'throughput(token/s)': 0.0}, 'throughput(token/s)': {'energy(J/token)': 0.0, 'throughput(token/s)': 0.0847044464934934}}))
  - 27: ({'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1}, ({'energy(J/token)': np.float64(1.271580330954871), 'throughput(token/s)': np.float64(837.71912497822)}, {'energy(J/token)': {'energy(J/token)': 1.9793429354798578e-07, 'throughput(token/s)': 0.0}, 'throughput(token/s)': {'energy(J/token)': 0.0, 'throughput(token/s)': 0.09423250857764075}}))
  - 19: ({'block_size': 128, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1}, ({'energy(J/token)': np.float64(1.278102424513723), 'throughput(token/s)': np.float64(837.7469871880041)}, {'energy(J/token)': {'energy(J/token)': 1.9792607550247586e-07, 'throughput(token/s)': 0.0}, 'throughput(token/s)': {'energy(J/token)': 0.0, 'throughput(token/s)': 0.08940440859202467}}))
  - 36: ({'block_size': 128, 'max_num_seqs': 64, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 2, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'dtype': 'auto', 'data_parallel_size': 1}, ({'energy(J/token)': np.float64(1.2548765154893164), 'throughput(token/s)': np.float64(506.48522175596725)}, {'energy(J/token)': {'energy(J/token)': 1.9794536792289806e-07, 'throughput(token/s)': 0.0}, 'throughput(token/s)': {'energy(J/token)': 0.0, 'throughput(token/s)': 0.09473576109681375}}))
  - 11: ({'block_size': 32, 'max_num_seqs': 128, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 1, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': False, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1}, ({'energy(J/token)': np.float64(0.9660656275548666), 'throughput(token/s)': np.float64(290.2310833332508)}, {'energy(J/token)': {'energy(J/token)': 1.9794562739941033e-07, 'throughput(token/s)': 0.0}, 'throughput(token/s)': {'energy(J/token)': 0.0, 'throughput(token/s)': 0.09474483215017471}}))
  - 24: ({'block_size': 32, 'max_num_seqs': 256, 'max_num_batched_tokens': 12288, 'tensor_parallel_size': 4, 'pipeline_parallel_size': 1, 'enable_chunked_prefill': True, 'enable_prefix_caching': True, 'dtype': 'auto', 'data_parallel_size': 1}, ({'energy(J/token)': np.float64(1.3227411224816747), 'throughput(token/s)': np.float64(841.7179218960209)}, {'energy(J/token)': {'energy(J/token)': 6.598011507208304e-08, 'throughput(token/s)': 0.0}, 'throughput(token/s)': {'energy(J/token)': 0.0, 'throughput(token/s)': 0.03138492707679212}}))
